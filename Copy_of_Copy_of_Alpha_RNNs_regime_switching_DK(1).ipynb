{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy_of_Copy_of_Alpha_RNNs_regime_switching_DK(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/Copy_of_Copy_of_Alpha_RNNs_regime_switching_DK(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDLTylUvNgsc",
        "outputId": "36227712-1bd3-4ab9-c945-2827929c2ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7WLRTI-WuTTU",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9BywHfc63kb",
        "colab": {},
        "outputId": "40a5bf41-07c2-4b3d-903e-85d238cb4d89"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "#from alphaRNN import *\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "import keras.layers\n",
        "#from alphaRNN import *\n",
        "from keras import *\n",
        "from keras.legacy import interfaces\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\t\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "# To make this notebook's output stable across runs\n",
        "#def reset_graph(seed=42):\n",
        "#    tf.reset_default_graph()\n",
        "#    tf.set_random_seed(seed)\n",
        "#    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/macbookpro/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5vN0pY69OFz",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYzhUo7CuZLa",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, eps=0.01):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    step_length=np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      mu[i*step_length:((i*step_length)+1)]=step_size*(-1)**i\n",
        "     \n",
        "    for i in range(1, length):\n",
        "        sigma[i]=sigma[i-1] + mu[i] + eps*np.random.normal(0,1)\n",
        "        \n",
        "    return sigma   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DuOojJHWukx-",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "brhmIH-k7_Xe",
        "colab": {}
      },
      "source": [
        "def simulateAlphaRNN(p, n, plain=False, fixed_alpha=False, alpha=1.0):\n",
        "    \"\"\"\n",
        "    p: is the order of the model\n",
        "    n: is the length of the simulated data\n",
        "    plain: is a boolean indicating whether to simulate plain RNN or alpha-RNN\n",
        "    fixed_alpha: is a boolean indicating whether alpha should be dynamic or static (fixed)\n",
        "    alpha: the default value of alpha (ignored if fixed_alpha=False)\n",
        "    \"\"\"\n",
        "    \n",
        "    x = np.array([0]*n, dtype='float32')\n",
        "    x[:p]= 1 # initialize the first p values\n",
        "    alpha_hat_ar=np.array([0]*n, dtype='float32')\n",
        "    s_ar=np.array([0]*len(x), dtype='float32')\n",
        "    h_tilde = 0 \n",
        "    s=0.5\n",
        "    s_=0.0\n",
        "    if fixed_alpha:\n",
        "        alpha_hat=alpha\n",
        "    for i in range(len(x)):\n",
        "        #if plain:\n",
        "        h_hat=0 # there is no recurrent step at the last lag: h_hat=sigma(w*x[i-p+j] + b)\n",
        "        #if i%100 ==0:  \n",
        "        #  if s==0:\n",
        "        #    s=1\n",
        "        #  else:\n",
        "        #    s=0  \n",
        "        for j in range(p):\n",
        "            if (i-p+j)>=0:\n",
        "                #h_hat=sigma(w*x[i-p+j] + u*h_tilde + b)\n",
        "                if not plain:\n",
        "                    if not fixed_alpha:\n",
        "                      alpha_hat=sigmoid(w_alpha*x[i-p+j] + u_alpha*h_tilde + b_alpha)    \n",
        "                    h_tilde = (1-alpha_hat)*h_tilde + alpha_hat*h_hat\n",
        "                else:\n",
        "                    h_tilde=h_hat\n",
        "                \n",
        "                s_=sigmoid(5000*(w_alpha*x[i-p+j] + u_alpha*h_tilde + b_alpha))\n",
        "                h_hat=np.tanh(w_h*x[i-p+j] + (s_*u_h1+(1-s_)*u_h2)*h_tilde + b_h)    \n",
        "            \n",
        "        x[i]=h_tilde + 0.001*np.random.randn()\n",
        "        alpha_hat_ar[i]=alpha_hat\n",
        "        s_ar[i] = s_ #w_alpha*x[i-p+j] + u_alpha*h_tilde + b_alpha\n",
        "        \n",
        "    return(x, alpha_hat_ar, s_ar)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "poVF1w468PI1",
        "colab": {}
      },
      "source": [
        "w_h,w_alpha = [1, -1.0] # weight\n",
        "u_h1,u_h2,u_alpha =[1.0 , 0.1, 0.1]\n",
        "b_h,b_alpha=[0,  0.5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fzFFt7u8UXQ",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "osF2M8Ay8GMc",
        "outputId": "409d0b32-f48d-47db-a859-eb3b0ea7867f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "x_t_RNN, alpha_hat_ar, s_ar=simulateAlphaRNN(30, 2000, plain=False, fixed_alpha=True, alpha=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmpBFh449ViY",
        "outputId": "c87f031b-485f-408d-9b62-47b34032831c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "plt.plot(x_t_RNN[:100])\n",
        "plt.plot(s_ar[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb9c3d8f400>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAgI8CGBuAcEBAT4kP8PI5VzadIO9ogAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JztXCwy5upi7",
        "colab": {}
      },
      "source": [
        "#p = 30 # the number of lags (in both the data and the models)\n",
        "#vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wfj1k-XhsKp",
        "outputId": "7bbb79a6-158f-40a0-ef03-8020805c92fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df = pd.read_csv(\"/Users/macbookpro/Desktop/Data/Load3.csv\")\n",
        "df = pd.DataFrame(df)\n",
        "df.head()\n",
        "\n",
        "#zip_path = tf.keras.utils.get_file(\n",
        "#    origin='https://drive.google.com/open?id=1KTEKHyXxx0S2ZFXp1evZHoU_Em1PIPxY',\n",
        "    #'https://datahub.io/core/finance-vix#resource-finance-vix_zip',\n",
        "    #'https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "#    fname='finance-vix_zip.zip', #'jena_climate_2009_2016.csv.zip',\n",
        "#    extract=True)\n",
        "#csv_path, _ = os.path.splitext(zip_path)\n",
        "#df = pd.read_csv(csv_path)\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utc_timestamp</th>\n",
              "      <th>DK_2_load_actual_net_consumption_tso</th>\n",
              "      <th>DK_2_load_actual_tso</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-01-01T00:00:00Z</td>\n",
              "      <td>1468.0</td>\n",
              "      <td>1499.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-01-01T01:00:00Z</td>\n",
              "      <td>1400.2</td>\n",
              "      <td>1439.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-01-01T02:00:00Z</td>\n",
              "      <td>1334.8</td>\n",
              "      <td>1372.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-01-01T03:00:00Z</td>\n",
              "      <td>1290.1</td>\n",
              "      <td>1324.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-01-01T04:00:00Z</td>\n",
              "      <td>1286.1</td>\n",
              "      <td>1314.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          utc_timestamp  DK_2_load_actual_net_consumption_tso  \\\n",
              "0  2008-01-01T00:00:00Z                                1468.0   \n",
              "1  2008-01-01T01:00:00Z                                1400.2   \n",
              "2  2008-01-01T02:00:00Z                                1334.8   \n",
              "3  2008-01-01T03:00:00Z                                1290.1   \n",
              "4  2008-01-01T04:00:00Z                                1286.1   \n",
              "\n",
              "   DK_2_load_actual_tso  \n",
              "0                1499.4  \n",
              "1                1439.4  \n",
              "2                1372.0  \n",
              "3                1324.9  \n",
              "4                1314.4  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6FlzpkZ5hw58",
        "colab": {},
        "outputId": "607ed889-7b43-438a-bc96-96f3ed2a273d"
      },
      "source": [
        "#uni_data = df['DE_load_actual_entsoe_transparency']\n",
        "uni_data = df['DK_2_load_actual_net_consumption_tso']\n",
        "uni_date = df['utc_timestamp']\n",
        "uni_data.index = uni_date\n",
        "uni_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "utc_timestamp\n",
              "2008-01-01T00:00:00Z    1468.0\n",
              "2008-01-01T01:00:00Z    1400.2\n",
              "2008-01-01T02:00:00Z    1334.8\n",
              "2008-01-01T03:00:00Z    1290.1\n",
              "2008-01-01T04:00:00Z    1286.1\n",
              "Name: DK_2_load_actual_net_consumption_tso, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "untInWSMuxSb",
        "colab": {}
      },
      "source": [
        "#df = pd.DataFrame(x_t_RNN, columns=['x'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7wGqBI6u0M6",
        "colab": {}
      },
      "source": [
        "#use_features = ['x'] \n",
        "#target = 'x'\n",
        "#use_features = ['DE_load_actual_entsoe_transparency']\n",
        "use_features = ['DK_2_load_actual_net_consumption_tso']\n",
        "target = 'DK_2_load_actual_net_consumption_tso'\n",
        "\n",
        "n_steps = 30 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(0.2*len(df)*train_weight)\n",
        "uni_train_mean = uni_data[:split].mean()\n",
        "uni_train_std = uni_data[:split].std()\n",
        "uni_data = (uni_data - uni_train_mean)/uni_train_std\n",
        "#print(uni_data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_df = scaler.fit_transform(df[use_features])\n",
        "scaled_df = pd.DataFrame(scaled_df, columns=['DK_2_load_actual_net_consumption_tso'])\n",
        "df_train = scaled_df[use_features].iloc[:split]\n",
        "#df_train = uni_data.iloc[:split]\n",
        "#print(df)\n",
        "df_test = scaled_df[use_features].iloc[split:]\n",
        "#df_test = uni_data.iloc[split:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTCv6NRBvBa4",
        "colab": {}
      },
      "source": [
        "def get_lagged_features(value, n_steps,n_steps_ahead):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps+n_steps_ahead-1, n_steps_ahead-1, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE0sdbrNvM3R",
        "colab": {}
      },
      "source": [
        "n_steps_ahead=10\n",
        "\n",
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps, n_steps_ahead).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps,n_steps_ahead).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBwt67S4vQmu",
        "outputId": "3f3acc20-d379-4164-a16c-ceb6721a08ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15746, 30, 1) (15746, 1, 1) (82837, 30, 1) (82837, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AFhK-lgavTuB",
        "colab": {}
      },
      "source": [
        "train_batch_size = y_train_reg.shape[0]\n",
        "test_batch_size = y_test_reg.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COZs3KSovWsf",
        "outputId": "d9226833-671a-4a72-b256-25c6d1aabea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pGb4NRQaCEOr",
        "colab": {}
      },
      "source": [
        "class AlphaRNNCell(Layer):\n",
        "    \"\"\"Cell class for AlphaRNN.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        super(AlphaRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        #self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        \n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        prev_output = states[0]\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(prev_output),\n",
        "                self.recurrent_dropout,\n",
        "                training=training)\n",
        "\n",
        "        dp_mask = self._dropout_mask\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if dp_mask is not None:\n",
        "            h = K.dot(inputs * dp_mask, self.kernel)\n",
        "        else:\n",
        "            h = K.dot(inputs, self.kernel)\n",
        "        if self.bias is not None:\n",
        "            h = K.bias_add(h, self.bias)\n",
        "\n",
        "        if rec_dp_mask is not None:\n",
        "            prev_output *= rec_dp_mask\n",
        "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        output = K.sigmoid(self.alpha)* output + (1-K.sigmoid(self.alpha))* prev_output\n",
        "        # Properly set learning phase on output tensor.\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                output._uses_learning_phase = True\n",
        "        return output, [output]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gkga_VrPW3th",
        "colab": {}
      },
      "source": [
        " class AlphaRNN(keras.layers.RNN):\n",
        "    \"\"\"Fully-connected AlphaRNN where the output is to be fed back to input.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if 'implementation' in kwargs:\n",
        "            kwargs.pop('implementation')\n",
        "            warnings.warn('The `implementation` argument '\n",
        "                          'in `SimpleRNN` has been deprecated. '\n",
        "                          'Please remove it from your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphaRNNCell(units,\n",
        "                             activation=activation,\n",
        "                             use_bias=use_bias,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             recurrent_initializer=recurrent_initializer,\n",
        "                             bias_initializer=bias_initializer,\n",
        "                             kernel_regularizer=kernel_regularizer,\n",
        "                             recurrent_regularizer=recurrent_regularizer,\n",
        "                             bias_regularizer=bias_regularizer,\n",
        "                             kernel_constraint=kernel_constraint,\n",
        "                             recurrent_constraint=recurrent_constraint,\n",
        "                             bias_constraint=bias_constraint,\n",
        "                             dropout=dropout,\n",
        "                             recurrent_dropout=recurrent_dropout)\n",
        "        super(AlphaRNN, self).__init__(cell,\n",
        "                                        return_sequences=return_sequences,\n",
        "                                        return_state=return_state,\n",
        "                                        go_backwards=go_backwards,\n",
        "                                        stateful=stateful,\n",
        "                                        unroll=unroll,\n",
        "                                        **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphaRNN, self).call(inputs,\n",
        "                                           mask=mask,\n",
        "                                           training=training,\n",
        "                                           initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config:\n",
        "            config.pop('implementation')\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AmXMteFc_r8r",
        "colab": {}
      },
      "source": [
        "class AlphatRNNCell(Layer):\n",
        "    \"\"\"Cell class for the AlphatRNN layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 **kwargs):\n",
        "        super(AlphatRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 2),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias_shape = (2, 2*self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            \n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # alpha\n",
        "        self.kernel_alpha = self.kernel[:, :self.units]\n",
        "        self.recurrent_kernel_alpha = self.recurrent_kernel[:, :self.units]\n",
        "        # recurrnce\n",
        "        self.kernel_h = self.kernel[:, self.units:]\n",
        "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_alpha = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units:]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "            \n",
        "            self.recurrent_bias_alpha = self.recurrent_bias[:self.units]    \n",
        "            self.recurrent_bias_h = self.recurrent_bias[self.units:]\n",
        "        else:\n",
        "            self.input_bias_alpha = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_alpha = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_alpha = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_alpha = input\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_alpha = K.dot(inputs_alpha, self.kernel_alpha)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_alpha = K.bias_add(x_alpha, self.input_bias_alpha)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_alpha = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_alpha = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_alpha = K.dot(h_tm1_alpha, self.recurrent_kernel_alpha)\n",
        "           \n",
        "            if self.use_bias:\n",
        "                recurrent_alpha = K.bias_add(recurrent_alpha, self.recurrent_bias_alpha)\n",
        "\n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "           \n",
        "            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n",
        "            if self.use_bias:\n",
        "                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
        "            x_alpha = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            \n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                  matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "            \n",
        "            recurrent_alpha = matrix_inner[:, :self.units] \n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "            recurrent_h = matrix_inner[:, self.units: 2 * self.units]  \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        h = alpha * h_tm1 + (1 - alpha) * hh\n",
        "\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphatRNN(keras.layers.RNN):\n",
        "    \"\"\"Alpha_t RNN\n",
        "    There are two variants. The default one is based on 1406.1078v3 and\n",
        "    has reset gate applied to hidden state before matrix multiplication. The\n",
        "    other one is based on original 1406.1078v1 and has the order reversed.\n",
        "    The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
        "    inference on CPU. Thus it has separate biases for `kernel` and\n",
        "    `recurrent_kernel`. Use `'reset_after'=True` and\n",
        "    `recurrent_activation='sigmoid'`.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        \n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphatRNNCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation)             \n",
        "        super(AlphatRNN, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphatRNN, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nk-9KznNsHQc",
        "colab": {}
      },
      "source": [
        "class RSCell(Layer):\n",
        "    \"\"\"Cell class for the RS layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        reset_after: GRU convention (whether to apply reset gate after or\n",
        "            before matrix multiplication). False = \"before\" (default),\n",
        "            True = \"after\" (CuDNN compatible).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 reset_after=False,\n",
        "                 **kwargs):\n",
        "        super(RSCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.reset_after = reset_after\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 3),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)    \n",
        "\n",
        "        if self.use_bias:\n",
        "            \n",
        "            bias_shape = (2, 3 * self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            # NOTE: need to flatten, since slicing in CNTK gives 2D array\n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # kernels\n",
        "        self.kernel_s = self.kernel[:, :self.units]\n",
        "        self.kernel_h = self.kernel[:, self.units: self.units * 2]\n",
        "        \n",
        "        # recurrent kernels\n",
        "        self.recurrent_kernel_s = self.recurrent_kernel[:, :self.units]\n",
        "        # h1\n",
        "        self.recurrent_kernel_h1 = self.recurrent_kernel[:,\n",
        "                                                        self.units:\n",
        "                                                        self.units * 2]\n",
        "        #h2\n",
        "        self.recurrent_kernel_h2 = self.recurrent_kernel[:, self.units * 2:]                                                \n",
        "        \n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_s = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units: self.units * 2]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "           \n",
        "            self.recurrent_bias_s = self.recurrent_bias[:self.units]\n",
        "            self.recurrent_bias_h1 = (\n",
        "            self.recurrent_bias[self.units: self.units * 2])\n",
        "            self.recurrent_bias_h2 = self.recurrent_bias[self.units * 2:]\n",
        "        else:\n",
        "            self.input_bias_s = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_s = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=3)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=3)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_s = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_s = inputs\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_s = K.dot(inputs_s, self.kernel_s)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_s = K.bias_add(x_s, self.input_bias_s)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_s = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_s = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_s = K.dot(h_tm1_s, self.recurrent_kernel_s)\n",
        "            recurrent_h1 = K.dot(h_tm1_h, self.recurrent_kernel_h1)\n",
        "            recurrent_h2 = K.dot(h_tm1_h, self.recurrent_kernel_h2)\n",
        "            if self.use_bias:\n",
        "                recurrent_s = K.bias_add(recurrent_s, self.recurrent_bias_s)\n",
        "                recurrent_h1 = K.bias_add(recurrent_h1, self.recurrent_bias_h1)\n",
        "                recurrent_h2 = K.bias_add(recurrent_h2, self.recurrent_bias_h2)\n",
        "\n",
        "            s= K.sigmoid(5000*(x_s + recurrent_s)) # smooth approximation to heaviside\n",
        "            #s = K.sigmoid((x_s + recurrent_s))\n",
        "            #s = self.recurrent_activation(x_s + recurrent_s)\n",
        "            #s = heaviside(x_s + recurrent_s)\n",
        "            #h = self.recurrent_activation(x_h + recurrent_h)\n",
        "\n",
        "            # reset gate applied after/before matrix multiplication\n",
        "            #if self.reset_after:\n",
        "            #    recurrent_s = K.dot(h_tm1_s, self.recurrent_kernel_s)\n",
        "            #    if self.use_bias:\n",
        "            #        recurrent_s = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            #    recurrent_h = r * recurrent_h\n",
        "            #else:\n",
        "            recurrent_h = recurrent_h1*s + recurrent_h2*(1-s)\n",
        "\n",
        "            h = self.activation(x_h + recurrent_h)\n",
        "            h = K.sigmoid(self.alpha)* h + 1-K.sigmoid(self.alpha)*h_tm1\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias[:2*self.units])\n",
        "            x_s = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            #x_h = matrix_x[:, 2 * self.units:]\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            # hidden state projected by all gate matrices at once\n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "\n",
        "            recurrent_s = matrix_inner[:, :self.units]\n",
        "            recurrent_h1 = matrix_inner[:, self.units: 2 * self.units]\n",
        "            recurrent_h2 = matrix_inner[:, 2 * self.units:]\n",
        "\n",
        "            #s = self.recurrent_activation(x_s + recurrent_s)\n",
        "            s= K.sigmoid(5000*(x_s + recurrent_s)) # smooth approximation to heaviside\n",
        "            \n",
        "            recurrent_h = recurrent_h1*s + recurrent_h2*(1-s)\n",
        "\n",
        "            h = self.activation(x_h + recurrent_h)\n",
        "            h = K.sigmoid(self.alpha)*h + (1-K.sigmoid(self.alpha))*h_tm1\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        #h = z * h_tm1 + (1 - z) * hh\n",
        "      \n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation,\n",
        "                  'reset_after': self.reset_after}\n",
        "        base_config = super(RSCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class RS(keras.layers.RNN):\n",
        "    \"\"\"RS \n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        reset_after: GRU convention (whether to apply reset gate after or\n",
        "            before matrix multiplication). False = \"before\" (default),\n",
        "            True = \"after\" (CuDNN compatible).\n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 reset_after=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = RSCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation,\n",
        "                       reset_after=reset_after)\n",
        "        super(RS, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(RS, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    @property\n",
        "    def reset_after(self):\n",
        "        return self.cell.reset_after\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation,\n",
        "                  'reset_after': self.reset_after}\n",
        "        base_config = super(GRU, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aHJkBQ3XDO0h",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "\n",
        "\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JA0onJt1FY38",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-8, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXZrHJ9IeYmL",
        "colab": {}
      },
      "source": [
        "def AlphaRNNt(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  #reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(AlphatRNN(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  \n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_--_hYDVNQI",
        "colab": {}
      },
      "source": [
        "def Alpha_Rnn(n_units = 10, l1_reg=0):\n",
        "  reg_model2 = Sequential()\n",
        "  reg_model2.add(AlphaRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model2.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #reg_model2.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DV63hsm9pOyr",
        "colab": {}
      },
      "source": [
        "def Plain_Rnn(n_units = 10, l1_reg=0):\n",
        "  reg_model2 = Sequential()\n",
        "  reg_model2.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model2.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YBmOg6KDtpr2",
        "colab": {}
      },
      "source": [
        "def GRU_(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_LtjCtD_rdLV",
        "colab": {}
      },
      "source": [
        "def LSTM_(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzMO95WR3c8p",
        "colab": {}
      },
      "source": [
        "def RS_(n_units = 10, l1_reg=0, seed=0):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(RS(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.normal(seed=seed), bias_initializer=keras.initializers.normal(seed=seed), recurrent_initializer=keras.initializers.normal(seed=seed), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False, implementation=2))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=seed), bias_initializer=keras.initializers.normal(seed=seed), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L14vD29hfT4S",
        "colab_type": "code",
        "colab": {},
        "outputId": "59c1599b-73e6-4884-feb3-a838d55bd8de"
      },
      "source": [
        "batch_size = 1500 #(int(0.1*split))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZW6uZAlteu5z",
        "colab": {},
        "outputId": "e49d47c1-3edc-44aa-d30b-2374ffc70983"
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for AlphaRNNt...\")\n",
        "model = KerasRegressor(build_fn=AlphaRNNt, epochs=2000, batch_size=batch_size, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnnt = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyper parameter tuning for AlphaRNNt...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  11 out of  17 | elapsed:    0.2s remaining:    0.1s\n",
            "Exception in thread QueueFeederThread:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n",
            "    obj_ = dumps(obj, reducers=reducers)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n",
            "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n",
            "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n",
            "    self.save(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 771, in save_tuple\n",
            "    save(element)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n",
            "    rv = reduce(self.proto)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\", line 1263, in __getstate__\n",
            "    return saving.pickle_model(self)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 429, in pickle_model\n",
            "    _serialize_model(model, f)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 83, in _serialize_model\n",
            "    model_config['config'] = model.get_config()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 278, in get_config\n",
            "    'config': layer.get_config()\n",
            "  File \"<ipython-input-15-9b3e22363810>\", line 570, in get_config\n",
            "    base_config = super(GRU, self).get_config()\n",
            "TypeError: super(type, obj): obj must be an instance or subtype of type\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 175, in _feed\n",
            "    onerror(e, obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 310, in _on_queue_feeder_error\n",
            "    self.thread_wakeup.wakeup()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 155, in wakeup\n",
            "    self._writer.send_bytes(b\"\")\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 183, in send_bytes\n",
            "    self._check_closed()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
            "    raise OSError(\"handle is closed\")\n",
            "OSError: handle is closed\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not pickle the task to send it to the workers.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n    rv = reduce(self.proto)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\", line 1263, in __getstate__\n    return saving.pickle_model(self)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 429, in pickle_model\n    _serialize_model(model, f)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 83, in _serialize_model\n    model_config['config'] = model.get_config()\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 278, in get_config\n    'config': layer.get_config()\n  File \"<ipython-input-15-9b3e22363810>\", line 570, in get_config\n    base_config = super(GRU, self).get_config()\nTypeError: super(type, obj): obj must be an instance or subtype of type\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-8e7ce69d629c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAlphaRNNt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtscv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLYwCOwmfTMI",
        "colab": {},
        "outputId": "422aaf45-ce9c-472a-e4a1-fee9ae167061"
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0, 0.001,0.01,0.1]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "#tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for AlphaRNN...\")\n",
        "model = KerasRegressor(build_fn=Alpha_Rnn, epochs=2000, batch_size=1500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyper parameter tuning for AlphaRNN...\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:    0.3s remaining:    0.1s\n",
            "Exception in thread QueueFeederThread:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n",
            "    obj_ = dumps(obj, reducers=reducers)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n",
            "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n",
            "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n",
            "    self.save(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 771, in save_tuple\n",
            "    save(element)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n",
            "    rv = reduce(self.proto)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\", line 1263, in __getstate__\n",
            "    return saving.pickle_model(self)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 429, in pickle_model\n",
            "    _serialize_model(model, f)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 83, in _serialize_model\n",
            "    model_config['config'] = model.get_config()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 278, in get_config\n",
            "    'config': layer.get_config()\n",
            "  File \"<ipython-input-34-9b3e22363810>\", line 570, in get_config\n",
            "    base_config = super(GRU, self).get_config()\n",
            "TypeError: super(type, obj): obj must be an instance or subtype of type\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 175, in _feed\n",
            "    onerror(e, obj)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 310, in _on_queue_feeder_error\n",
            "    self.thread_wakeup.wakeup()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 155, in wakeup\n",
            "    self._writer.send_bytes(b\"\")\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 183, in send_bytes\n",
            "    self._check_closed()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
            "    raise OSError(\"handle is closed\")\n",
            "OSError: handle is closed\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not pickle the task to send it to the workers.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n    rv = reduce(self.proto)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\", line 1263, in __getstate__\n    return saving.pickle_model(self)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 429, in pickle_model\n    _serialize_model(model, f)\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\", line 83, in _serialize_model\n    model_config['config'] = model.get_config()\n  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 278, in get_config\n    'config': layer.get_config()\n  File \"<ipython-input-34-9b3e22363810>\", line 570, in get_config\n    base_config = super(GRU, self).get_config()\nTypeError: super(type, obj): obj must be an instance or subtype of type\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-195129219f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAlpha_Rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtscv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0WKePk9fT4Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "f04c084e-4096-4de3-919d-eaee18db0ae8"
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0, 0.001,0.01,0.1]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "#tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for Plain_RNN...\")\n",
        "model = KerasRegressor(build_fn=Plain_Rnn, epochs=2000, batch_size=1500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyper parameter tuning for Plain_RNN...\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  8.8min\n",
            "/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "exception calling callback for <Future at 0x136441940 state=finished raised TerminatedWorkerError>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
            "    callback(self)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 309, in __call__\n",
            "    self.parallel.dispatch_next()\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 731, in dispatch_next\n",
            "    if not self.dispatch_one_batch(self._original_iterator):\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 759, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 716, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 510, in apply_async\n",
            "    future = self._workers.submit(SafeFunction(func))\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/reusable_executor.py\", line 151, in submit\n",
            "    fn, *args, **kwargs)\n",
            "  File \"/Users/macbookpro/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 1022, in submit\n",
            "    raise self._flags.broken\n",
            "sklearn.externals.joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TerminatedWorkerError",
          "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3f0aafe5fb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPlain_Rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtscv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/_base.py\u001b[0m in \u001b[0;36m_invoke_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exception calling callback for %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \"\"\"\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             return super(_ReusablePoolExecutor, self).submit(\n\u001b[0;32m--> 151\u001b[0;31m                 fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 raise ShutdownExecutorError(\n",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG8YcGHtfT4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0, 0.001,0.01,0.1]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "#tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for AlphaRS...\")\n",
        "model = KerasRegressor(build_fn=RS_, epochs=2000, batch_size=batch_size, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2s2tRB3U-A72",
        "colab": {}
      },
      "source": [
        "nodes=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RZmpZFAFpZ9p",
        "colab": {},
        "outputId": "317aa4b1-9deb-4fef-98b6-f1b3a6ff058b"
      },
      "source": [
        "#train optimized model\n",
        "rnn = Plain_Rnn(10,0)\n",
        "rnn.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=1500,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "15746/15746 [==============================] - 1s 71us/step - loss: 1.1328\n",
            "Epoch 2/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1305\n",
            "Epoch 3/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1290\n",
            "Epoch 4/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1275\n",
            "Epoch 5/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1260\n",
            "Epoch 6/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1242\n",
            "Epoch 7/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1222\n",
            "Epoch 8/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1198\n",
            "Epoch 9/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1169\n",
            "Epoch 10/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1135\n",
            "Epoch 11/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1099\n",
            "Epoch 12/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1062\n",
            "Epoch 13/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1033\n",
            "Epoch 14/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1013\n",
            "Epoch 15/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.1000\n",
            "Epoch 16/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0992\n",
            "Epoch 17/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0987\n",
            "Epoch 18/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0983\n",
            "Epoch 19/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0980\n",
            "Epoch 20/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0978\n",
            "Epoch 21/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0976\n",
            "Epoch 22/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0974\n",
            "Epoch 23/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0972\n",
            "Epoch 24/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0970\n",
            "Epoch 25/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0968\n",
            "Epoch 26/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0966\n",
            "Epoch 27/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0964\n",
            "Epoch 28/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0962\n",
            "Epoch 29/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0960\n",
            "Epoch 30/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0958\n",
            "Epoch 31/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0956\n",
            "Epoch 32/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0954\n",
            "Epoch 33/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0952\n",
            "Epoch 34/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0950\n",
            "Epoch 35/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0948\n",
            "Epoch 36/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0946\n",
            "Epoch 37/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0943\n",
            "Epoch 38/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0940\n",
            "Epoch 39/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0938\n",
            "Epoch 40/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0935\n",
            "Epoch 41/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0932\n",
            "Epoch 42/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0928\n",
            "Epoch 43/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0925\n",
            "Epoch 44/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0921\n",
            "Epoch 45/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0916\n",
            "Epoch 46/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0912\n",
            "Epoch 47/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0906\n",
            "Epoch 48/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0901\n",
            "Epoch 49/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0894\n",
            "Epoch 50/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0887\n",
            "Epoch 51/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0878\n",
            "Epoch 52/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0869\n",
            "Epoch 53/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0858\n",
            "Epoch 54/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0844\n",
            "Epoch 55/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0828\n",
            "Epoch 56/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0808\n",
            "Epoch 57/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0783\n",
            "Epoch 58/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0749\n",
            "Epoch 59/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0702\n",
            "Epoch 60/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0633\n",
            "Epoch 61/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.0526\n",
            "Epoch 62/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0354\n",
            "Epoch 63/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 1.0065\n",
            "Epoch 64/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.9563\n",
            "Epoch 65/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.8796\n",
            "Epoch 66/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.8191\n",
            "Epoch 67/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.7787\n",
            "Epoch 68/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.7536\n",
            "Epoch 69/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.7363\n",
            "Epoch 70/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.7185\n",
            "Epoch 71/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.6884\n",
            "Epoch 72/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.6544\n",
            "Epoch 73/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.6182\n",
            "Epoch 74/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.5745\n",
            "Epoch 75/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.5250\n",
            "Epoch 76/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.4734\n",
            "Epoch 77/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.4304\n",
            "Epoch 78/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.3909\n",
            "Epoch 79/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.3618\n",
            "Epoch 80/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.3399\n",
            "Epoch 81/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.3174\n",
            "Epoch 82/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2935\n",
            "Epoch 83/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2722\n",
            "Epoch 84/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2570\n",
            "Epoch 85/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2471\n",
            "Epoch 86/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2408\n",
            "Epoch 87/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2363\n",
            "Epoch 88/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2329\n",
            "Epoch 89/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2298\n",
            "Epoch 90/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2273\n",
            "Epoch 91/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2254\n",
            "Epoch 92/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2241\n",
            "Epoch 93/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2236\n",
            "Epoch 94/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2240\n",
            "Epoch 95/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2256\n",
            "Epoch 96/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2278\n",
            "Epoch 97/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2280\n",
            "Epoch 98/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2233\n",
            "Epoch 99/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2168\n",
            "Epoch 100/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2127\n",
            "Epoch 101/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2109\n",
            "Epoch 102/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2101\n",
            "Epoch 103/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2097\n",
            "Epoch 104/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2094\n",
            "Epoch 105/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2089\n",
            "Epoch 106/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2081\n",
            "Epoch 107/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2071\n",
            "Epoch 108/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2061\n",
            "Epoch 109/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2051\n",
            "Epoch 110/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2042\n",
            "Epoch 111/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2034\n",
            "Epoch 112/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2027\n",
            "Epoch 113/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2020\n",
            "Epoch 114/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2014\n",
            "Epoch 115/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2008\n",
            "Epoch 116/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.2002\n",
            "Epoch 117/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1996\n",
            "Epoch 118/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1991\n",
            "Epoch 119/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1985\n",
            "Epoch 120/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1980\n",
            "Epoch 121/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1976\n",
            "Epoch 122/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1971\n",
            "Epoch 123/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1966\n",
            "Epoch 124/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1962\n",
            "Epoch 125/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1958\n",
            "Epoch 126/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1954\n",
            "Epoch 127/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1950\n",
            "Epoch 128/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1946\n",
            "Epoch 129/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1943\n",
            "Epoch 130/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1939\n",
            "Epoch 131/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1935\n",
            "Epoch 132/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1932\n",
            "Epoch 133/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1929\n",
            "Epoch 134/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1926\n",
            "Epoch 135/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1923\n",
            "Epoch 136/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1919\n",
            "Epoch 137/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1917\n",
            "Epoch 138/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1914\n",
            "Epoch 139/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1911\n",
            "Epoch 140/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1908\n",
            "Epoch 141/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1905\n",
            "Epoch 142/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1903\n",
            "Epoch 143/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1900\n",
            "Epoch 144/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1898\n",
            "Epoch 145/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1895\n",
            "Epoch 146/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1893\n",
            "Epoch 147/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1890\n",
            "Epoch 148/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1888\n",
            "Epoch 149/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1886\n",
            "Epoch 150/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1883\n",
            "Epoch 151/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1881\n",
            "Epoch 152/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1879\n",
            "Epoch 153/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1877\n",
            "Epoch 154/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1875\n",
            "Epoch 155/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1873\n",
            "Epoch 156/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1871\n",
            "Epoch 157/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1869\n",
            "Epoch 158/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1867\n",
            "Epoch 159/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1865\n",
            "Epoch 160/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1863\n",
            "Epoch 161/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1861\n",
            "Epoch 162/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1859\n",
            "Epoch 163/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1857\n",
            "Epoch 164/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1855\n",
            "Epoch 165/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1854\n",
            "Epoch 166/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1852\n",
            "Epoch 167/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1850\n",
            "Epoch 168/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1848\n",
            "Epoch 169/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1847\n",
            "Epoch 170/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1845\n",
            "Epoch 171/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1844\n",
            "Epoch 172/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1842\n",
            "Epoch 173/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1840\n",
            "Epoch 174/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1839\n",
            "Epoch 175/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1837\n",
            "Epoch 176/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1836\n",
            "Epoch 177/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1834\n",
            "Epoch 178/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1833\n",
            "Epoch 179/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1831\n",
            "Epoch 180/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1830\n",
            "Epoch 181/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1828\n",
            "Epoch 182/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1827\n",
            "Epoch 183/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1826\n",
            "Epoch 184/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1824\n",
            "Epoch 185/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1823\n",
            "Epoch 186/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1821\n",
            "Epoch 187/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1820\n",
            "Epoch 188/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1819\n",
            "Epoch 189/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1817\n",
            "Epoch 190/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1816\n",
            "Epoch 191/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1815\n",
            "Epoch 192/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1814\n",
            "Epoch 193/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1812\n",
            "Epoch 194/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1811\n",
            "Epoch 195/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1810\n",
            "Epoch 196/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1809\n",
            "Epoch 197/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1807\n",
            "Epoch 198/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1806\n",
            "Epoch 199/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1805\n",
            "Epoch 200/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1804\n",
            "Epoch 201/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1803\n",
            "Epoch 202/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1802\n",
            "Epoch 203/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1800\n",
            "Epoch 204/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1799\n",
            "Epoch 205/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1798\n",
            "Epoch 206/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1797\n",
            "Epoch 207/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1796\n",
            "Epoch 208/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1795\n",
            "Epoch 209/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1794\n",
            "Epoch 210/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1793\n",
            "Epoch 211/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1792\n",
            "Epoch 212/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1791\n",
            "Epoch 213/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1790\n",
            "Epoch 214/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1789\n",
            "Epoch 215/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1788\n",
            "Epoch 216/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1787\n",
            "Epoch 217/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1786\n",
            "Epoch 218/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1785\n",
            "Epoch 219/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1784\n",
            "Epoch 220/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1783\n",
            "Epoch 221/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1782\n",
            "Epoch 222/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1781\n",
            "Epoch 223/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1780\n",
            "Epoch 224/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1779\n",
            "Epoch 225/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1778\n",
            "Epoch 226/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1777\n",
            "Epoch 227/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1776\n",
            "Epoch 228/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1775\n",
            "Epoch 229/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1774\n",
            "Epoch 230/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1773\n",
            "Epoch 231/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1772\n",
            "Epoch 232/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1771\n",
            "Epoch 233/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1770\n",
            "Epoch 234/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1770\n",
            "Epoch 235/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1769\n",
            "Epoch 236/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1768\n",
            "Epoch 237/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1767\n",
            "Epoch 238/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1766\n",
            "Epoch 239/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1765\n",
            "Epoch 240/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1764\n",
            "Epoch 241/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1764\n",
            "Epoch 242/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1763\n",
            "Epoch 243/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1762\n",
            "Epoch 244/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1761\n",
            "Epoch 245/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1760\n",
            "Epoch 246/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1759\n",
            "Epoch 247/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1759\n",
            "Epoch 248/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1758\n",
            "Epoch 249/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1757\n",
            "Epoch 250/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1756\n",
            "Epoch 251/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1755\n",
            "Epoch 252/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1755\n",
            "Epoch 253/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1754\n",
            "Epoch 254/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1753\n",
            "Epoch 255/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1752\n",
            "Epoch 256/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1752\n",
            "Epoch 257/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1751\n",
            "Epoch 258/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1750\n",
            "Epoch 259/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1749\n",
            "Epoch 260/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1749\n",
            "Epoch 261/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1748\n",
            "Epoch 262/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1747\n",
            "Epoch 263/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1746\n",
            "Epoch 264/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1746\n",
            "Epoch 265/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1745\n",
            "Epoch 266/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1744\n",
            "Epoch 267/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1744\n",
            "Epoch 268/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1743\n",
            "Epoch 269/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1742\n",
            "Epoch 270/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1741\n",
            "Epoch 271/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1741\n",
            "Epoch 272/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1740\n",
            "Epoch 273/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1739\n",
            "Epoch 274/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1739\n",
            "Epoch 275/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1738\n",
            "Epoch 276/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1737\n",
            "Epoch 277/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1737\n",
            "Epoch 278/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1736\n",
            "Epoch 279/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1735\n",
            "Epoch 280/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1735\n",
            "Epoch 281/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1734\n",
            "Epoch 282/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1733\n",
            "Epoch 283/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1733\n",
            "Epoch 284/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1732\n",
            "Epoch 285/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1731\n",
            "Epoch 286/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1731\n",
            "Epoch 287/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1730\n",
            "Epoch 288/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1729\n",
            "Epoch 289/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1729\n",
            "Epoch 290/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1728\n",
            "Epoch 291/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1728\n",
            "Epoch 292/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1727\n",
            "Epoch 293/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1726\n",
            "Epoch 294/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1726\n",
            "Epoch 295/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1725\n",
            "Epoch 296/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1724\n",
            "Epoch 297/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1724\n",
            "Epoch 298/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1723\n",
            "Epoch 299/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1723\n",
            "Epoch 300/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1722\n",
            "Epoch 301/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1721\n",
            "Epoch 302/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1721\n",
            "Epoch 303/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1720\n",
            "Epoch 304/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1720\n",
            "Epoch 305/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1719\n",
            "Epoch 306/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1718\n",
            "Epoch 307/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1718\n",
            "Epoch 308/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1717\n",
            "Epoch 309/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1717\n",
            "Epoch 310/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1716\n",
            "Epoch 311/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1715\n",
            "Epoch 312/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1715\n",
            "Epoch 313/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1714\n",
            "Epoch 314/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1714\n",
            "Epoch 315/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1713\n",
            "Epoch 316/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1713\n",
            "Epoch 317/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1712\n",
            "Epoch 318/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1711\n",
            "Epoch 319/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1711\n",
            "Epoch 320/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1710\n",
            "Epoch 321/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1710\n",
            "Epoch 322/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1709\n",
            "Epoch 323/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1709\n",
            "Epoch 324/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1708\n",
            "Epoch 325/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1707\n",
            "Epoch 326/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1707\n",
            "Epoch 327/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1706\n",
            "Epoch 328/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1706\n",
            "Epoch 329/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1705\n",
            "Epoch 330/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1705\n",
            "Epoch 331/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1704\n",
            "Epoch 332/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1703\n",
            "Epoch 333/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1703\n",
            "Epoch 334/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1702\n",
            "Epoch 335/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1702\n",
            "Epoch 336/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1701\n",
            "Epoch 337/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1701\n",
            "Epoch 338/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1700\n",
            "Epoch 339/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1700\n",
            "Epoch 340/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1699\n",
            "Epoch 341/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1699\n",
            "Epoch 342/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1698\n",
            "Epoch 343/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1697\n",
            "Epoch 344/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1697\n",
            "Epoch 345/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1696\n",
            "Epoch 346/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1696\n",
            "Epoch 347/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1695\n",
            "Epoch 348/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1695\n",
            "Epoch 349/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1694\n",
            "Epoch 350/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1694\n",
            "Epoch 351/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1693\n",
            "Epoch 352/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1693\n",
            "Epoch 353/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1692\n",
            "Epoch 354/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1691\n",
            "Epoch 355/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1691\n",
            "Epoch 356/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1690\n",
            "Epoch 357/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1690\n",
            "Epoch 358/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1689\n",
            "Epoch 359/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1689\n",
            "Epoch 360/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1688\n",
            "Epoch 361/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1688\n",
            "Epoch 362/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1687\n",
            "Epoch 363/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1687\n",
            "Epoch 364/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1686\n",
            "Epoch 365/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1685\n",
            "Epoch 366/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1685\n",
            "Epoch 367/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1684\n",
            "Epoch 368/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1684\n",
            "Epoch 369/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1683\n",
            "Epoch 370/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1683\n",
            "Epoch 371/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1682\n",
            "Epoch 372/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1682\n",
            "Epoch 373/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1681\n",
            "Epoch 374/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1681\n",
            "Epoch 375/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1680\n",
            "Epoch 376/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1679\n",
            "Epoch 377/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1679\n",
            "Epoch 378/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1678\n",
            "Epoch 379/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1678\n",
            "Epoch 380/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1677\n",
            "Epoch 381/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1677\n",
            "Epoch 382/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1676\n",
            "Epoch 383/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1676\n",
            "Epoch 384/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1675\n",
            "Epoch 385/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1674\n",
            "Epoch 386/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1674\n",
            "Epoch 387/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1673\n",
            "Epoch 388/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1673\n",
            "Epoch 389/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1672\n",
            "Epoch 390/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1672\n",
            "Epoch 391/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1671\n",
            "Epoch 392/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1670\n",
            "Epoch 393/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1670\n",
            "Epoch 394/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1669\n",
            "Epoch 395/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1669\n",
            "Epoch 396/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1668\n",
            "Epoch 397/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1667\n",
            "Epoch 398/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1667\n",
            "Epoch 399/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1666\n",
            "Epoch 400/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1666\n",
            "Epoch 401/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1665\n",
            "Epoch 402/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1665\n",
            "Epoch 403/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1664\n",
            "Epoch 404/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1663\n",
            "Epoch 405/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1663\n",
            "Epoch 406/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1662\n",
            "Epoch 407/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1662\n",
            "Epoch 408/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1661\n",
            "Epoch 409/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1660\n",
            "Epoch 410/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1660\n",
            "Epoch 411/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1659\n",
            "Epoch 412/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1658\n",
            "Epoch 413/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1658\n",
            "Epoch 414/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1657\n",
            "Epoch 415/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1656\n",
            "Epoch 416/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1656\n",
            "Epoch 417/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1655\n",
            "Epoch 418/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1655\n",
            "Epoch 419/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1654\n",
            "Epoch 420/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1653\n",
            "Epoch 421/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1653\n",
            "Epoch 422/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1652\n",
            "Epoch 423/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1651\n",
            "Epoch 424/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1651\n",
            "Epoch 425/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1650\n",
            "Epoch 426/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1649\n",
            "Epoch 427/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1649\n",
            "Epoch 428/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1648\n",
            "Epoch 429/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1647\n",
            "Epoch 430/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1646\n",
            "Epoch 431/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1646\n",
            "Epoch 432/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1645\n",
            "Epoch 433/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1644\n",
            "Epoch 434/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1644\n",
            "Epoch 435/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1643\n",
            "Epoch 436/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1642\n",
            "Epoch 437/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1641\n",
            "Epoch 438/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1641\n",
            "Epoch 439/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1640\n",
            "Epoch 440/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1639\n",
            "Epoch 441/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1638\n",
            "Epoch 442/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1638\n",
            "Epoch 443/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1637\n",
            "Epoch 444/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1636\n",
            "Epoch 445/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1635\n",
            "Epoch 446/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1634\n",
            "Epoch 447/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1634\n",
            "Epoch 448/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1633\n",
            "Epoch 449/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1632\n",
            "Epoch 450/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1631\n",
            "Epoch 451/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1630\n",
            "Epoch 452/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1629\n",
            "Epoch 453/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1629\n",
            "Epoch 454/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1628\n",
            "Epoch 455/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1627\n",
            "Epoch 456/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1626\n",
            "Epoch 457/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1625\n",
            "Epoch 458/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1624\n",
            "Epoch 459/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1623\n",
            "Epoch 460/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1622\n",
            "Epoch 461/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1621\n",
            "Epoch 462/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1620\n",
            "Epoch 463/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1619\n",
            "Epoch 464/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1618\n",
            "Epoch 465/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1617\n",
            "Epoch 466/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1616\n",
            "Epoch 467/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1615\n",
            "Epoch 468/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1615\n",
            "Epoch 469/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1614\n",
            "Epoch 470/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1613\n",
            "Epoch 471/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1611\n",
            "Epoch 472/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1610\n",
            "Epoch 473/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1609\n",
            "Epoch 474/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1608\n",
            "Epoch 475/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1607\n",
            "Epoch 476/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1606\n",
            "Epoch 477/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1605\n",
            "Epoch 478/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1604\n",
            "Epoch 479/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1603\n",
            "Epoch 480/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1602\n",
            "Epoch 481/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1601\n",
            "Epoch 482/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1600\n",
            "Epoch 483/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1599\n",
            "Epoch 484/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1598\n",
            "Epoch 485/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1597\n",
            "Epoch 486/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1596\n",
            "Epoch 487/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1595\n",
            "Epoch 488/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1594\n",
            "Epoch 489/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1593\n",
            "Epoch 490/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1592\n",
            "Epoch 491/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1591\n",
            "Epoch 492/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1590\n",
            "Epoch 493/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1589\n",
            "Epoch 494/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1588\n",
            "Epoch 495/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1587\n",
            "Epoch 496/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1586\n",
            "Epoch 497/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1585\n",
            "Epoch 498/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1584\n",
            "Epoch 499/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1583\n",
            "Epoch 500/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1582\n",
            "Epoch 501/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1581\n",
            "Epoch 502/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1580\n",
            "Epoch 503/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1579\n",
            "Epoch 504/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1578\n",
            "Epoch 505/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1577\n",
            "Epoch 506/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1576\n",
            "Epoch 507/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1575\n",
            "Epoch 508/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1574\n",
            "Epoch 509/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1573\n",
            "Epoch 510/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1572\n",
            "Epoch 511/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1571\n",
            "Epoch 512/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1570\n",
            "Epoch 513/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1569\n",
            "Epoch 514/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1568\n",
            "Epoch 515/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1567\n",
            "Epoch 516/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1566\n",
            "Epoch 517/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1565\n",
            "Epoch 518/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1564\n",
            "Epoch 519/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1563\n",
            "Epoch 520/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1562\n",
            "Epoch 521/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1561\n",
            "Epoch 522/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1560\n",
            "Epoch 523/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1559\n",
            "Epoch 524/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1558\n",
            "Epoch 525/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1557\n",
            "Epoch 526/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1556\n",
            "Epoch 527/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1555\n",
            "Epoch 528/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1554\n",
            "Epoch 529/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1553\n",
            "Epoch 530/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1552\n",
            "Epoch 531/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1551\n",
            "Epoch 532/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1550\n",
            "Epoch 533/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1549\n",
            "Epoch 534/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1548\n",
            "Epoch 535/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1547\n",
            "Epoch 536/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1546\n",
            "Epoch 537/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1545\n",
            "Epoch 538/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1544\n",
            "Epoch 539/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1543\n",
            "Epoch 540/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1542\n",
            "Epoch 541/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1541\n",
            "Epoch 542/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1540\n",
            "Epoch 543/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1539\n",
            "Epoch 544/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1538\n",
            "Epoch 545/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1537\n",
            "Epoch 546/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1536\n",
            "Epoch 547/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1535\n",
            "Epoch 548/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1534\n",
            "Epoch 549/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1533\n",
            "Epoch 550/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1532\n",
            "Epoch 551/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1531\n",
            "Epoch 552/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1530\n",
            "Epoch 553/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1530\n",
            "Epoch 554/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1529\n",
            "Epoch 555/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1528\n",
            "Epoch 556/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1527\n",
            "Epoch 557/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1526\n",
            "Epoch 558/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1525\n",
            "Epoch 559/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1524\n",
            "Epoch 560/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1523\n",
            "Epoch 561/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1523\n",
            "Epoch 562/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1521\n",
            "Epoch 563/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1519\n",
            "Epoch 564/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1522\n",
            "Epoch 565/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1521\n",
            "Epoch 566/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1516\n",
            "Epoch 567/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1518\n",
            "Epoch 568/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1540\n",
            "Epoch 569/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1531\n",
            "Epoch 570/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1549\n",
            "Epoch 571/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1585\n",
            "Epoch 572/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1634\n",
            "Epoch 573/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1620\n",
            "Epoch 574/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1576\n",
            "Epoch 575/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1579\n",
            "Epoch 576/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1534\n",
            "Epoch 577/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1513\n",
            "Epoch 578/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1510\n",
            "Epoch 579/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1516\n",
            "Epoch 580/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1517\n",
            "Epoch 581/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1508\n",
            "Epoch 582/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1504\n",
            "Epoch 583/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1508\n",
            "Epoch 584/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1515\n",
            "Epoch 585/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1507\n",
            "Epoch 586/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1502\n",
            "Epoch 587/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1503\n",
            "Epoch 588/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1520\n",
            "Epoch 589/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1510\n",
            "Epoch 590/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1503\n",
            "Epoch 591/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1504\n",
            "Epoch 592/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1534\n",
            "Epoch 593/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1522\n",
            "Epoch 594/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1512\n",
            "Epoch 595/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1516\n",
            "Epoch 596/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1545\n",
            "Epoch 597/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1532\n",
            "Epoch 598/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1511\n",
            "Epoch 599/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1512\n",
            "Epoch 600/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1524\n",
            "Epoch 601/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1533\n",
            "Epoch 602/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1501\n",
            "Epoch 603/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1501\n",
            "Epoch 604/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1501\n",
            "Epoch 605/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1528\n",
            "Epoch 606/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1497\n",
            "Epoch 607/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1495\n",
            "Epoch 608/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1491\n",
            "Epoch 609/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1520\n",
            "Epoch 610/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1499\n",
            "Epoch 611/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1491\n",
            "Epoch 612/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1490\n",
            "Epoch 613/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1510\n",
            "Epoch 614/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1507\n",
            "Epoch 615/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1489\n",
            "Epoch 616/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1493\n",
            "Epoch 617/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1501\n",
            "Epoch 618/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1521\n",
            "Epoch 619/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1488\n",
            "Epoch 620/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1493\n",
            "Epoch 621/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1492\n",
            "Epoch 622/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1528\n",
            "Epoch 623/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1488\n",
            "Epoch 624/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1489\n",
            "Epoch 625/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1486\n",
            "Epoch 626/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1520\n",
            "Epoch 627/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1491\n",
            "Epoch 628/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1483\n",
            "Epoch 629/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1486\n",
            "Epoch 630/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1505\n",
            "Epoch 631/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1500\n",
            "Epoch 632/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1479\n",
            "Epoch 633/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1486\n",
            "Epoch 634/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1491\n",
            "Epoch 635/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1511\n",
            "Epoch 636/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1477\n",
            "Epoch 637/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1483\n",
            "Epoch 638/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1482\n",
            "Epoch 639/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1513\n",
            "Epoch 640/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1477\n",
            "Epoch 641/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1478\n",
            "Epoch 642/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1479\n",
            "Epoch 643/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1505\n",
            "Epoch 644/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1483\n",
            "Epoch 645/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1473\n",
            "Epoch 646/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1480\n",
            "Epoch 647/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1493\n",
            "Epoch 648/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1494\n",
            "Epoch 649/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1469\n",
            "Epoch 650/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1481\n",
            "Epoch 651/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1482\n",
            "Epoch 652/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1505\n",
            "Epoch 653/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1468\n",
            "Epoch 654/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1477\n",
            "Epoch 655/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1475\n",
            "Epoch 656/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1506\n",
            "Epoch 657/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1469\n",
            "Epoch 658/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1470\n",
            "Epoch 659/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1474\n",
            "Epoch 660/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1497\n",
            "Epoch 661/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1474\n",
            "Epoch 662/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1464\n",
            "Epoch 663/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1475\n",
            "Epoch 664/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1485\n",
            "Epoch 665/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1485\n",
            "Epoch 666/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 667/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1475\n",
            "Epoch 668/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1475\n",
            "Epoch 669/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1496\n",
            "Epoch 670/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1459\n",
            "Epoch 671/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1470\n",
            "Epoch 672/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1470\n",
            "Epoch 673/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1497\n",
            "Epoch 674/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 675/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1463\n",
            "Epoch 676/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1470\n",
            "Epoch 677/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1489\n",
            "Epoch 678/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1466\n",
            "Epoch 679/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1457\n",
            "Epoch 680/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1472\n",
            "Epoch 681/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1478\n",
            "Epoch 682/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1476\n",
            "Epoch 683/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1453\n",
            "Epoch 684/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1471\n",
            "Epoch 685/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1470\n",
            "Epoch 686/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1487\n",
            "Epoch 687/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 688/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1466\n",
            "Epoch 689/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1466\n",
            "Epoch 690/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1489\n",
            "Epoch 691/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 692/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1458\n",
            "Epoch 693/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1465\n",
            "Epoch 694/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1484\n",
            "Epoch 695/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1455\n",
            "Epoch 696/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 697/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1468\n",
            "Epoch 698/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1475\n",
            "Epoch 699/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 700/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1447\n",
            "Epoch 701/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1469\n",
            "Epoch 702/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1467\n",
            "Epoch 703/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1474\n",
            "Epoch 704/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1445\n",
            "Epoch 705/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1466\n",
            "Epoch 706/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1462\n",
            "Epoch 707/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1479\n",
            "Epoch 708/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1443\n",
            "Epoch 709/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1459\n",
            "Epoch 710/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1461\n",
            "Epoch 711/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1479\n",
            "Epoch 712/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1444\n",
            "Epoch 713/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 714/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1463\n",
            "Epoch 715/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1473\n",
            "Epoch 716/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1448\n",
            "Epoch 717/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1445\n",
            "Epoch 718/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1467\n",
            "Epoch 719/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1467\n",
            "Epoch 720/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1455\n",
            "Epoch 721/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1442\n",
            "Epoch 722/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1469\n",
            "Epoch 723/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1462\n",
            "Epoch 724/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1461\n",
            "Epoch 725/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1439\n",
            "Epoch 726/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1466\n",
            "Epoch 727/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1458\n",
            "Epoch 728/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 729/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1437\n",
            "Epoch 730/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 731/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1456\n",
            "Epoch 732/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1465\n",
            "Epoch 733/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1435\n",
            "Epoch 734/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 735/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1456\n",
            "Epoch 736/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 737/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1435\n",
            "Epoch 738/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1446\n",
            "Epoch 739/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 740/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1463\n",
            "Epoch 741/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1437\n",
            "Epoch 742/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1442\n",
            "Epoch 743/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1466\n",
            "Epoch 744/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1463\n",
            "Epoch 745/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1441\n",
            "Epoch 746/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1441\n",
            "Epoch 747/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1473\n",
            "Epoch 748/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1462\n",
            "Epoch 749/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1443\n",
            "Epoch 750/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1441\n",
            "Epoch 751/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1475\n",
            "Epoch 752/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 753/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1443\n",
            "Epoch 754/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1439\n",
            "Epoch 755/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1471\n",
            "Epoch 756/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1457\n",
            "Epoch 757/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1441\n",
            "Epoch 758/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1436\n",
            "Epoch 759/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 760/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1453\n",
            "Epoch 761/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1439\n",
            "Epoch 762/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1434\n",
            "Epoch 763/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1458\n",
            "Epoch 764/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1449\n",
            "Epoch 765/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1438\n",
            "Epoch 766/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1432\n",
            "Epoch 767/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1453\n",
            "Epoch 768/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1447\n",
            "Epoch 769/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1436\n",
            "Epoch 770/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1430\n",
            "Epoch 771/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 772/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1446\n",
            "Epoch 773/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1436\n",
            "Epoch 774/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1428\n",
            "Epoch 775/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1450\n",
            "Epoch 776/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1446\n",
            "Epoch 777/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1436\n",
            "Epoch 778/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1427\n",
            "Epoch 779/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 780/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1447\n",
            "Epoch 781/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1436\n",
            "Epoch 782/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1426\n",
            "Epoch 783/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1455\n",
            "Epoch 784/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1449\n",
            "Epoch 785/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1438\n",
            "Epoch 786/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1425\n",
            "Epoch 787/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1459\n",
            "Epoch 788/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 789/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1441\n",
            "Epoch 790/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1425\n",
            "Epoch 791/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1461\n",
            "Epoch 792/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 793/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1444\n",
            "Epoch 794/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1423\n",
            "Epoch 795/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1457\n",
            "Epoch 796/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 797/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1445\n",
            "Epoch 798/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1421\n",
            "Epoch 799/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1449\n",
            "Epoch 800/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 801/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1444\n",
            "Epoch 802/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 803/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1441\n",
            "Epoch 804/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1452\n",
            "Epoch 805/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1442\n",
            "Epoch 806/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1419\n",
            "Epoch 807/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1435\n",
            "Epoch 808/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1454\n",
            "Epoch 809/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1442\n",
            "Epoch 810/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 811/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1432\n",
            "Epoch 812/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1456\n",
            "Epoch 813/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1442\n",
            "Epoch 814/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1419\n",
            "Epoch 815/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1431\n",
            "Epoch 816/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1460\n",
            "Epoch 817/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1443\n",
            "Epoch 818/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 819/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1430\n",
            "Epoch 820/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1464\n",
            "Epoch 821/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1445\n",
            "Epoch 822/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 823/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1430\n",
            "Epoch 824/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1466\n",
            "Epoch 825/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1445\n",
            "Epoch 826/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 827/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1429\n",
            "Epoch 828/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1464\n",
            "Epoch 829/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1443\n",
            "Epoch 830/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1418\n",
            "Epoch 831/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1427\n",
            "Epoch 832/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1458\n",
            "Epoch 833/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1440\n",
            "Epoch 834/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1418\n",
            "Epoch 835/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1425\n",
            "Epoch 836/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1451\n",
            "Epoch 837/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1437\n",
            "Epoch 838/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1418\n",
            "Epoch 839/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1423\n",
            "Epoch 840/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1445\n",
            "Epoch 841/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1434\n",
            "Epoch 842/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1417\n",
            "Epoch 843/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1421\n",
            "Epoch 844/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1440\n",
            "Epoch 845/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1431\n",
            "Epoch 846/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1417\n",
            "Epoch 847/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1419\n",
            "Epoch 848/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1437\n",
            "Epoch 849/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1429\n",
            "Epoch 850/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1415\n",
            "Epoch 851/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1418\n",
            "Epoch 852/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1436\n",
            "Epoch 853/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1428\n",
            "Epoch 854/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1414\n",
            "Epoch 855/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1417\n",
            "Epoch 856/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1437\n",
            "Epoch 857/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1427\n",
            "Epoch 858/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1411\n",
            "Epoch 859/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1416\n",
            "Epoch 860/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1441\n",
            "Epoch 861/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1427\n",
            "Epoch 862/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1409\n",
            "Epoch 863/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1417\n",
            "Epoch 864/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1449\n",
            "Epoch 865/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1428\n",
            "Epoch 866/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1408\n",
            "Epoch 867/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1420\n",
            "Epoch 868/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1463\n",
            "Epoch 869/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1432\n",
            "Epoch 870/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1409\n",
            "Epoch 871/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1426\n",
            "Epoch 872/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1481\n",
            "Epoch 873/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1440\n",
            "Epoch 874/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1413\n",
            "Epoch 875/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1434\n",
            "Epoch 876/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1499\n",
            "Epoch 877/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1449\n",
            "Epoch 878/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1416\n",
            "Epoch 879/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1437\n",
            "Epoch 880/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1493\n",
            "Epoch 881/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1448\n",
            "Epoch 882/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1412\n",
            "Epoch 883/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1430\n",
            "Epoch 884/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1458\n",
            "Epoch 885/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1439\n",
            "Epoch 886/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1415\n",
            "Epoch 887/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1422\n",
            "Epoch 888/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1436\n",
            "Epoch 889/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1429\n",
            "Epoch 890/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1419\n",
            "Epoch 891/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1421\n",
            "Epoch 892/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1426\n",
            "Epoch 893/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1424\n",
            "Epoch 894/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1420\n",
            "Epoch 895/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1420\n",
            "Epoch 896/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1422\n",
            "Epoch 897/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1421\n",
            "Epoch 898/2000\n",
            "15746/15746 [==============================] - ETA: 0s - loss: 0.150 - 0s 6us/step - loss: 0.1419\n",
            "Epoch 899/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 900/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 901/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1419\n",
            "Epoch 902/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1417\n",
            "Epoch 903/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1417\n",
            "Epoch 904/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1417\n",
            "Epoch 905/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1417\n",
            "Epoch 906/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1416\n",
            "Epoch 907/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1415\n",
            "Epoch 908/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1415\n",
            "Epoch 909/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1415\n",
            "Epoch 910/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1414\n",
            "Epoch 911/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1413\n",
            "Epoch 912/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1413\n",
            "Epoch 913/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1413\n",
            "Epoch 914/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1412\n",
            "Epoch 915/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1412\n",
            "Epoch 916/2000\n",
            "15746/15746 [==============================] - 0s 5us/step - loss: 0.1412\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00916: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1461accc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IkNiwfvihDM8",
        "colab": {},
        "outputId": "a0286904-e758-4b3f-b42a-91997ba17b53"
      },
      "source": [
        "#train optimized model\n",
        "alpharnn = Alpha_Rnn(10,0)\n",
        "alpharnn.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=1500,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "15746/15746 [==============================] - 3s 163us/step - loss: 1.1334\n",
            "Epoch 2/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1308\n",
            "Epoch 3/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1291\n",
            "Epoch 4/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1275\n",
            "Epoch 5/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1257\n",
            "Epoch 6/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1237\n",
            "Epoch 7/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1214\n",
            "Epoch 8/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1190\n",
            "Epoch 9/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 1.1164\n",
            "Epoch 10/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1138\n",
            "Epoch 11/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1114\n",
            "Epoch 12/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1091\n",
            "Epoch 13/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1070\n",
            "Epoch 14/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1051\n",
            "Epoch 15/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1034\n",
            "Epoch 16/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1018\n",
            "Epoch 17/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.1003\n",
            "Epoch 18/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 1.0988\n",
            "Epoch 19/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 1.0971\n",
            "Epoch 20/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 1.0953\n",
            "Epoch 21/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.0929\n",
            "Epoch 22/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.0897\n",
            "Epoch 23/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 1.0847\n",
            "Epoch 24/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 1.0766\n",
            "Epoch 25/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 1.0615\n",
            "Epoch 26/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 1.0301\n",
            "Epoch 27/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.9564\n",
            "Epoch 28/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.7939\n",
            "Epoch 29/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.6012\n",
            "Epoch 30/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.5072\n",
            "Epoch 31/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4820\n",
            "Epoch 32/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4742\n",
            "Epoch 33/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4658\n",
            "Epoch 34/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4578\n",
            "Epoch 35/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4507\n",
            "Epoch 36/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4437\n",
            "Epoch 37/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4368\n",
            "Epoch 38/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4302\n",
            "Epoch 39/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4240\n",
            "Epoch 40/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.4183\n",
            "Epoch 41/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.4130\n",
            "Epoch 42/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4082\n",
            "Epoch 43/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.4036\n",
            "Epoch 44/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3993\n",
            "Epoch 45/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3953\n",
            "Epoch 46/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3916\n",
            "Epoch 47/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3879\n",
            "Epoch 48/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3845\n",
            "Epoch 49/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3811\n",
            "Epoch 50/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3779\n",
            "Epoch 51/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3747\n",
            "Epoch 52/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3716\n",
            "Epoch 53/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3686\n",
            "Epoch 54/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3655\n",
            "Epoch 55/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3624\n",
            "Epoch 56/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3594\n",
            "Epoch 57/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3563\n",
            "Epoch 58/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3531\n",
            "Epoch 59/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3498\n",
            "Epoch 60/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3464\n",
            "Epoch 61/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.3428\n",
            "Epoch 62/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.3390\n",
            "Epoch 63/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3350\n",
            "Epoch 64/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3306\n",
            "Epoch 65/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3259\n",
            "Epoch 66/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3207\n",
            "Epoch 67/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3151\n",
            "Epoch 68/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3092\n",
            "Epoch 69/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.3031\n",
            "Epoch 70/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2971\n",
            "Epoch 71/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2914\n",
            "Epoch 72/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2862\n",
            "Epoch 73/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2814\n",
            "Epoch 74/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2771\n",
            "Epoch 75/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2731\n",
            "Epoch 76/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2693\n",
            "Epoch 77/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.2656\n",
            "Epoch 78/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2622\n",
            "Epoch 79/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2590\n",
            "Epoch 80/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2563\n",
            "Epoch 81/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2539\n",
            "Epoch 82/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2519\n",
            "Epoch 83/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2502\n",
            "Epoch 84/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2487\n",
            "Epoch 85/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2474\n",
            "Epoch 86/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2464\n",
            "Epoch 87/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2456\n",
            "Epoch 88/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2450\n",
            "Epoch 89/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2447\n",
            "Epoch 90/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2445\n",
            "Epoch 91/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2444\n",
            "Epoch 92/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2444\n",
            "Epoch 93/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2440\n",
            "Epoch 94/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2432\n",
            "Epoch 95/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.2419\n",
            "Epoch 96/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2405\n",
            "Epoch 97/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2392\n",
            "Epoch 98/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2381\n",
            "Epoch 99/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2373\n",
            "Epoch 100/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2366\n",
            "Epoch 101/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2360\n",
            "Epoch 102/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2354\n",
            "Epoch 103/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2348\n",
            "Epoch 104/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2342\n",
            "Epoch 105/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2335\n",
            "Epoch 106/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2329\n",
            "Epoch 107/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2322\n",
            "Epoch 108/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2316\n",
            "Epoch 109/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2310\n",
            "Epoch 110/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2304\n",
            "Epoch 111/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2297\n",
            "Epoch 112/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2291\n",
            "Epoch 113/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.2285\n",
            "Epoch 114/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2278\n",
            "Epoch 115/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2272\n",
            "Epoch 116/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2265\n",
            "Epoch 117/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2259\n",
            "Epoch 118/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2252\n",
            "Epoch 119/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2245\n",
            "Epoch 120/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2239\n",
            "Epoch 121/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2232\n",
            "Epoch 122/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2225\n",
            "Epoch 123/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2218\n",
            "Epoch 124/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2210\n",
            "Epoch 125/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2203\n",
            "Epoch 126/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2195\n",
            "Epoch 127/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2188\n",
            "Epoch 128/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2180\n",
            "Epoch 129/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2172\n",
            "Epoch 130/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2164\n",
            "Epoch 131/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2156\n",
            "Epoch 132/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2148\n",
            "Epoch 133/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2140\n",
            "Epoch 134/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2132\n",
            "Epoch 135/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2123\n",
            "Epoch 136/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2115\n",
            "Epoch 137/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2106\n",
            "Epoch 138/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2098\n",
            "Epoch 139/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2089\n",
            "Epoch 140/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2081\n",
            "Epoch 141/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2073\n",
            "Epoch 142/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2064\n",
            "Epoch 143/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.2056\n",
            "Epoch 144/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2048\n",
            "Epoch 145/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2040\n",
            "Epoch 146/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2033\n",
            "Epoch 147/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2025\n",
            "Epoch 148/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2018\n",
            "Epoch 149/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2011\n",
            "Epoch 150/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.2004\n",
            "Epoch 151/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1997\n",
            "Epoch 152/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1991\n",
            "Epoch 153/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1985\n",
            "Epoch 154/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1979\n",
            "Epoch 155/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1974\n",
            "Epoch 156/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1968\n",
            "Epoch 157/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1963\n",
            "Epoch 158/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1958\n",
            "Epoch 159/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1953\n",
            "Epoch 160/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1949\n",
            "Epoch 161/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1944\n",
            "Epoch 162/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1940\n",
            "Epoch 163/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1936\n",
            "Epoch 164/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1932\n",
            "Epoch 165/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1928\n",
            "Epoch 166/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1925\n",
            "Epoch 167/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1921\n",
            "Epoch 168/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1918\n",
            "Epoch 169/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1915\n",
            "Epoch 170/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1911\n",
            "Epoch 171/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1908\n",
            "Epoch 172/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1905\n",
            "Epoch 173/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1903\n",
            "Epoch 174/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1900\n",
            "Epoch 175/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1897\n",
            "Epoch 176/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1895\n",
            "Epoch 177/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1892\n",
            "Epoch 178/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1890\n",
            "Epoch 179/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1887\n",
            "Epoch 180/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1885\n",
            "Epoch 181/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1883\n",
            "Epoch 182/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1881\n",
            "Epoch 183/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1879\n",
            "Epoch 184/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1876\n",
            "Epoch 185/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1874\n",
            "Epoch 186/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1872\n",
            "Epoch 187/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1871\n",
            "Epoch 188/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1869\n",
            "Epoch 189/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1867\n",
            "Epoch 190/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1865\n",
            "Epoch 191/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1863\n",
            "Epoch 192/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1861\n",
            "Epoch 193/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1860\n",
            "Epoch 194/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1858\n",
            "Epoch 195/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1856\n",
            "Epoch 196/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1855\n",
            "Epoch 197/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1853\n",
            "Epoch 198/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1852\n",
            "Epoch 199/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1850\n",
            "Epoch 200/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1849\n",
            "Epoch 201/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1847\n",
            "Epoch 202/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1846\n",
            "Epoch 203/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1844\n",
            "Epoch 204/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1843\n",
            "Epoch 205/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1841\n",
            "Epoch 206/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1840\n",
            "Epoch 207/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1839\n",
            "Epoch 208/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1837\n",
            "Epoch 209/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1836\n",
            "Epoch 210/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1835\n",
            "Epoch 211/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1834\n",
            "Epoch 212/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1832\n",
            "Epoch 213/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1831\n",
            "Epoch 214/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1830\n",
            "Epoch 215/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1829\n",
            "Epoch 216/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1827\n",
            "Epoch 217/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1826\n",
            "Epoch 218/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1825\n",
            "Epoch 219/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1824\n",
            "Epoch 220/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1823\n",
            "Epoch 221/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1822\n",
            "Epoch 222/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1821\n",
            "Epoch 223/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1819\n",
            "Epoch 224/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1818\n",
            "Epoch 225/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1817\n",
            "Epoch 226/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1816\n",
            "Epoch 227/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1815\n",
            "Epoch 228/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1814\n",
            "Epoch 229/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1813\n",
            "Epoch 230/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1812\n",
            "Epoch 231/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1811\n",
            "Epoch 232/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1810\n",
            "Epoch 233/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1809\n",
            "Epoch 234/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1808\n",
            "Epoch 235/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1807\n",
            "Epoch 236/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1806\n",
            "Epoch 237/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1805\n",
            "Epoch 238/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1804\n",
            "Epoch 239/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1803\n",
            "Epoch 240/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1802\n",
            "Epoch 241/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1801\n",
            "Epoch 242/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1800\n",
            "Epoch 243/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1799\n",
            "Epoch 244/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1798\n",
            "Epoch 245/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1797\n",
            "Epoch 246/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1796\n",
            "Epoch 247/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1796\n",
            "Epoch 248/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1795\n",
            "Epoch 249/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1794\n",
            "Epoch 250/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1793\n",
            "Epoch 251/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1792\n",
            "Epoch 252/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1791\n",
            "Epoch 253/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1790\n",
            "Epoch 254/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1789\n",
            "Epoch 255/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1788\n",
            "Epoch 256/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1788\n",
            "Epoch 257/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1787\n",
            "Epoch 258/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1786\n",
            "Epoch 259/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1785\n",
            "Epoch 260/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1784\n",
            "Epoch 261/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1783\n",
            "Epoch 262/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1782\n",
            "Epoch 263/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1782\n",
            "Epoch 264/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1781\n",
            "Epoch 265/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1780\n",
            "Epoch 266/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1779\n",
            "Epoch 267/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1778\n",
            "Epoch 268/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1778\n",
            "Epoch 269/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1777\n",
            "Epoch 270/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1776\n",
            "Epoch 271/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1775\n",
            "Epoch 272/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1774\n",
            "Epoch 273/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1773\n",
            "Epoch 274/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1773\n",
            "Epoch 275/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1772\n",
            "Epoch 276/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1771\n",
            "Epoch 277/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1770\n",
            "Epoch 278/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1770\n",
            "Epoch 279/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1769\n",
            "Epoch 280/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1768\n",
            "Epoch 281/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1767\n",
            "Epoch 282/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1766\n",
            "Epoch 283/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1766\n",
            "Epoch 284/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1765\n",
            "Epoch 285/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1764\n",
            "Epoch 286/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1763\n",
            "Epoch 287/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1763\n",
            "Epoch 288/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1762\n",
            "Epoch 289/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1761\n",
            "Epoch 290/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1760\n",
            "Epoch 291/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1760\n",
            "Epoch 292/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1759\n",
            "Epoch 293/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1758\n",
            "Epoch 294/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1757\n",
            "Epoch 295/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1757\n",
            "Epoch 296/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1756\n",
            "Epoch 297/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1755\n",
            "Epoch 298/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1754\n",
            "Epoch 299/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1754\n",
            "Epoch 300/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1753\n",
            "Epoch 301/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1752\n",
            "Epoch 302/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1751\n",
            "Epoch 303/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1751\n",
            "Epoch 304/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1750\n",
            "Epoch 305/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1749\n",
            "Epoch 306/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1748\n",
            "Epoch 307/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1748\n",
            "Epoch 308/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1747\n",
            "Epoch 309/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1746\n",
            "Epoch 310/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1745\n",
            "Epoch 311/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1745\n",
            "Epoch 312/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1744\n",
            "Epoch 313/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1743\n",
            "Epoch 314/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1743\n",
            "Epoch 315/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1742\n",
            "Epoch 316/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1741\n",
            "Epoch 317/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1740\n",
            "Epoch 318/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1740\n",
            "Epoch 319/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1739\n",
            "Epoch 320/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1738\n",
            "Epoch 321/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1738\n",
            "Epoch 322/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1737\n",
            "Epoch 323/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1736\n",
            "Epoch 324/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1735\n",
            "Epoch 325/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1735\n",
            "Epoch 326/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1734\n",
            "Epoch 327/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1733\n",
            "Epoch 328/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1733\n",
            "Epoch 329/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1732\n",
            "Epoch 330/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1731\n",
            "Epoch 331/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1730\n",
            "Epoch 332/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1730\n",
            "Epoch 333/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1729\n",
            "Epoch 334/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1728\n",
            "Epoch 335/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1728\n",
            "Epoch 336/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1727\n",
            "Epoch 337/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1726\n",
            "Epoch 338/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1725\n",
            "Epoch 339/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1725\n",
            "Epoch 340/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1724\n",
            "Epoch 341/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1723\n",
            "Epoch 342/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1723\n",
            "Epoch 343/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1722\n",
            "Epoch 344/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1721\n",
            "Epoch 345/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1720\n",
            "Epoch 346/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1720\n",
            "Epoch 347/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1719\n",
            "Epoch 348/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1718\n",
            "Epoch 349/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1718\n",
            "Epoch 350/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1717\n",
            "Epoch 351/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1716\n",
            "Epoch 352/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1715\n",
            "Epoch 353/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1715\n",
            "Epoch 354/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1714\n",
            "Epoch 355/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1713\n",
            "Epoch 356/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1713\n",
            "Epoch 357/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1712\n",
            "Epoch 358/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1711\n",
            "Epoch 359/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1710\n",
            "Epoch 360/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1710\n",
            "Epoch 361/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1709\n",
            "Epoch 362/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1708\n",
            "Epoch 363/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1707\n",
            "Epoch 364/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1707\n",
            "Epoch 365/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1706\n",
            "Epoch 366/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1705\n",
            "Epoch 367/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1705\n",
            "Epoch 368/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1704\n",
            "Epoch 369/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1703\n",
            "Epoch 370/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1702\n",
            "Epoch 371/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1702\n",
            "Epoch 372/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1701\n",
            "Epoch 373/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1700\n",
            "Epoch 374/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1700\n",
            "Epoch 375/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1699\n",
            "Epoch 376/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1698\n",
            "Epoch 377/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1697\n",
            "Epoch 378/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1697\n",
            "Epoch 379/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1696\n",
            "Epoch 380/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1695\n",
            "Epoch 381/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1694\n",
            "Epoch 382/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1694\n",
            "Epoch 383/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1693\n",
            "Epoch 384/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1692\n",
            "Epoch 385/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1691\n",
            "Epoch 386/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1691\n",
            "Epoch 387/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1690\n",
            "Epoch 388/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1689\n",
            "Epoch 389/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1688\n",
            "Epoch 390/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1688\n",
            "Epoch 391/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1687\n",
            "Epoch 392/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1686\n",
            "Epoch 393/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1685\n",
            "Epoch 394/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1685\n",
            "Epoch 395/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1684\n",
            "Epoch 396/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1683\n",
            "Epoch 397/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1682\n",
            "Epoch 398/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1682\n",
            "Epoch 399/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1681\n",
            "Epoch 400/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1680\n",
            "Epoch 401/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1679\n",
            "Epoch 402/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1679\n",
            "Epoch 403/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1678\n",
            "Epoch 404/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1677\n",
            "Epoch 405/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1676\n",
            "Epoch 406/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1676\n",
            "Epoch 407/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1675\n",
            "Epoch 408/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1674\n",
            "Epoch 409/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1673\n",
            "Epoch 410/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1673\n",
            "Epoch 411/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1672\n",
            "Epoch 412/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1671\n",
            "Epoch 413/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1670\n",
            "Epoch 414/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1670\n",
            "Epoch 415/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1669\n",
            "Epoch 416/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1668\n",
            "Epoch 417/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1667\n",
            "Epoch 418/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1667\n",
            "Epoch 419/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1666\n",
            "Epoch 420/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1665\n",
            "Epoch 421/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1664\n",
            "Epoch 422/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1663\n",
            "Epoch 423/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1663\n",
            "Epoch 424/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1662\n",
            "Epoch 425/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1661\n",
            "Epoch 426/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1660\n",
            "Epoch 427/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1660\n",
            "Epoch 428/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1659\n",
            "Epoch 429/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1658\n",
            "Epoch 430/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1657\n",
            "Epoch 431/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1656\n",
            "Epoch 432/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1656\n",
            "Epoch 433/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1655\n",
            "Epoch 434/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1654\n",
            "Epoch 435/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1653\n",
            "Epoch 436/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1653\n",
            "Epoch 437/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1652\n",
            "Epoch 438/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1651\n",
            "Epoch 439/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1650\n",
            "Epoch 440/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1649\n",
            "Epoch 441/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1649\n",
            "Epoch 442/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1648\n",
            "Epoch 443/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1647\n",
            "Epoch 444/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1646\n",
            "Epoch 445/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1645\n",
            "Epoch 446/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1645\n",
            "Epoch 447/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1644\n",
            "Epoch 448/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1643\n",
            "Epoch 449/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1642\n",
            "Epoch 450/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1641\n",
            "Epoch 451/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1641\n",
            "Epoch 452/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1640\n",
            "Epoch 453/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1639\n",
            "Epoch 454/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1638\n",
            "Epoch 455/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1637\n",
            "Epoch 456/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1637\n",
            "Epoch 457/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1636\n",
            "Epoch 458/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1635\n",
            "Epoch 459/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1634\n",
            "Epoch 460/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1633\n",
            "Epoch 461/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1633\n",
            "Epoch 462/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1632\n",
            "Epoch 463/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1631\n",
            "Epoch 464/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1630\n",
            "Epoch 465/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1629\n",
            "Epoch 466/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1629\n",
            "Epoch 467/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1628\n",
            "Epoch 468/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1627\n",
            "Epoch 469/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1626\n",
            "Epoch 470/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1625\n",
            "Epoch 471/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1625\n",
            "Epoch 472/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1624\n",
            "Epoch 473/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1623\n",
            "Epoch 474/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1622\n",
            "Epoch 475/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1621\n",
            "Epoch 476/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1621\n",
            "Epoch 477/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1620\n",
            "Epoch 478/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1619\n",
            "Epoch 479/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1618\n",
            "Epoch 480/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1617\n",
            "Epoch 481/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1616\n",
            "Epoch 482/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1616\n",
            "Epoch 483/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1615\n",
            "Epoch 484/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1614\n",
            "Epoch 485/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1613\n",
            "Epoch 486/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1612\n",
            "Epoch 487/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1612\n",
            "Epoch 488/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1611\n",
            "Epoch 489/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1610\n",
            "Epoch 490/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1609\n",
            "Epoch 491/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1608\n",
            "Epoch 492/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1607\n",
            "Epoch 493/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1607\n",
            "Epoch 494/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1606\n",
            "Epoch 495/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1605\n",
            "Epoch 496/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1604\n",
            "Epoch 497/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1603\n",
            "Epoch 498/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1602\n",
            "Epoch 499/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1602\n",
            "Epoch 500/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1601\n",
            "Epoch 501/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1600\n",
            "Epoch 502/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1599\n",
            "Epoch 503/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1598\n",
            "Epoch 504/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1597\n",
            "Epoch 505/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1597\n",
            "Epoch 506/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1596\n",
            "Epoch 507/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1595\n",
            "Epoch 508/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1594\n",
            "Epoch 509/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1593\n",
            "Epoch 510/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1592\n",
            "Epoch 511/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1592\n",
            "Epoch 512/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1591\n",
            "Epoch 513/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1590\n",
            "Epoch 514/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1589\n",
            "Epoch 515/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1588\n",
            "Epoch 516/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1587\n",
            "Epoch 517/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1586\n",
            "Epoch 518/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1586\n",
            "Epoch 519/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1585\n",
            "Epoch 520/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1584\n",
            "Epoch 521/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1583\n",
            "Epoch 522/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1582\n",
            "Epoch 523/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1581\n",
            "Epoch 524/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1581\n",
            "Epoch 525/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1580\n",
            "Epoch 526/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1579\n",
            "Epoch 527/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1578\n",
            "Epoch 528/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1577\n",
            "Epoch 529/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1576\n",
            "Epoch 530/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1575\n",
            "Epoch 531/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1575\n",
            "Epoch 532/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1574\n",
            "Epoch 533/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1573\n",
            "Epoch 534/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1572\n",
            "Epoch 535/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1571\n",
            "Epoch 536/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1570\n",
            "Epoch 537/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1570\n",
            "Epoch 538/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1569\n",
            "Epoch 539/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1568\n",
            "Epoch 540/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1567\n",
            "Epoch 541/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1566\n",
            "Epoch 542/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1565\n",
            "Epoch 543/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1564\n",
            "Epoch 544/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1564\n",
            "Epoch 545/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1563\n",
            "Epoch 546/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1562\n",
            "Epoch 547/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1561\n",
            "Epoch 548/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1560\n",
            "Epoch 549/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1559\n",
            "Epoch 550/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1559\n",
            "Epoch 551/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1558\n",
            "Epoch 552/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1557\n",
            "Epoch 553/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 554/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1555\n",
            "Epoch 555/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1554\n",
            "Epoch 556/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1553\n",
            "Epoch 557/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1553\n",
            "Epoch 558/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1552\n",
            "Epoch 559/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1551\n",
            "Epoch 560/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1550\n",
            "Epoch 561/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1549\n",
            "Epoch 562/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1548\n",
            "Epoch 563/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1547\n",
            "Epoch 564/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1547\n",
            "Epoch 565/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1546\n",
            "Epoch 566/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1545\n",
            "Epoch 567/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1544\n",
            "Epoch 568/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1543\n",
            "Epoch 569/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1542\n",
            "Epoch 570/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1542\n",
            "Epoch 571/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1541\n",
            "Epoch 572/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1540\n",
            "Epoch 573/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1539\n",
            "Epoch 574/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1538\n",
            "Epoch 575/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1537\n",
            "Epoch 576/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1536\n",
            "Epoch 577/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1536\n",
            "Epoch 578/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1535\n",
            "Epoch 579/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1534\n",
            "Epoch 580/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1533\n",
            "Epoch 581/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1532\n",
            "Epoch 582/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1531\n",
            "Epoch 583/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1531\n",
            "Epoch 584/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1530\n",
            "Epoch 585/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1529\n",
            "Epoch 586/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1528\n",
            "Epoch 587/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1527\n",
            "Epoch 588/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1526\n",
            "Epoch 589/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1525\n",
            "Epoch 590/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1525\n",
            "Epoch 591/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1524\n",
            "Epoch 592/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1523\n",
            "Epoch 593/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1522\n",
            "Epoch 594/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1521\n",
            "Epoch 595/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1520\n",
            "Epoch 596/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1520\n",
            "Epoch 597/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1519\n",
            "Epoch 598/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1518\n",
            "Epoch 599/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1517\n",
            "Epoch 600/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1516\n",
            "Epoch 601/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1516\n",
            "Epoch 602/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1515\n",
            "Epoch 603/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1514\n",
            "Epoch 604/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1513\n",
            "Epoch 605/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1512\n",
            "Epoch 606/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1511\n",
            "Epoch 607/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1511\n",
            "Epoch 608/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1510\n",
            "Epoch 609/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1509\n",
            "Epoch 610/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1508\n",
            "Epoch 611/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1507\n",
            "Epoch 612/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1507\n",
            "Epoch 613/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1506\n",
            "Epoch 614/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1505\n",
            "Epoch 615/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1504\n",
            "Epoch 616/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1503\n",
            "Epoch 617/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1503\n",
            "Epoch 618/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1502\n",
            "Epoch 619/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1501\n",
            "Epoch 620/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1500\n",
            "Epoch 621/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1499\n",
            "Epoch 622/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1499\n",
            "Epoch 623/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1498\n",
            "Epoch 624/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1497\n",
            "Epoch 625/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1496\n",
            "Epoch 626/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1495\n",
            "Epoch 627/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1495\n",
            "Epoch 628/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1494\n",
            "Epoch 629/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1493\n",
            "Epoch 630/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1492\n",
            "Epoch 631/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1492\n",
            "Epoch 632/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1491\n",
            "Epoch 633/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1490\n",
            "Epoch 634/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1489\n",
            "Epoch 635/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1489\n",
            "Epoch 636/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1488\n",
            "Epoch 637/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1487\n",
            "Epoch 638/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1486\n",
            "Epoch 639/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1486\n",
            "Epoch 640/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1485\n",
            "Epoch 641/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1484\n",
            "Epoch 642/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1483\n",
            "Epoch 643/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1483\n",
            "Epoch 644/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1482\n",
            "Epoch 645/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1481\n",
            "Epoch 646/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1480\n",
            "Epoch 647/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1480\n",
            "Epoch 648/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1479\n",
            "Epoch 649/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1478\n",
            "Epoch 650/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1477\n",
            "Epoch 651/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1477\n",
            "Epoch 652/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1476\n",
            "Epoch 653/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1475\n",
            "Epoch 654/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1475\n",
            "Epoch 655/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1474\n",
            "Epoch 656/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1473\n",
            "Epoch 657/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1472\n",
            "Epoch 658/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1472\n",
            "Epoch 659/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1471\n",
            "Epoch 660/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1470\n",
            "Epoch 661/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1470\n",
            "Epoch 662/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1469\n",
            "Epoch 663/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1468\n",
            "Epoch 664/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1468\n",
            "Epoch 665/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1467\n",
            "Epoch 666/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1466\n",
            "Epoch 667/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1466\n",
            "Epoch 668/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1465\n",
            "Epoch 669/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 670/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1464\n",
            "Epoch 671/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1463\n",
            "Epoch 672/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1462\n",
            "Epoch 673/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1462\n",
            "Epoch 674/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1461\n",
            "Epoch 675/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1460\n",
            "Epoch 676/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1460\n",
            "Epoch 677/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1459\n",
            "Epoch 678/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1458\n",
            "Epoch 679/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1458\n",
            "Epoch 680/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1457\n",
            "Epoch 681/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1456\n",
            "Epoch 682/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1456\n",
            "Epoch 683/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1455\n",
            "Epoch 684/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1455\n",
            "Epoch 685/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1454\n",
            "Epoch 686/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1453\n",
            "Epoch 687/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1453\n",
            "Epoch 688/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1452\n",
            "Epoch 689/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1451\n",
            "Epoch 690/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1451\n",
            "Epoch 691/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1450\n",
            "Epoch 692/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1450\n",
            "Epoch 693/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1449\n",
            "Epoch 694/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1448\n",
            "Epoch 695/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1448\n",
            "Epoch 696/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1447\n",
            "Epoch 697/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1447\n",
            "Epoch 698/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1446\n",
            "Epoch 699/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1445\n",
            "Epoch 700/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1445\n",
            "Epoch 701/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1444\n",
            "Epoch 702/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1444\n",
            "Epoch 703/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1443\n",
            "Epoch 704/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1442\n",
            "Epoch 705/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1442\n",
            "Epoch 706/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1441\n",
            "Epoch 707/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1441\n",
            "Epoch 708/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1440\n",
            "Epoch 709/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1440\n",
            "Epoch 710/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1439\n",
            "Epoch 711/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1438\n",
            "Epoch 712/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1438\n",
            "Epoch 713/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1437\n",
            "Epoch 714/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1437\n",
            "Epoch 715/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1436\n",
            "Epoch 716/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1436\n",
            "Epoch 717/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1435\n",
            "Epoch 718/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1434\n",
            "Epoch 719/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1434\n",
            "Epoch 720/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1433\n",
            "Epoch 721/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1433\n",
            "Epoch 722/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1432\n",
            "Epoch 723/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1432\n",
            "Epoch 724/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1431\n",
            "Epoch 725/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1431\n",
            "Epoch 726/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1430\n",
            "Epoch 727/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1430\n",
            "Epoch 728/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1429\n",
            "Epoch 729/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1428\n",
            "Epoch 730/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1428\n",
            "Epoch 731/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1427\n",
            "Epoch 732/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1427\n",
            "Epoch 733/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1426\n",
            "Epoch 734/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1426\n",
            "Epoch 735/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1425\n",
            "Epoch 736/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1425\n",
            "Epoch 737/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1424\n",
            "Epoch 738/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1424\n",
            "Epoch 739/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1423\n",
            "Epoch 740/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1423\n",
            "Epoch 741/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1422\n",
            "Epoch 742/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1422\n",
            "Epoch 743/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1421\n",
            "Epoch 744/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1420\n",
            "Epoch 745/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1420\n",
            "Epoch 746/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1419\n",
            "Epoch 747/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1419\n",
            "Epoch 748/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1418\n",
            "Epoch 749/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1418\n",
            "Epoch 750/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1417\n",
            "Epoch 751/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1417\n",
            "Epoch 752/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1416\n",
            "Epoch 753/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1416\n",
            "Epoch 754/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1415\n",
            "Epoch 755/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1415\n",
            "Epoch 756/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1414\n",
            "Epoch 757/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1414\n",
            "Epoch 758/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1413\n",
            "Epoch 759/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1413\n",
            "Epoch 760/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1412\n",
            "Epoch 761/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1412\n",
            "Epoch 762/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1411\n",
            "Epoch 763/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1411\n",
            "Epoch 764/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1410\n",
            "Epoch 765/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1410\n",
            "Epoch 766/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1409\n",
            "Epoch 767/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1409\n",
            "Epoch 768/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1408\n",
            "Epoch 769/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1408\n",
            "Epoch 770/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1407\n",
            "Epoch 771/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1407\n",
            "Epoch 772/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1406\n",
            "Epoch 773/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1406\n",
            "Epoch 774/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1405\n",
            "Epoch 775/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1405\n",
            "Epoch 776/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1404\n",
            "Epoch 777/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1403\n",
            "Epoch 778/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1403\n",
            "Epoch 779/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1402\n",
            "Epoch 780/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1402\n",
            "Epoch 781/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1401\n",
            "Epoch 782/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1401\n",
            "Epoch 783/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1400\n",
            "Epoch 784/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1400\n",
            "Epoch 785/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1399\n",
            "Epoch 786/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1399\n",
            "Epoch 787/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1398\n",
            "Epoch 788/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1398\n",
            "Epoch 789/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1397\n",
            "Epoch 790/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1397\n",
            "Epoch 791/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1396\n",
            "Epoch 792/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1396\n",
            "Epoch 793/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1395\n",
            "Epoch 794/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1395\n",
            "Epoch 795/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1394\n",
            "Epoch 796/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1394\n",
            "Epoch 797/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1393\n",
            "Epoch 798/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1393\n",
            "Epoch 799/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1392\n",
            "Epoch 800/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1392\n",
            "Epoch 801/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1391\n",
            "Epoch 802/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1391\n",
            "Epoch 803/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1390\n",
            "Epoch 804/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1389\n",
            "Epoch 805/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1389\n",
            "Epoch 806/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1388\n",
            "Epoch 807/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1388\n",
            "Epoch 808/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1387\n",
            "Epoch 809/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1387\n",
            "Epoch 810/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1386\n",
            "Epoch 811/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1386\n",
            "Epoch 812/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1385\n",
            "Epoch 813/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1385\n",
            "Epoch 814/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1384\n",
            "Epoch 815/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1383\n",
            "Epoch 816/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1383\n",
            "Epoch 817/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1382\n",
            "Epoch 818/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1382\n",
            "Epoch 819/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1381\n",
            "Epoch 820/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1381\n",
            "Epoch 821/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1380\n",
            "Epoch 822/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1380\n",
            "Epoch 823/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1379\n",
            "Epoch 824/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1378\n",
            "Epoch 825/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1378\n",
            "Epoch 826/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1377\n",
            "Epoch 827/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1377\n",
            "Epoch 828/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 829/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1375\n",
            "Epoch 830/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1375\n",
            "Epoch 831/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1374\n",
            "Epoch 832/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1374\n",
            "Epoch 833/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1373\n",
            "Epoch 834/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1372\n",
            "Epoch 835/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1372\n",
            "Epoch 836/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1371\n",
            "Epoch 837/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1370\n",
            "Epoch 838/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1370\n",
            "Epoch 839/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1369\n",
            "Epoch 840/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1368\n",
            "Epoch 841/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1368\n",
            "Epoch 842/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1367\n",
            "Epoch 843/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1366\n",
            "Epoch 844/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1366\n",
            "Epoch 845/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1365\n",
            "Epoch 846/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1364\n",
            "Epoch 847/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1364\n",
            "Epoch 848/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1363\n",
            "Epoch 849/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1362\n",
            "Epoch 850/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1362\n",
            "Epoch 851/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1361\n",
            "Epoch 852/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1360\n",
            "Epoch 853/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1359\n",
            "Epoch 854/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1359\n",
            "Epoch 855/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1358\n",
            "Epoch 856/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1357\n",
            "Epoch 857/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1356\n",
            "Epoch 858/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1355\n",
            "Epoch 859/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1355\n",
            "Epoch 860/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1354\n",
            "Epoch 861/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1353\n",
            "Epoch 862/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1352\n",
            "Epoch 863/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1351\n",
            "Epoch 864/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1350\n",
            "Epoch 865/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1350\n",
            "Epoch 866/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1349\n",
            "Epoch 867/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1348\n",
            "Epoch 868/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1347\n",
            "Epoch 869/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1346\n",
            "Epoch 870/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1345\n",
            "Epoch 871/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1344\n",
            "Epoch 872/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1344\n",
            "Epoch 873/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1343\n",
            "Epoch 874/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1342\n",
            "Epoch 875/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1341\n",
            "Epoch 876/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1340\n",
            "Epoch 877/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1340\n",
            "Epoch 878/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1339\n",
            "Epoch 879/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1338\n",
            "Epoch 880/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1337\n",
            "Epoch 881/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1336\n",
            "Epoch 882/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1336\n",
            "Epoch 883/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1335\n",
            "Epoch 884/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1334\n",
            "Epoch 885/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1333\n",
            "Epoch 886/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1333\n",
            "Epoch 887/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1332\n",
            "Epoch 888/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1331\n",
            "Epoch 889/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1330\n",
            "Epoch 890/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1330\n",
            "Epoch 891/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1329\n",
            "Epoch 892/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1328\n",
            "Epoch 893/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1327\n",
            "Epoch 894/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1326\n",
            "Epoch 895/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1325\n",
            "Epoch 896/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1325\n",
            "Epoch 897/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1324\n",
            "Epoch 898/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1323\n",
            "Epoch 899/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1322\n",
            "Epoch 900/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1321\n",
            "Epoch 901/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1320\n",
            "Epoch 902/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1320\n",
            "Epoch 903/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1319\n",
            "Epoch 904/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1318\n",
            "Epoch 905/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1317\n",
            "Epoch 906/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1316\n",
            "Epoch 907/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1315\n",
            "Epoch 908/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1315\n",
            "Epoch 909/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1314\n",
            "Epoch 910/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1313\n",
            "Epoch 911/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1312\n",
            "Epoch 912/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1311\n",
            "Epoch 913/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1311\n",
            "Epoch 914/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1310\n",
            "Epoch 915/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1309\n",
            "Epoch 916/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1308\n",
            "Epoch 917/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1307\n",
            "Epoch 918/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1306\n",
            "Epoch 919/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1306\n",
            "Epoch 920/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1305\n",
            "Epoch 921/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1304\n",
            "Epoch 922/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1303\n",
            "Epoch 923/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1302\n",
            "Epoch 924/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1302\n",
            "Epoch 925/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1301\n",
            "Epoch 926/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1300\n",
            "Epoch 927/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1299\n",
            "Epoch 928/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1298\n",
            "Epoch 929/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1298\n",
            "Epoch 930/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1297\n",
            "Epoch 931/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1296\n",
            "Epoch 932/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1295\n",
            "Epoch 933/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1295\n",
            "Epoch 934/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1294\n",
            "Epoch 935/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1293\n",
            "Epoch 936/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1292\n",
            "Epoch 937/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1291\n",
            "Epoch 938/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1291\n",
            "Epoch 939/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1290\n",
            "Epoch 940/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1289\n",
            "Epoch 941/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1288\n",
            "Epoch 942/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1287\n",
            "Epoch 943/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1286\n",
            "Epoch 944/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1286\n",
            "Epoch 945/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1285\n",
            "Epoch 946/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1284\n",
            "Epoch 947/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1283\n",
            "Epoch 948/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1282\n",
            "Epoch 949/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1282\n",
            "Epoch 950/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1281\n",
            "Epoch 951/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1280\n",
            "Epoch 952/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1279\n",
            "Epoch 953/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1278\n",
            "Epoch 954/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1277\n",
            "Epoch 955/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1276\n",
            "Epoch 956/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1276\n",
            "Epoch 957/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1275\n",
            "Epoch 958/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1274\n",
            "Epoch 959/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1273\n",
            "Epoch 960/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1272\n",
            "Epoch 961/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1271\n",
            "Epoch 962/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1271\n",
            "Epoch 963/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1270\n",
            "Epoch 964/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1269\n",
            "Epoch 965/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1268\n",
            "Epoch 966/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1267\n",
            "Epoch 967/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1266\n",
            "Epoch 968/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1266\n",
            "Epoch 969/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1265\n",
            "Epoch 970/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1264\n",
            "Epoch 971/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1263\n",
            "Epoch 972/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1262\n",
            "Epoch 973/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1261\n",
            "Epoch 974/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1261\n",
            "Epoch 975/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1260\n",
            "Epoch 976/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1259\n",
            "Epoch 977/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1258\n",
            "Epoch 978/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1257\n",
            "Epoch 979/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1257\n",
            "Epoch 980/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1256\n",
            "Epoch 981/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1255\n",
            "Epoch 982/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1254\n",
            "Epoch 983/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1253\n",
            "Epoch 984/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1253\n",
            "Epoch 985/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1252\n",
            "Epoch 986/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1251\n",
            "Epoch 987/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1250\n",
            "Epoch 988/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1250\n",
            "Epoch 989/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1249\n",
            "Epoch 990/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1248\n",
            "Epoch 991/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1247\n",
            "Epoch 992/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1246\n",
            "Epoch 993/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1246\n",
            "Epoch 994/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1245\n",
            "Epoch 995/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1244\n",
            "Epoch 996/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1243\n",
            "Epoch 997/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1243\n",
            "Epoch 998/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1242\n",
            "Epoch 999/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1241\n",
            "Epoch 1000/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1241\n",
            "Epoch 1001/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1240\n",
            "Epoch 1002/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1239\n",
            "Epoch 1003/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1238\n",
            "Epoch 1004/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1238\n",
            "Epoch 1005/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1237\n",
            "Epoch 1006/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1236\n",
            "Epoch 1007/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1236\n",
            "Epoch 1008/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1235\n",
            "Epoch 1009/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1234\n",
            "Epoch 1010/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1233\n",
            "Epoch 1011/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1012/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1232\n",
            "Epoch 1013/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1231\n",
            "Epoch 1014/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1231\n",
            "Epoch 1015/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1230\n",
            "Epoch 1016/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1229\n",
            "Epoch 1017/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1229\n",
            "Epoch 1018/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1228\n",
            "Epoch 1019/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1227\n",
            "Epoch 1020/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1227\n",
            "Epoch 1021/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1226\n",
            "Epoch 1022/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1225\n",
            "Epoch 1023/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1225\n",
            "Epoch 1024/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1224\n",
            "Epoch 1025/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1224\n",
            "Epoch 1026/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1223\n",
            "Epoch 1027/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1222\n",
            "Epoch 1028/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1222\n",
            "Epoch 1029/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1221\n",
            "Epoch 1030/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1220\n",
            "Epoch 1031/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1220\n",
            "Epoch 1032/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1219\n",
            "Epoch 1033/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1219\n",
            "Epoch 1034/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1218\n",
            "Epoch 1035/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1217\n",
            "Epoch 1036/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1217\n",
            "Epoch 1037/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1216\n",
            "Epoch 1038/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1216\n",
            "Epoch 1039/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1215\n",
            "Epoch 1040/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1215\n",
            "Epoch 1041/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1214\n",
            "Epoch 1042/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1213\n",
            "Epoch 1043/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1213\n",
            "Epoch 1044/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1212\n",
            "Epoch 1045/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1212\n",
            "Epoch 1046/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1211\n",
            "Epoch 1047/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1211\n",
            "Epoch 1048/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1210\n",
            "Epoch 1049/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1210\n",
            "Epoch 1050/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1209\n",
            "Epoch 1051/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1209\n",
            "Epoch 1052/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1208\n",
            "Epoch 1053/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1208\n",
            "Epoch 1054/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1207\n",
            "Epoch 1055/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1206\n",
            "Epoch 1056/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1206\n",
            "Epoch 1057/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1205\n",
            "Epoch 1058/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1205\n",
            "Epoch 1059/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1204\n",
            "Epoch 1060/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1204\n",
            "Epoch 1061/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1203\n",
            "Epoch 1062/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1203\n",
            "Epoch 1063/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1202\n",
            "Epoch 1064/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1202\n",
            "Epoch 1065/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1201\n",
            "Epoch 1066/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1201\n",
            "Epoch 1067/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1200\n",
            "Epoch 1068/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1200\n",
            "Epoch 1069/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1199\n",
            "Epoch 1070/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1199\n",
            "Epoch 1071/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1198\n",
            "Epoch 1072/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1198\n",
            "Epoch 1073/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1197\n",
            "Epoch 1074/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1197\n",
            "Epoch 1075/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1196\n",
            "Epoch 1076/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1196\n",
            "Epoch 1077/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1196\n",
            "Epoch 1078/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1195\n",
            "Epoch 1079/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1195\n",
            "Epoch 1080/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1194\n",
            "Epoch 1081/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1194\n",
            "Epoch 1082/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1193\n",
            "Epoch 1083/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1193\n",
            "Epoch 1084/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1192\n",
            "Epoch 1085/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1192\n",
            "Epoch 1086/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1191\n",
            "Epoch 1087/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1191\n",
            "Epoch 1088/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1190\n",
            "Epoch 1089/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1190\n",
            "Epoch 1090/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1190\n",
            "Epoch 1091/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1189\n",
            "Epoch 1092/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1189\n",
            "Epoch 1093/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1188\n",
            "Epoch 1094/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1188\n",
            "Epoch 1095/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1187\n",
            "Epoch 1096/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1187\n",
            "Epoch 1097/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1186\n",
            "Epoch 1098/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1186\n",
            "Epoch 1099/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1186\n",
            "Epoch 1100/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1185\n",
            "Epoch 1101/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1185\n",
            "Epoch 1102/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1184\n",
            "Epoch 1103/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1184\n",
            "Epoch 1104/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1183\n",
            "Epoch 1105/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1183\n",
            "Epoch 1106/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1182\n",
            "Epoch 1107/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1182\n",
            "Epoch 1108/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1182\n",
            "Epoch 1109/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1181\n",
            "Epoch 1110/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1181\n",
            "Epoch 1111/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1180\n",
            "Epoch 1112/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1180\n",
            "Epoch 1113/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1180\n",
            "Epoch 1114/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1179\n",
            "Epoch 1115/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1179\n",
            "Epoch 1116/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1178\n",
            "Epoch 1117/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1178\n",
            "Epoch 1118/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1177\n",
            "Epoch 1119/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1177\n",
            "Epoch 1120/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1177\n",
            "Epoch 1121/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1176\n",
            "Epoch 1122/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1176\n",
            "Epoch 1123/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1175\n",
            "Epoch 1124/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1175\n",
            "Epoch 1125/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1175\n",
            "Epoch 1126/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1174\n",
            "Epoch 1127/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1174\n",
            "Epoch 1128/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1173\n",
            "Epoch 1129/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1173\n",
            "Epoch 1130/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1173\n",
            "Epoch 1131/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1172\n",
            "Epoch 1132/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1172\n",
            "Epoch 1133/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1171\n",
            "Epoch 1134/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1171\n",
            "Epoch 1135/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1171\n",
            "Epoch 1136/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1170\n",
            "Epoch 1137/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1170\n",
            "Epoch 1138/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1169\n",
            "Epoch 1139/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1169\n",
            "Epoch 1140/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1169\n",
            "Epoch 1141/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1168\n",
            "Epoch 1142/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1168\n",
            "Epoch 1143/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1168\n",
            "Epoch 1144/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1167\n",
            "Epoch 1145/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1167\n",
            "Epoch 1146/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1166\n",
            "Epoch 1147/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1166\n",
            "Epoch 1148/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1166\n",
            "Epoch 1149/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1165\n",
            "Epoch 1150/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1165\n",
            "Epoch 1151/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1165\n",
            "Epoch 1152/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1164\n",
            "Epoch 1153/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1164\n",
            "Epoch 1154/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1163\n",
            "Epoch 1155/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1163\n",
            "Epoch 1156/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1163\n",
            "Epoch 1157/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1162\n",
            "Epoch 1158/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1162\n",
            "Epoch 1159/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1162\n",
            "Epoch 1160/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1161\n",
            "Epoch 1161/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1161\n",
            "Epoch 1162/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1160\n",
            "Epoch 1163/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1160\n",
            "Epoch 1164/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1160\n",
            "Epoch 1165/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1159\n",
            "Epoch 1166/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1159\n",
            "Epoch 1167/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1159\n",
            "Epoch 1168/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1158\n",
            "Epoch 1169/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1158\n",
            "Epoch 1170/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1158\n",
            "Epoch 1171/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1157\n",
            "Epoch 1172/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1157\n",
            "Epoch 1173/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1156\n",
            "Epoch 1174/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1156\n",
            "Epoch 1175/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1156\n",
            "Epoch 1176/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1155\n",
            "Epoch 1177/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1155\n",
            "Epoch 1178/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1155\n",
            "Epoch 1179/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1154\n",
            "Epoch 1180/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1154\n",
            "Epoch 1181/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1154\n",
            "Epoch 1182/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1153\n",
            "Epoch 1183/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1153\n",
            "Epoch 1184/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1153\n",
            "Epoch 1185/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1152\n",
            "Epoch 1186/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1152\n",
            "Epoch 1187/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1152\n",
            "Epoch 1188/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1151\n",
            "Epoch 1189/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1151\n",
            "Epoch 1190/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1151\n",
            "Epoch 1191/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1150\n",
            "Epoch 1192/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1150\n",
            "Epoch 1193/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1150\n",
            "Epoch 1194/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1149\n",
            "Epoch 1195/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1149\n",
            "Epoch 1196/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1148\n",
            "Epoch 1197/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1148\n",
            "Epoch 1198/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1148\n",
            "Epoch 1199/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1147\n",
            "Epoch 1200/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1147\n",
            "Epoch 1201/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1147\n",
            "Epoch 1202/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1146\n",
            "Epoch 1203/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1146\n",
            "Epoch 1204/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1146\n",
            "Epoch 1205/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1145\n",
            "Epoch 1206/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1145\n",
            "Epoch 1207/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1145\n",
            "Epoch 1208/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1144\n",
            "Epoch 1209/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1144\n",
            "Epoch 1210/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1144\n",
            "Epoch 1211/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1143\n",
            "Epoch 1212/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1143\n",
            "Epoch 1213/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1143\n",
            "Epoch 1214/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1142\n",
            "Epoch 1215/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1142\n",
            "Epoch 1216/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1142\n",
            "Epoch 1217/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1141\n",
            "Epoch 1218/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1141\n",
            "Epoch 1219/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1141\n",
            "Epoch 1220/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1141\n",
            "Epoch 1221/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1140\n",
            "Epoch 1222/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1140\n",
            "Epoch 1223/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1140\n",
            "Epoch 1224/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1139\n",
            "Epoch 1225/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1139\n",
            "Epoch 1226/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1139\n",
            "Epoch 1227/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1138\n",
            "Epoch 1228/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1138\n",
            "Epoch 1229/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1138\n",
            "Epoch 1230/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1137\n",
            "Epoch 1231/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1137\n",
            "Epoch 1232/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1137\n",
            "Epoch 1233/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1136\n",
            "Epoch 1234/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1136\n",
            "Epoch 1235/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1136\n",
            "Epoch 1236/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1135\n",
            "Epoch 1237/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1135\n",
            "Epoch 1238/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1135\n",
            "Epoch 1239/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1134\n",
            "Epoch 1240/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1134\n",
            "Epoch 1241/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1134\n",
            "Epoch 1242/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1133\n",
            "Epoch 1243/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1133\n",
            "Epoch 1244/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1133\n",
            "Epoch 1245/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1133\n",
            "Epoch 1246/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1132\n",
            "Epoch 1247/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1132\n",
            "Epoch 1248/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1132\n",
            "Epoch 1249/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1131\n",
            "Epoch 1250/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1131\n",
            "Epoch 1251/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1131\n",
            "Epoch 1252/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1130\n",
            "Epoch 1253/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1130\n",
            "Epoch 1254/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1130\n",
            "Epoch 1255/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1129\n",
            "Epoch 1256/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1129\n",
            "Epoch 1257/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1129\n",
            "Epoch 1258/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1129\n",
            "Epoch 1259/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1128\n",
            "Epoch 1260/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1128\n",
            "Epoch 1261/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1128\n",
            "Epoch 1262/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1127\n",
            "Epoch 1263/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1127\n",
            "Epoch 1264/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1127\n",
            "Epoch 1265/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1126\n",
            "Epoch 1266/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1126\n",
            "Epoch 1267/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1126\n",
            "Epoch 1268/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1126\n",
            "Epoch 1269/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1125\n",
            "Epoch 1270/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1125\n",
            "Epoch 1271/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1125\n",
            "Epoch 1272/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1124\n",
            "Epoch 1273/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1124\n",
            "Epoch 1274/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1124\n",
            "Epoch 1275/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1123\n",
            "Epoch 1276/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1123\n",
            "Epoch 1277/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1123\n",
            "Epoch 1278/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1123\n",
            "Epoch 1279/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1122\n",
            "Epoch 1280/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1122\n",
            "Epoch 1281/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1122\n",
            "Epoch 1282/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1121\n",
            "Epoch 1283/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1121\n",
            "Epoch 1284/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1121\n",
            "Epoch 1285/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1120\n",
            "Epoch 1286/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1120\n",
            "Epoch 1287/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1120\n",
            "Epoch 1288/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1120\n",
            "Epoch 1289/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1119\n",
            "Epoch 1290/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1119\n",
            "Epoch 1291/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1119\n",
            "Epoch 1292/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1118\n",
            "Epoch 1293/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1118\n",
            "Epoch 1294/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1118\n",
            "Epoch 1295/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1118\n",
            "Epoch 1296/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1117\n",
            "Epoch 1297/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1117\n",
            "Epoch 1298/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1117\n",
            "Epoch 1299/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1116\n",
            "Epoch 1300/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1116\n",
            "Epoch 1301/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1116\n",
            "Epoch 1302/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1116\n",
            "Epoch 1303/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1115\n",
            "Epoch 1304/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1115\n",
            "Epoch 1305/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1115\n",
            "Epoch 1306/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1114\n",
            "Epoch 1307/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1114\n",
            "Epoch 1308/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1114\n",
            "Epoch 1309/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1114\n",
            "Epoch 1310/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1113\n",
            "Epoch 1311/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1113\n",
            "Epoch 1312/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1113\n",
            "Epoch 1313/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1112\n",
            "Epoch 1314/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1112\n",
            "Epoch 1315/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1112\n",
            "Epoch 1316/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1112\n",
            "Epoch 1317/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1111\n",
            "Epoch 1318/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1111\n",
            "Epoch 1319/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1111\n",
            "Epoch 1320/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1111\n",
            "Epoch 1321/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1110\n",
            "Epoch 1322/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1110\n",
            "Epoch 1323/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1110\n",
            "Epoch 1324/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1109\n",
            "Epoch 1325/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1109\n",
            "Epoch 1326/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1109\n",
            "Epoch 1327/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1109\n",
            "Epoch 1328/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1108\n",
            "Epoch 1329/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1108\n",
            "Epoch 1330/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1108\n",
            "Epoch 1331/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1108\n",
            "Epoch 1332/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1107\n",
            "Epoch 1333/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1107\n",
            "Epoch 1334/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1107\n",
            "Epoch 1335/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1106\n",
            "Epoch 1336/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1106\n",
            "Epoch 1337/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1106\n",
            "Epoch 1338/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1106\n",
            "Epoch 1339/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1105\n",
            "Epoch 1340/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1105\n",
            "Epoch 1341/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1105\n",
            "Epoch 1342/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1105\n",
            "Epoch 1343/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1104\n",
            "Epoch 1344/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1104\n",
            "Epoch 1345/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1104\n",
            "Epoch 1346/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1104\n",
            "Epoch 1347/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1103\n",
            "Epoch 1348/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1103\n",
            "Epoch 1349/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1103\n",
            "Epoch 1350/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1103\n",
            "Epoch 1351/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1102\n",
            "Epoch 1352/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1102\n",
            "Epoch 1353/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1102\n",
            "Epoch 1354/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1101\n",
            "Epoch 1355/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1101\n",
            "Epoch 1356/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1101\n",
            "Epoch 1357/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1101\n",
            "Epoch 1358/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1100\n",
            "Epoch 1359/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1100\n",
            "Epoch 1360/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1100\n",
            "Epoch 1361/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1100\n",
            "Epoch 1362/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1099\n",
            "Epoch 1363/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1099\n",
            "Epoch 1364/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1099\n",
            "Epoch 1365/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1099\n",
            "Epoch 1366/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1098\n",
            "Epoch 1367/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1098\n",
            "Epoch 1368/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1098\n",
            "Epoch 1369/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1098\n",
            "Epoch 1370/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1097\n",
            "Epoch 1371/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1097\n",
            "Epoch 1372/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1097\n",
            "Epoch 1373/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1097\n",
            "Epoch 1374/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1096\n",
            "Epoch 1375/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1096\n",
            "Epoch 1376/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1096\n",
            "Epoch 1377/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1096\n",
            "Epoch 1378/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1095\n",
            "Epoch 1379/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1095\n",
            "Epoch 1380/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1095\n",
            "Epoch 1381/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1095\n",
            "Epoch 1382/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1095\n",
            "Epoch 1383/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1094\n",
            "Epoch 1384/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1094\n",
            "Epoch 1385/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1094\n",
            "Epoch 1386/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1094\n",
            "Epoch 1387/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1093\n",
            "Epoch 1388/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1093\n",
            "Epoch 1389/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1093\n",
            "Epoch 1390/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1093\n",
            "Epoch 1391/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1092\n",
            "Epoch 1392/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1092\n",
            "Epoch 1393/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1092\n",
            "Epoch 1394/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1092\n",
            "Epoch 1395/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1091\n",
            "Epoch 1396/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1091\n",
            "Epoch 1397/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1091\n",
            "Epoch 1398/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1091\n",
            "Epoch 1399/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1091\n",
            "Epoch 1400/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1090\n",
            "Epoch 1401/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1090\n",
            "Epoch 1402/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1090\n",
            "Epoch 1403/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1090\n",
            "Epoch 1404/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1089\n",
            "Epoch 1405/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1089\n",
            "Epoch 1406/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1089\n",
            "Epoch 1407/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1089\n",
            "Epoch 1408/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1088\n",
            "Epoch 1409/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1088\n",
            "Epoch 1410/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1088\n",
            "Epoch 1411/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1088\n",
            "Epoch 1412/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1088\n",
            "Epoch 1413/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1087\n",
            "Epoch 1414/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1087\n",
            "Epoch 1415/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1087\n",
            "Epoch 1416/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1087\n",
            "Epoch 1417/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1086\n",
            "Epoch 1418/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1086\n",
            "Epoch 1419/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1086\n",
            "Epoch 1420/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1086\n",
            "Epoch 1421/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1086\n",
            "Epoch 1422/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1085\n",
            "Epoch 1423/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1085\n",
            "Epoch 1424/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1085\n",
            "Epoch 1425/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1085\n",
            "Epoch 1426/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1084\n",
            "Epoch 1427/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1084\n",
            "Epoch 1428/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1084\n",
            "Epoch 1429/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1084\n",
            "Epoch 1430/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1084\n",
            "Epoch 1431/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1083\n",
            "Epoch 1432/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1083\n",
            "Epoch 1433/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1083\n",
            "Epoch 1434/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1083\n",
            "Epoch 1435/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1083\n",
            "Epoch 1436/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1082\n",
            "Epoch 1437/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1082\n",
            "Epoch 1438/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1082\n",
            "Epoch 1439/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1082\n",
            "Epoch 1440/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1082\n",
            "Epoch 1441/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1081\n",
            "Epoch 1442/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1081\n",
            "Epoch 1443/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1081\n",
            "Epoch 1444/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1081\n",
            "Epoch 1445/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1080\n",
            "Epoch 1446/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1080\n",
            "Epoch 1447/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1080\n",
            "Epoch 1448/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1080\n",
            "Epoch 1449/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1080\n",
            "Epoch 1450/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1079\n",
            "Epoch 1451/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1079\n",
            "Epoch 1452/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1079\n",
            "Epoch 1453/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1079\n",
            "Epoch 1454/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1079\n",
            "Epoch 1455/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1078\n",
            "Epoch 1456/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1078\n",
            "Epoch 1457/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1078\n",
            "Epoch 1458/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1078\n",
            "Epoch 1459/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1078\n",
            "Epoch 1460/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1077\n",
            "Epoch 1461/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1077\n",
            "Epoch 1462/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1077\n",
            "Epoch 1463/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1077\n",
            "Epoch 1464/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1077\n",
            "Epoch 1465/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1076\n",
            "Epoch 1466/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1076\n",
            "Epoch 1467/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1076\n",
            "Epoch 1468/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1076\n",
            "Epoch 1469/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1076\n",
            "Epoch 1470/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1075\n",
            "Epoch 1471/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1075\n",
            "Epoch 1472/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1075\n",
            "Epoch 1473/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1075\n",
            "Epoch 1474/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1075\n",
            "Epoch 1475/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1075\n",
            "Epoch 1476/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1074\n",
            "Epoch 1477/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1074\n",
            "Epoch 1478/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1074\n",
            "Epoch 1479/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1074\n",
            "Epoch 1480/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1074\n",
            "Epoch 1481/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1073\n",
            "Epoch 1482/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1073\n",
            "Epoch 1483/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1073\n",
            "Epoch 1484/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1073\n",
            "Epoch 1485/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1073\n",
            "Epoch 1486/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1072\n",
            "Epoch 1487/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1072\n",
            "Epoch 1488/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1072\n",
            "Epoch 1489/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1072\n",
            "Epoch 1490/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1072\n",
            "Epoch 1491/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1072\n",
            "Epoch 1492/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1071\n",
            "Epoch 1493/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1071\n",
            "Epoch 1494/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1071\n",
            "Epoch 1495/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1071\n",
            "Epoch 1496/2000\n",
            "15746/15746 [==============================] - 0s 13us/step - loss: 0.1071\n",
            "Epoch 1497/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1070\n",
            "Epoch 1498/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1070\n",
            "Epoch 1499/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1070\n",
            "Epoch 1500/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1070\n",
            "Epoch 1501/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1070\n",
            "Epoch 1502/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1070\n",
            "Epoch 1503/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1069\n",
            "Epoch 1504/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1069\n",
            "Epoch 1505/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1069\n",
            "Epoch 1506/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1069\n",
            "Epoch 1507/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1069\n",
            "Epoch 1508/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1068\n",
            "Epoch 1509/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1068\n",
            "Epoch 1510/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1068\n",
            "Epoch 1511/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1068\n",
            "Epoch 1512/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1068\n",
            "Epoch 1513/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1068\n",
            "Epoch 1514/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1067\n",
            "Epoch 1515/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1067\n",
            "Epoch 1516/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1067\n",
            "Epoch 1517/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1067\n",
            "Epoch 1518/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1067\n",
            "Epoch 1519/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1067\n",
            "Epoch 1520/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1066\n",
            "Epoch 1521/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1066\n",
            "Epoch 1522/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1066\n",
            "Epoch 1523/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1066\n",
            "Epoch 1524/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1066\n",
            "Epoch 1525/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1066\n",
            "Epoch 1526/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1065\n",
            "Epoch 1527/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1065\n",
            "Epoch 1528/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1065\n",
            "Epoch 1529/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1065\n",
            "Epoch 1530/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1065\n",
            "Epoch 1531/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1065\n",
            "Epoch 1532/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1064\n",
            "Epoch 1533/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1064\n",
            "Epoch 1534/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1064\n",
            "Epoch 1535/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1064\n",
            "Epoch 1536/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1064\n",
            "Epoch 1537/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1064\n",
            "Epoch 1538/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1063\n",
            "Epoch 1539/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1063\n",
            "Epoch 1540/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1063\n",
            "Epoch 1541/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1063\n",
            "Epoch 1542/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1063\n",
            "Epoch 1543/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1063\n",
            "Epoch 1544/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1062\n",
            "Epoch 1545/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1062\n",
            "Epoch 1546/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1062\n",
            "Epoch 1547/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1062\n",
            "Epoch 1548/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1062\n",
            "Epoch 1549/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1062\n",
            "Epoch 1550/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1061\n",
            "Epoch 1551/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1061\n",
            "Epoch 1552/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1061\n",
            "Epoch 1553/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1061\n",
            "Epoch 1554/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1061\n",
            "Epoch 1555/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1061\n",
            "Epoch 1556/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1557/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n",
            "Epoch 1558/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n",
            "Epoch 1559/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n",
            "Epoch 1560/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n",
            "Epoch 1561/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1060\n",
            "Epoch 1562/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1060\n",
            "Epoch 1563/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1059\n",
            "Epoch 1564/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1059\n",
            "Epoch 1565/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1059\n",
            "Epoch 1566/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1059\n",
            "Epoch 1567/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1059\n",
            "Epoch 1568/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1059\n",
            "Epoch 1569/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1058\n",
            "Epoch 1570/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1058\n",
            "Epoch 1571/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1058\n",
            "Epoch 1572/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1058\n",
            "Epoch 1573/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1058\n",
            "Epoch 1574/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1058\n",
            "Epoch 1575/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1058\n",
            "Epoch 1576/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1057\n",
            "Epoch 1577/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1057\n",
            "Epoch 1578/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1057\n",
            "Epoch 1579/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1057\n",
            "Epoch 1580/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1057\n",
            "Epoch 1581/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1057\n",
            "Epoch 1582/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1056\n",
            "Epoch 1583/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1056\n",
            "Epoch 1584/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1056\n",
            "Epoch 1585/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1056\n",
            "Epoch 1586/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1056\n",
            "Epoch 1587/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1056\n",
            "Epoch 1588/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1056\n",
            "Epoch 1589/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1590/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1591/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1592/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1593/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1055\n",
            "Epoch 1594/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1595/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1055\n",
            "Epoch 1596/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1054\n",
            "Epoch 1597/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1054\n",
            "Epoch 1598/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1054\n",
            "Epoch 1599/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1054\n",
            "Epoch 1600/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1054\n",
            "Epoch 1601/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1054\n",
            "Epoch 1602/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1054\n",
            "Epoch 1603/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1053\n",
            "Epoch 1604/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1053\n",
            "Epoch 1605/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1053\n",
            "Epoch 1606/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1053\n",
            "Epoch 1607/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1053\n",
            "Epoch 1608/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1053\n",
            "Epoch 1609/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1053\n",
            "Epoch 1610/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1052\n",
            "Epoch 1611/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1052\n",
            "Epoch 1612/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1052\n",
            "Epoch 1613/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1052\n",
            "Epoch 1614/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1052\n",
            "Epoch 1615/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1052\n",
            "Epoch 1616/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1052\n",
            "Epoch 1617/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1051\n",
            "Epoch 1618/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1051\n",
            "Epoch 1619/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1051\n",
            "Epoch 1620/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1051\n",
            "Epoch 1621/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1051\n",
            "Epoch 1622/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1051\n",
            "Epoch 1623/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1051\n",
            "Epoch 1624/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1050\n",
            "Epoch 1625/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1050\n",
            "Epoch 1626/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1050\n",
            "Epoch 1627/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1050\n",
            "Epoch 1628/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1050\n",
            "Epoch 1629/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1050\n",
            "Epoch 1630/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1050\n",
            "Epoch 1631/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1632/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1633/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1634/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1635/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1636/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1049\n",
            "Epoch 1637/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1638/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1049\n",
            "Epoch 1639/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1048\n",
            "Epoch 1640/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1048\n",
            "Epoch 1641/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1048\n",
            "Epoch 1642/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1048\n",
            "Epoch 1643/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1048\n",
            "Epoch 1644/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1048\n",
            "Epoch 1645/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1048\n",
            "Epoch 1646/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1647/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1648/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1649/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1650/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1651/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1047\n",
            "Epoch 1652/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1047\n",
            "Epoch 1653/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1047\n",
            "Epoch 1654/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1046\n",
            "Epoch 1655/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1046\n",
            "Epoch 1656/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1046\n",
            "Epoch 1657/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1046\n",
            "Epoch 1658/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1046\n",
            "Epoch 1659/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1046\n",
            "Epoch 1660/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1046\n",
            "Epoch 1661/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1045\n",
            "Epoch 1662/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1045\n",
            "Epoch 1663/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1045\n",
            "Epoch 1664/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1045\n",
            "Epoch 1665/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1045\n",
            "Epoch 1666/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1045\n",
            "Epoch 1667/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1045\n",
            "Epoch 1668/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1045\n",
            "Epoch 1669/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1044\n",
            "Epoch 1670/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1044\n",
            "Epoch 1671/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1044\n",
            "Epoch 1672/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1044\n",
            "Epoch 1673/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1044\n",
            "Epoch 1674/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1044\n",
            "Epoch 1675/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1044\n",
            "Epoch 1676/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1044\n",
            "Epoch 1677/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1678/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1679/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1680/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1681/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1682/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1043\n",
            "Epoch 1683/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1043\n",
            "Epoch 1684/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1043\n",
            "Epoch 1685/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1042\n",
            "Epoch 1686/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1042\n",
            "Epoch 1687/2000\n",
            "15746/15746 [==============================] - 0s 6us/step - loss: 0.1042\n",
            "Epoch 1688/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1042\n",
            "Epoch 1689/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1042\n",
            "Epoch 1690/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1042\n",
            "Epoch 1691/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1042\n",
            "Epoch 1692/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1042\n",
            "Epoch 1693/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1694/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1695/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1696/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1697/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1041\n",
            "Epoch 1698/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1699/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1041\n",
            "Epoch 1700/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1041\n",
            "Epoch 1701/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1040\n",
            "Epoch 1702/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1040\n",
            "Epoch 1703/2000\n",
            "15746/15746 [==============================] - 0s 12us/step - loss: 0.1040\n",
            "Epoch 1704/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1040\n",
            "Epoch 1705/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1040\n",
            "Epoch 1706/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1040\n",
            "Epoch 1707/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1040\n",
            "Epoch 1708/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1040\n",
            "Epoch 1709/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1039\n",
            "Epoch 1710/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1711/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1712/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1039\n",
            "Epoch 1713/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1714/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1039: 0s - loss: 0.1\n",
            "Epoch 1715/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1716/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1717/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1039\n",
            "Epoch 1718/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1719/2000\n",
            "15746/15746 [==============================] - 0s 10us/step - loss: 0.1038\n",
            "Epoch 1720/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1721/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1722/2000\n",
            "15746/15746 [==============================] - 0s 11us/step - loss: 0.1038\n",
            "Epoch 1723/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1724/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1725/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1038\n",
            "Epoch 1726/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1037\n",
            "Epoch 1727/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1037\n",
            "Epoch 1728/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1037\n",
            "Epoch 1729/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1037\n",
            "Epoch 1730/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1037\n",
            "Epoch 1731/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1037\n",
            "Epoch 1732/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1037\n",
            "Epoch 1733/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1037\n",
            "Epoch 1734/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1735/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1736/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1737/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1036\n",
            "Epoch 1738/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1739/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1036\n",
            "Epoch 1740/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1741/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1036\n",
            "Epoch 1742/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1036\n",
            "Epoch 1743/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1744/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1745/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1746/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1747/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1748/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1749/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1750/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1751/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1035\n",
            "Epoch 1752/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1753/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1754/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1755/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1756/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1757/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1758/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1759/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1034\n",
            "Epoch 1760/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1033\n",
            "Epoch 1761/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1033\n",
            "Epoch 1762/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1033\n",
            "Epoch 1763/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1033\n",
            "Epoch 1764/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1033\n",
            "Epoch 1765/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1033\n",
            "Epoch 1766/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1033\n",
            "Epoch 1767/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1033\n",
            "Epoch 1768/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1033\n",
            "Epoch 1769/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1770/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1032\n",
            "Epoch 1771/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1772/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1032\n",
            "Epoch 1773/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1032\n",
            "Epoch 1774/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1775/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1776/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1777/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1032\n",
            "Epoch 1778/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1779/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1780/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1781/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1031\n",
            "Epoch 1782/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1031\n",
            "Epoch 1783/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1031\n",
            "Epoch 1784/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1785/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1786/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1031\n",
            "Epoch 1787/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1788/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1030\n",
            "Epoch 1789/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1790/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1791/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1792/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1793/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1030\n",
            "Epoch 1794/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1030\n",
            "Epoch 1795/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1030\n",
            "Epoch 1796/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1029\n",
            "Epoch 1797/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1029\n",
            "Epoch 1798/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1799/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1800/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1029\n",
            "Epoch 1801/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1802/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1803/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1804/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1029\n",
            "Epoch 1805/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1029\n",
            "Epoch 1806/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1028\n",
            "Epoch 1807/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1028\n",
            "Epoch 1808/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1028\n",
            "Epoch 1809/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1028\n",
            "Epoch 1810/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1028\n",
            "Epoch 1811/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1028\n",
            "Epoch 1812/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1028\n",
            "Epoch 1813/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1028\n",
            "Epoch 1814/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1028\n",
            "Epoch 1815/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1816/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1027\n",
            "Epoch 1817/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1818/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1027\n",
            "Epoch 1819/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1820/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1821/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1822/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1027\n",
            "Epoch 1823/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1027\n",
            "Epoch 1824/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1825/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1826/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1827/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1828/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1829/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1830/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1831/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1832/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1026\n",
            "Epoch 1833/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1026\n",
            "Epoch 1834/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1835/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1836/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1837/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1025\n",
            "Epoch 1838/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1839/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1025\n",
            "Epoch 1840/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1841/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1842/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1025\n",
            "Epoch 1843/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1844/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1845/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1024\n",
            "Epoch 1846/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1024\n",
            "Epoch 1847/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1848/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1024\n",
            "Epoch 1849/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1850/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1851/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1852/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1024\n",
            "Epoch 1853/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1023\n",
            "Epoch 1854/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1023\n",
            "Epoch 1855/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1023\n",
            "Epoch 1856/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1023\n",
            "Epoch 1857/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1023\n",
            "Epoch 1858/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1023\n",
            "Epoch 1859/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1023\n",
            "Epoch 1860/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1023\n",
            "Epoch 1861/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1023\n",
            "Epoch 1862/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1023\n",
            "Epoch 1863/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1864/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1865/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1866/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1867/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1868/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1869/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1870/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1871/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1872/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1022\n",
            "Epoch 1873/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1874/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1875/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1876/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1877/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1878/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1879/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1880/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1881/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1882/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1021\n",
            "Epoch 1883/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1884/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1885/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1886/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1887/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1888/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1889/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1890/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1020\n",
            "Epoch 1891/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1020\n",
            "Epoch 1892/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1020\n",
            "Epoch 1893/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1894/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1895/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1896/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1897/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1019\n",
            "Epoch 1898/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1899/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1900/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1019\n",
            "Epoch 1901/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1019\n",
            "Epoch 1902/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1019\n",
            "Epoch 1903/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1018\n",
            "Epoch 1904/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1018\n",
            "Epoch 1905/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1906/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1907/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1908/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1909/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1910/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1911/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1912/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1018\n",
            "Epoch 1913/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1017\n",
            "Epoch 1914/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1915/2000\n",
            "15746/15746 [==============================] - 0s 9us/step - loss: 0.1017\n",
            "Epoch 1916/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1917/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1918/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1919/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1920/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1017\n",
            "Epoch 1921/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1017\n",
            "Epoch 1922/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1017\n",
            "Epoch 1923/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1016\n",
            "Epoch 1924/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1925/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1926/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1927/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1928/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1929/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1930/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1931/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1932/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1933/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1016\n",
            "Epoch 1934/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1015\n",
            "Epoch 1935/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1936/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1015\n",
            "Epoch 1937/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1938/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1939/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1940/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1941/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1942/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1943/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1015\n",
            "Epoch 1944/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1945/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1946/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1014\n",
            "Epoch 1947/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1948/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1949/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1950/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1951/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1952/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1953/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1954/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1014\n",
            "Epoch 1955/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1956/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1957/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1013\n",
            "Epoch 1958/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1959/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1960/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1961/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1962/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1013\n",
            "Epoch 1963/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1964/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1013\n",
            "Epoch 1965/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1966/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1967/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1968/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1969/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1012\n",
            "Epoch 1970/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1971/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1012\n",
            "Epoch 1972/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1973/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1974/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1975/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1012\n",
            "Epoch 1976/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1977/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1011\n",
            "Epoch 1978/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1979/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1980/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1981/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1982/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1983/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1984/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1985/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1986/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1011\n",
            "Epoch 1987/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1988/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1989/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1990/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1991/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1992/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1993/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1994/2000\n",
            "15746/15746 [==============================] - 0s 8us/step - loss: 0.1010\n",
            "Epoch 1995/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1996/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1997/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1010\n",
            "Epoch 1998/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1009\n",
            "Epoch 1999/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1009\n",
            "Epoch 2000/2000\n",
            "15746/15746 [==============================] - 0s 7us/step - loss: 0.1009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x14f86e1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dqOnLqiyA7Vo",
        "colab": {},
        "outputId": "78512486-4b50-445a-a500-e3f48e82da7d"
      },
      "source": [
        "gru = GRU_(10,0)\n",
        "gru.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=1500,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "15746/15746 [==============================] - 4s 281us/step - loss: 1.1353\n",
            "Epoch 2/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1326\n",
            "Epoch 3/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1308\n",
            "Epoch 4/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1291\n",
            "Epoch 5/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1274\n",
            "Epoch 6/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 1.1254\n",
            "Epoch 7/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1233\n",
            "Epoch 8/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1209\n",
            "Epoch 9/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1184\n",
            "Epoch 10/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1157\n",
            "Epoch 11/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1130\n",
            "Epoch 12/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1104\n",
            "Epoch 13/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1080\n",
            "Epoch 14/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1059\n",
            "Epoch 15/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1041\n",
            "Epoch 16/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 1.1025\n",
            "Epoch 17/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 1.1012\n",
            "Epoch 18/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.1001\n",
            "Epoch 19/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0991\n",
            "Epoch 20/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0982\n",
            "Epoch 21/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0974\n",
            "Epoch 22/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0966\n",
            "Epoch 23/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0959\n",
            "Epoch 24/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0952\n",
            "Epoch 25/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0944\n",
            "Epoch 26/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0936\n",
            "Epoch 27/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0927\n",
            "Epoch 28/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 1.0918\n",
            "Epoch 29/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0907\n",
            "Epoch 30/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0896\n",
            "Epoch 31/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0882\n",
            "Epoch 32/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0866\n",
            "Epoch 33/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 1.0846\n",
            "Epoch 34/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 1.0822\n",
            "Epoch 35/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0792\n",
            "Epoch 36/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0754\n",
            "Epoch 37/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 1.0703\n",
            "Epoch 38/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 1.0634\n",
            "Epoch 39/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0537\n",
            "Epoch 40/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 1.0394\n",
            "Epoch 41/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 1.0166\n",
            "Epoch 42/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.9780\n",
            "Epoch 43/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9089\n",
            "Epoch 44/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.8125\n",
            "Epoch 45/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.7433\n",
            "Epoch 46/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.6957\n",
            "Epoch 47/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.6557\n",
            "Epoch 48/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.6185\n",
            "Epoch 49/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.5805\n",
            "Epoch 50/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.5406\n",
            "Epoch 51/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.4989\n",
            "Epoch 52/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.4572\n",
            "Epoch 53/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.4179\n",
            "Epoch 54/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.3844\n",
            "Epoch 55/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3589\n",
            "Epoch 56/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3408\n",
            "Epoch 57/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3277\n",
            "Epoch 58/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3174\n",
            "Epoch 59/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3088\n",
            "Epoch 60/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.3016\n",
            "Epoch 61/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2952\n",
            "Epoch 62/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2894\n",
            "Epoch 63/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.2839\n",
            "Epoch 64/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2789\n",
            "Epoch 65/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2742\n",
            "Epoch 66/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2698\n",
            "Epoch 67/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2656\n",
            "Epoch 68/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2616\n",
            "Epoch 69/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2579\n",
            "Epoch 70/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.2544\n",
            "Epoch 71/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2510\n",
            "Epoch 72/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2479\n",
            "Epoch 73/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2449\n",
            "Epoch 74/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2421\n",
            "Epoch 75/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2394\n",
            "Epoch 76/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2369\n",
            "Epoch 77/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2345\n",
            "Epoch 78/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2322\n",
            "Epoch 79/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2300\n",
            "Epoch 80/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2279\n",
            "Epoch 81/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2259\n",
            "Epoch 82/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2240\n",
            "Epoch 83/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2221\n",
            "Epoch 84/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2202\n",
            "Epoch 85/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2185\n",
            "Epoch 86/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2167\n",
            "Epoch 87/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2150\n",
            "Epoch 88/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2134\n",
            "Epoch 89/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2118\n",
            "Epoch 90/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2102\n",
            "Epoch 91/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.2087\n",
            "Epoch 92/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2072\n",
            "Epoch 93/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.2058\n",
            "Epoch 94/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2044\n",
            "Epoch 95/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2030\n",
            "Epoch 96/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2017\n",
            "Epoch 97/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2004\n",
            "Epoch 98/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1992\n",
            "Epoch 99/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1980\n",
            "Epoch 100/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1968\n",
            "Epoch 101/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1957\n",
            "Epoch 102/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1946\n",
            "Epoch 103/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1935\n",
            "Epoch 104/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1925\n",
            "Epoch 105/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1914\n",
            "Epoch 106/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1905\n",
            "Epoch 107/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1895\n",
            "Epoch 108/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1886\n",
            "Epoch 109/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1877\n",
            "Epoch 110/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1868\n",
            "Epoch 111/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1860\n",
            "Epoch 112/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1851\n",
            "Epoch 113/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1843\n",
            "Epoch 114/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1836\n",
            "Epoch 115/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1828\n",
            "Epoch 116/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1821\n",
            "Epoch 117/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1814\n",
            "Epoch 118/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1807\n",
            "Epoch 119/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1800\n",
            "Epoch 120/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1794\n",
            "Epoch 121/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1788\n",
            "Epoch 122/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1782\n",
            "Epoch 123/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1776\n",
            "Epoch 124/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1770\n",
            "Epoch 125/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1764\n",
            "Epoch 126/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1759\n",
            "Epoch 127/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1754\n",
            "Epoch 128/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1748\n",
            "Epoch 129/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1743\n",
            "Epoch 130/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1738\n",
            "Epoch 131/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1733\n",
            "Epoch 132/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1729\n",
            "Epoch 133/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1724\n",
            "Epoch 134/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1720\n",
            "Epoch 135/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1715\n",
            "Epoch 136/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1711\n",
            "Epoch 137/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1706\n",
            "Epoch 138/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1702\n",
            "Epoch 139/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1698\n",
            "Epoch 140/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1694\n",
            "Epoch 141/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1690\n",
            "Epoch 142/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1686\n",
            "Epoch 143/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1682\n",
            "Epoch 144/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1678\n",
            "Epoch 145/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1674\n",
            "Epoch 146/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1670\n",
            "Epoch 147/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1666\n",
            "Epoch 148/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1662\n",
            "Epoch 149/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1659\n",
            "Epoch 150/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1655\n",
            "Epoch 151/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1651\n",
            "Epoch 152/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1647\n",
            "Epoch 153/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1644\n",
            "Epoch 154/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1640\n",
            "Epoch 155/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1637\n",
            "Epoch 156/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1633\n",
            "Epoch 157/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1630\n",
            "Epoch 158/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1626\n",
            "Epoch 159/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1623\n",
            "Epoch 160/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1619\n",
            "Epoch 161/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1616\n",
            "Epoch 162/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1613\n",
            "Epoch 163/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1610\n",
            "Epoch 164/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1606\n",
            "Epoch 165/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1603\n",
            "Epoch 166/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1600\n",
            "Epoch 167/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1597\n",
            "Epoch 168/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1593\n",
            "Epoch 169/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1590\n",
            "Epoch 170/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1587\n",
            "Epoch 171/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1584\n",
            "Epoch 172/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1581\n",
            "Epoch 173/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1578\n",
            "Epoch 174/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1575\n",
            "Epoch 175/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1572\n",
            "Epoch 176/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1569\n",
            "Epoch 177/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1566\n",
            "Epoch 178/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1563\n",
            "Epoch 179/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1560\n",
            "Epoch 180/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1557\n",
            "Epoch 181/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1554\n",
            "Epoch 182/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1551\n",
            "Epoch 183/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1548\n",
            "Epoch 184/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1545\n",
            "Epoch 185/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1542\n",
            "Epoch 186/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1539\n",
            "Epoch 187/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1537\n",
            "Epoch 188/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1534\n",
            "Epoch 189/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1531\n",
            "Epoch 190/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1528\n",
            "Epoch 191/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1525\n",
            "Epoch 192/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1523\n",
            "Epoch 193/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1520\n",
            "Epoch 194/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1517\n",
            "Epoch 195/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1514\n",
            "Epoch 196/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1512\n",
            "Epoch 197/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1509\n",
            "Epoch 198/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1506\n",
            "Epoch 199/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1504\n",
            "Epoch 200/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1501\n",
            "Epoch 201/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1498\n",
            "Epoch 202/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1496\n",
            "Epoch 203/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1493\n",
            "Epoch 204/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1491\n",
            "Epoch 205/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1488\n",
            "Epoch 206/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1486\n",
            "Epoch 207/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1483\n",
            "Epoch 208/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1481\n",
            "Epoch 209/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1478\n",
            "Epoch 210/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1476\n",
            "Epoch 211/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1473\n",
            "Epoch 212/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1471\n",
            "Epoch 213/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1468\n",
            "Epoch 214/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1466\n",
            "Epoch 215/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1463\n",
            "Epoch 216/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1461\n",
            "Epoch 217/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1459\n",
            "Epoch 218/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1456\n",
            "Epoch 219/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1454\n",
            "Epoch 220/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1452\n",
            "Epoch 221/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1449\n",
            "Epoch 222/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1447\n",
            "Epoch 223/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1445\n",
            "Epoch 224/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1442\n",
            "Epoch 225/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1440\n",
            "Epoch 226/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1438\n",
            "Epoch 227/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1435\n",
            "Epoch 228/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1433\n",
            "Epoch 229/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1431\n",
            "Epoch 230/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1429\n",
            "Epoch 231/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1427\n",
            "Epoch 232/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1424\n",
            "Epoch 233/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1422\n",
            "Epoch 234/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1420\n",
            "Epoch 235/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1418\n",
            "Epoch 236/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1415\n",
            "Epoch 237/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1413\n",
            "Epoch 238/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1411\n",
            "Epoch 239/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1409\n",
            "Epoch 240/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1407\n",
            "Epoch 241/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1404\n",
            "Epoch 242/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1402\n",
            "Epoch 243/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1400\n",
            "Epoch 244/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1398\n",
            "Epoch 245/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1396\n",
            "Epoch 246/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1394\n",
            "Epoch 247/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1391\n",
            "Epoch 248/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1389\n",
            "Epoch 249/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1387\n",
            "Epoch 250/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1385\n",
            "Epoch 251/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1383\n",
            "Epoch 252/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1381\n",
            "Epoch 253/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1378\n",
            "Epoch 254/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1376\n",
            "Epoch 255/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1374\n",
            "Epoch 256/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1372\n",
            "Epoch 257/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1370\n",
            "Epoch 258/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1367\n",
            "Epoch 259/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1365\n",
            "Epoch 260/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1363\n",
            "Epoch 261/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1361\n",
            "Epoch 262/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1359\n",
            "Epoch 263/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1357\n",
            "Epoch 264/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1354\n",
            "Epoch 265/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1352\n",
            "Epoch 266/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1350\n",
            "Epoch 267/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1348\n",
            "Epoch 268/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1345\n",
            "Epoch 269/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1343\n",
            "Epoch 270/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1341\n",
            "Epoch 271/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1339\n",
            "Epoch 272/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1336\n",
            "Epoch 273/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1334\n",
            "Epoch 274/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1332\n",
            "Epoch 275/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1329\n",
            "Epoch 276/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1327\n",
            "Epoch 277/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1325\n",
            "Epoch 278/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1322\n",
            "Epoch 279/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1320\n",
            "Epoch 280/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1318\n",
            "Epoch 281/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1315\n",
            "Epoch 282/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1313\n",
            "Epoch 283/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1310\n",
            "Epoch 284/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1308\n",
            "Epoch 285/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1306\n",
            "Epoch 286/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1303\n",
            "Epoch 287/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1301\n",
            "Epoch 288/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1299\n",
            "Epoch 289/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1296\n",
            "Epoch 290/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1294\n",
            "Epoch 291/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1292\n",
            "Epoch 292/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1289\n",
            "Epoch 293/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1287\n",
            "Epoch 294/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1285\n",
            "Epoch 295/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1283\n",
            "Epoch 296/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1280\n",
            "Epoch 297/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1278\n",
            "Epoch 298/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1276\n",
            "Epoch 299/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1274\n",
            "Epoch 300/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1271\n",
            "Epoch 301/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1269\n",
            "Epoch 302/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1267\n",
            "Epoch 303/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1265\n",
            "Epoch 304/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1263\n",
            "Epoch 305/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1261\n",
            "Epoch 306/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1259\n",
            "Epoch 307/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1257\n",
            "Epoch 308/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1255\n",
            "Epoch 309/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1253\n",
            "Epoch 310/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1251\n",
            "Epoch 311/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1249\n",
            "Epoch 312/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1247\n",
            "Epoch 313/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1245\n",
            "Epoch 314/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1243\n",
            "Epoch 315/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1242\n",
            "Epoch 316/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1240\n",
            "Epoch 317/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1238\n",
            "Epoch 318/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1236\n",
            "Epoch 319/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1235\n",
            "Epoch 320/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1233\n",
            "Epoch 321/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1231\n",
            "Epoch 322/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1230\n",
            "Epoch 323/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1228\n",
            "Epoch 324/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1227\n",
            "Epoch 325/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1225\n",
            "Epoch 326/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1224\n",
            "Epoch 327/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1222\n",
            "Epoch 328/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1221\n",
            "Epoch 329/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1219\n",
            "Epoch 330/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1218\n",
            "Epoch 331/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1216\n",
            "Epoch 332/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1215\n",
            "Epoch 333/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1214\n",
            "Epoch 334/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1212\n",
            "Epoch 335/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1211\n",
            "Epoch 336/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1209\n",
            "Epoch 337/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1208\n",
            "Epoch 338/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1207\n",
            "Epoch 339/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1206\n",
            "Epoch 340/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1204\n",
            "Epoch 341/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1203\n",
            "Epoch 342/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1202\n",
            "Epoch 343/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1201\n",
            "Epoch 344/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1199\n",
            "Epoch 345/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1198\n",
            "Epoch 346/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1197\n",
            "Epoch 347/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1196\n",
            "Epoch 348/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1194\n",
            "Epoch 349/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1193\n",
            "Epoch 350/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1192\n",
            "Epoch 351/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1191\n",
            "Epoch 352/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1190\n",
            "Epoch 353/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1189\n",
            "Epoch 354/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1188\n",
            "Epoch 355/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1186\n",
            "Epoch 356/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1185\n",
            "Epoch 357/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1184\n",
            "Epoch 358/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1183\n",
            "Epoch 359/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1182\n",
            "Epoch 360/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1181\n",
            "Epoch 361/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1180\n",
            "Epoch 362/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1179\n",
            "Epoch 363/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1178\n",
            "Epoch 364/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1176\n",
            "Epoch 365/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1175\n",
            "Epoch 366/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1174\n",
            "Epoch 367/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1173\n",
            "Epoch 368/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1172\n",
            "Epoch 369/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1171\n",
            "Epoch 370/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1170\n",
            "Epoch 371/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1169\n",
            "Epoch 372/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1168\n",
            "Epoch 373/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1167\n",
            "Epoch 374/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1166\n",
            "Epoch 375/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1165\n",
            "Epoch 376/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1164\n",
            "Epoch 377/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1163\n",
            "Epoch 378/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1162\n",
            "Epoch 379/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1161\n",
            "Epoch 380/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1160\n",
            "Epoch 381/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1159\n",
            "Epoch 382/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1158\n",
            "Epoch 383/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1157\n",
            "Epoch 384/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1156\n",
            "Epoch 385/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1155\n",
            "Epoch 386/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1154\n",
            "Epoch 387/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1153\n",
            "Epoch 388/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1152\n",
            "Epoch 389/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1151\n",
            "Epoch 390/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1150\n",
            "Epoch 391/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1149\n",
            "Epoch 392/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1148\n",
            "Epoch 393/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1148\n",
            "Epoch 394/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1147\n",
            "Epoch 395/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1146\n",
            "Epoch 396/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1145\n",
            "Epoch 397/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1144\n",
            "Epoch 398/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1143\n",
            "Epoch 399/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1142\n",
            "Epoch 400/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1141\n",
            "Epoch 401/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1140\n",
            "Epoch 402/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1139\n",
            "Epoch 403/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1138\n",
            "Epoch 404/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1137\n",
            "Epoch 405/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1136\n",
            "Epoch 406/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1135\n",
            "Epoch 407/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1134\n",
            "Epoch 408/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1133\n",
            "Epoch 409/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1132\n",
            "Epoch 410/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1132\n",
            "Epoch 411/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.1131\n",
            "Epoch 412/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1130\n",
            "Epoch 413/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1129\n",
            "Epoch 414/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1128\n",
            "Epoch 415/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1127\n",
            "Epoch 416/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1126\n",
            "Epoch 417/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1125\n",
            "Epoch 418/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1124\n",
            "Epoch 419/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1123\n",
            "Epoch 420/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1123\n",
            "Epoch 421/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1122\n",
            "Epoch 422/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1121\n",
            "Epoch 423/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1120\n",
            "Epoch 424/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1119\n",
            "Epoch 425/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1118\n",
            "Epoch 426/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1117\n",
            "Epoch 427/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1116\n",
            "Epoch 428/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1116\n",
            "Epoch 429/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1114\n",
            "Epoch 430/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1114\n",
            "Epoch 431/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1113\n",
            "Epoch 432/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1112\n",
            "Epoch 433/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1111\n",
            "Epoch 434/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1110\n",
            "Epoch 435/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1109\n",
            "Epoch 436/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1108\n",
            "Epoch 437/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1107\n",
            "Epoch 438/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1106\n",
            "Epoch 439/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1106\n",
            "Epoch 440/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1105\n",
            "Epoch 441/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1104\n",
            "Epoch 442/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1103\n",
            "Epoch 443/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1102\n",
            "Epoch 444/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1101\n",
            "Epoch 445/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1100\n",
            "Epoch 446/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1099\n",
            "Epoch 447/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1098\n",
            "Epoch 448/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1097\n",
            "Epoch 449/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1097\n",
            "Epoch 450/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1096\n",
            "Epoch 451/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1095\n",
            "Epoch 452/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1094\n",
            "Epoch 453/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1093\n",
            "Epoch 454/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1092\n",
            "Epoch 455/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1091\n",
            "Epoch 456/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1090\n",
            "Epoch 457/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1089\n",
            "Epoch 458/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1088\n",
            "Epoch 459/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1087\n",
            "Epoch 460/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1087\n",
            "Epoch 461/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1086\n",
            "Epoch 462/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1085\n",
            "Epoch 463/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1084\n",
            "Epoch 464/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1083\n",
            "Epoch 465/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1082\n",
            "Epoch 466/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1081\n",
            "Epoch 467/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1081\n",
            "Epoch 468/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1080\n",
            "Epoch 469/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1079\n",
            "Epoch 470/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1078\n",
            "Epoch 471/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1078\n",
            "Epoch 472/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1077\n",
            "Epoch 473/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1076\n",
            "Epoch 474/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1074\n",
            "Epoch 475/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1073\n",
            "Epoch 476/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1072\n",
            "Epoch 477/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1071\n",
            "Epoch 478/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1071\n",
            "Epoch 479/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1070\n",
            "Epoch 480/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1069\n",
            "Epoch 481/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1068\n",
            "Epoch 482/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1067\n",
            "Epoch 483/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1066\n",
            "Epoch 484/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1065\n",
            "Epoch 485/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1064\n",
            "Epoch 486/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1063\n",
            "Epoch 487/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1063\n",
            "Epoch 488/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1062\n",
            "Epoch 489/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1062\n",
            "Epoch 490/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1061\n",
            "Epoch 491/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1060\n",
            "Epoch 492/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1059\n",
            "Epoch 493/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1058\n",
            "Epoch 494/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1057\n",
            "Epoch 495/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1056\n",
            "Epoch 496/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1055\n",
            "Epoch 497/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1053\n",
            "Epoch 498/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1052\n",
            "Epoch 499/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1051\n",
            "Epoch 500/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1050\n",
            "Epoch 501/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1049\n",
            "Epoch 502/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1048\n",
            "Epoch 503/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1047\n",
            "Epoch 504/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1046\n",
            "Epoch 505/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1045\n",
            "Epoch 506/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1044\n",
            "Epoch 507/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1043\n",
            "Epoch 508/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1042\n",
            "Epoch 509/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1041\n",
            "Epoch 510/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1040\n",
            "Epoch 511/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1039\n",
            "Epoch 512/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1038\n",
            "Epoch 513/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1037\n",
            "Epoch 514/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1036\n",
            "Epoch 515/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1035\n",
            "Epoch 516/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1034\n",
            "Epoch 517/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1033\n",
            "Epoch 518/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1032\n",
            "Epoch 519/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1031\n",
            "Epoch 520/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1030\n",
            "Epoch 521/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1030\n",
            "Epoch 522/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1029\n",
            "Epoch 523/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1028\n",
            "Epoch 524/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1027\n",
            "Epoch 525/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1026\n",
            "Epoch 526/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1025\n",
            "Epoch 527/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1024\n",
            "Epoch 528/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1024\n",
            "Epoch 529/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1023\n",
            "Epoch 530/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1022\n",
            "Epoch 531/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1021\n",
            "Epoch 532/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1020\n",
            "Epoch 533/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1019\n",
            "Epoch 534/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1018\n",
            "Epoch 535/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1018\n",
            "Epoch 536/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1017\n",
            "Epoch 537/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1016\n",
            "Epoch 538/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1015\n",
            "Epoch 539/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1014\n",
            "Epoch 540/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1013\n",
            "Epoch 541/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1013: 0s - loss: 0.11\n",
            "Epoch 542/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1012\n",
            "Epoch 543/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1012\n",
            "Epoch 544/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1011\n",
            "Epoch 545/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1011\n",
            "Epoch 546/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1010\n",
            "Epoch 547/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1010\n",
            "Epoch 548/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1009\n",
            "Epoch 549/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1009\n",
            "Epoch 550/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1008\n",
            "Epoch 551/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1007\n",
            "Epoch 552/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1007\n",
            "Epoch 553/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.1006\n",
            "Epoch 554/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1005\n",
            "Epoch 555/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1005\n",
            "Epoch 556/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1004\n",
            "Epoch 557/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.1003\n",
            "Epoch 558/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1002\n",
            "Epoch 559/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1001\n",
            "Epoch 560/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1001\n",
            "Epoch 561/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.1000\n",
            "Epoch 562/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1000\n",
            "Epoch 563/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0999\n",
            "Epoch 564/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0998\n",
            "Epoch 565/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0998\n",
            "Epoch 566/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0998\n",
            "Epoch 567/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0997\n",
            "Epoch 568/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0997\n",
            "Epoch 569/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0997\n",
            "Epoch 570/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0997\n",
            "Epoch 571/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0997\n",
            "Epoch 572/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0996\n",
            "Epoch 573/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0996\n",
            "Epoch 574/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0995\n",
            "Epoch 575/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0995\n",
            "Epoch 576/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0994\n",
            "Epoch 577/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0993\n",
            "Epoch 578/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0993\n",
            "Epoch 579/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0992\n",
            "Epoch 580/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0991\n",
            "Epoch 581/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0991\n",
            "Epoch 582/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0990\n",
            "Epoch 583/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0990\n",
            "Epoch 584/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0989\n",
            "Epoch 585/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0989\n",
            "Epoch 586/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0988\n",
            "Epoch 587/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0988\n",
            "Epoch 588/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0987\n",
            "Epoch 589/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0986\n",
            "Epoch 590/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0985\n",
            "Epoch 591/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0985\n",
            "Epoch 592/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0984\n",
            "Epoch 593/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0984\n",
            "Epoch 594/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0983\n",
            "Epoch 595/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.0983\n",
            "Epoch 596/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0982\n",
            "Epoch 597/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0981\n",
            "Epoch 598/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0981\n",
            "Epoch 599/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0980\n",
            "Epoch 600/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0979\n",
            "Epoch 601/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0979\n",
            "Epoch 602/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0978\n",
            "Epoch 603/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0978\n",
            "Epoch 604/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0977\n",
            "Epoch 605/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0977\n",
            "Epoch 606/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0976\n",
            "Epoch 607/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0976\n",
            "Epoch 608/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0975\n",
            "Epoch 609/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0974\n",
            "Epoch 610/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0974\n",
            "Epoch 611/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0973\n",
            "Epoch 612/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0973\n",
            "Epoch 613/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0972\n",
            "Epoch 614/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0972\n",
            "Epoch 615/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0971\n",
            "Epoch 616/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0971\n",
            "Epoch 617/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0970\n",
            "Epoch 618/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0969\n",
            "Epoch 619/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0969\n",
            "Epoch 620/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0968\n",
            "Epoch 621/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0968\n",
            "Epoch 622/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0967\n",
            "Epoch 623/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0967\n",
            "Epoch 624/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0966\n",
            "Epoch 625/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0966\n",
            "Epoch 626/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0966\n",
            "Epoch 627/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0965\n",
            "Epoch 628/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0964\n",
            "Epoch 629/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0964\n",
            "Epoch 630/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0964\n",
            "Epoch 631/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0963\n",
            "Epoch 632/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0962\n",
            "Epoch 633/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0962\n",
            "Epoch 634/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0962\n",
            "Epoch 635/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0961\n",
            "Epoch 636/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0960\n",
            "Epoch 637/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0960\n",
            "Epoch 638/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0959\n",
            "Epoch 639/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0959\n",
            "Epoch 640/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0959\n",
            "Epoch 641/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0959\n",
            "Epoch 642/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0958\n",
            "Epoch 643/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0958\n",
            "Epoch 644/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0958\n",
            "Epoch 645/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0957\n",
            "Epoch 646/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0957\n",
            "Epoch 647/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0956\n",
            "Epoch 648/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0956\n",
            "Epoch 649/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0955\n",
            "Epoch 650/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0955\n",
            "Epoch 651/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0954\n",
            "Epoch 652/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0954\n",
            "Epoch 653/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0953\n",
            "Epoch 654/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0953\n",
            "Epoch 655/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0952\n",
            "Epoch 656/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0952\n",
            "Epoch 657/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0952\n",
            "Epoch 658/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0951\n",
            "Epoch 659/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0951\n",
            "Epoch 660/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0950\n",
            "Epoch 661/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0950\n",
            "Epoch 662/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0950\n",
            "Epoch 663/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0949\n",
            "Epoch 664/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0949\n",
            "Epoch 665/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0948\n",
            "Epoch 666/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0948\n",
            "Epoch 667/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0948\n",
            "Epoch 668/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0947\n",
            "Epoch 669/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0947\n",
            "Epoch 670/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0946\n",
            "Epoch 671/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0946\n",
            "Epoch 672/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0945\n",
            "Epoch 673/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0945\n",
            "Epoch 674/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0945\n",
            "Epoch 675/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0944\n",
            "Epoch 676/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0944\n",
            "Epoch 677/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0944\n",
            "Epoch 678/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0943\n",
            "Epoch 679/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0943\n",
            "Epoch 680/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0942\n",
            "Epoch 681/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0941\n",
            "Epoch 682/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0941\n",
            "Epoch 683/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0941\n",
            "Epoch 684/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0940\n",
            "Epoch 685/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0940\n",
            "Epoch 686/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0939\n",
            "Epoch 687/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0939\n",
            "Epoch 688/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0939\n",
            "Epoch 689/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0938\n",
            "Epoch 690/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0938\n",
            "Epoch 691/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0938\n",
            "Epoch 692/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0937\n",
            "Epoch 693/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0937\n",
            "Epoch 694/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0937\n",
            "Epoch 695/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0936\n",
            "Epoch 696/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0936\n",
            "Epoch 697/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0935\n",
            "Epoch 698/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0935\n",
            "Epoch 699/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0934\n",
            "Epoch 700/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0934\n",
            "Epoch 701/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0934\n",
            "Epoch 702/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0934\n",
            "Epoch 703/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0933\n",
            "Epoch 704/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0933\n",
            "Epoch 705/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0932\n",
            "Epoch 706/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0932\n",
            "Epoch 707/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0931\n",
            "Epoch 708/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0931\n",
            "Epoch 709/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0930\n",
            "Epoch 710/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0930\n",
            "Epoch 711/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0930\n",
            "Epoch 712/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0930\n",
            "Epoch 713/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0929\n",
            "Epoch 714/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0929\n",
            "Epoch 715/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0929\n",
            "Epoch 716/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0928\n",
            "Epoch 717/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0928\n",
            "Epoch 718/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0927\n",
            "Epoch 719/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0927\n",
            "Epoch 720/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0926\n",
            "Epoch 721/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0926\n",
            "Epoch 722/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0925\n",
            "Epoch 723/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0925\n",
            "Epoch 724/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0925\n",
            "Epoch 725/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0924\n",
            "Epoch 726/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0924\n",
            "Epoch 727/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0924\n",
            "Epoch 728/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0924\n",
            "Epoch 729/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0923\n",
            "Epoch 730/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0923\n",
            "Epoch 731/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0923\n",
            "Epoch 732/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0922\n",
            "Epoch 733/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0922\n",
            "Epoch 734/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0921\n",
            "Epoch 735/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0921\n",
            "Epoch 736/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0920\n",
            "Epoch 737/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0920\n",
            "Epoch 738/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0919\n",
            "Epoch 739/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0919\n",
            "Epoch 740/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0919\n",
            "Epoch 741/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0918\n",
            "Epoch 742/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0918\n",
            "Epoch 743/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0918\n",
            "Epoch 744/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0917\n",
            "Epoch 745/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0917\n",
            "Epoch 746/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0917\n",
            "Epoch 747/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0917\n",
            "Epoch 748/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0916\n",
            "Epoch 749/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0916\n",
            "Epoch 750/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0916\n",
            "Epoch 751/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0915\n",
            "Epoch 752/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0915\n",
            "Epoch 753/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0915\n",
            "Epoch 754/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0914\n",
            "Epoch 755/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0914\n",
            "Epoch 756/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0913\n",
            "Epoch 757/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0913\n",
            "Epoch 758/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0913\n",
            "Epoch 759/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0912\n",
            "Epoch 760/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0912\n",
            "Epoch 761/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0912\n",
            "Epoch 762/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0912\n",
            "Epoch 763/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0911\n",
            "Epoch 764/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0911\n",
            "Epoch 765/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0911\n",
            "Epoch 766/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0910\n",
            "Epoch 767/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0910\n",
            "Epoch 768/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0910\n",
            "Epoch 769/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0909\n",
            "Epoch 770/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0909\n",
            "Epoch 771/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0908\n",
            "Epoch 772/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0908\n",
            "Epoch 773/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0908\n",
            "Epoch 774/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0908\n",
            "Epoch 775/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0907\n",
            "Epoch 776/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0907\n",
            "Epoch 777/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0907\n",
            "Epoch 778/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0907\n",
            "Epoch 779/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0906\n",
            "Epoch 780/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0906\n",
            "Epoch 781/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0905\n",
            "Epoch 782/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0905\n",
            "Epoch 783/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0905\n",
            "Epoch 784/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 785/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 786/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 787/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 788/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 789/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0904\n",
            "Epoch 790/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0903\n",
            "Epoch 791/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0903\n",
            "Epoch 792/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0903\n",
            "Epoch 793/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0902\n",
            "Epoch 794/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0902\n",
            "Epoch 795/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0902\n",
            "Epoch 796/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0901\n",
            "Epoch 797/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0901\n",
            "Epoch 798/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0900\n",
            "Epoch 799/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0900\n",
            "Epoch 800/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0899\n",
            "Epoch 801/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0899\n",
            "Epoch 802/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0899\n",
            "Epoch 803/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0898\n",
            "Epoch 804/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0898\n",
            "Epoch 805/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0898\n",
            "Epoch 806/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0898\n",
            "Epoch 807/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0897\n",
            "Epoch 808/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0897\n",
            "Epoch 809/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0897\n",
            "Epoch 810/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0897\n",
            "Epoch 811/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0896\n",
            "Epoch 812/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0896\n",
            "Epoch 813/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0896\n",
            "Epoch 814/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0896\n",
            "Epoch 815/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0895\n",
            "Epoch 816/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0895\n",
            "Epoch 817/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0895\n",
            "Epoch 818/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0894\n",
            "Epoch 819/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0894\n",
            "Epoch 820/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0894\n",
            "Epoch 821/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0894\n",
            "Epoch 822/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0893\n",
            "Epoch 823/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0893\n",
            "Epoch 824/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0893\n",
            "Epoch 825/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0893\n",
            "Epoch 826/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0892\n",
            "Epoch 827/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0892\n",
            "Epoch 828/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0892\n",
            "Epoch 829/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0892\n",
            "Epoch 830/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0891\n",
            "Epoch 831/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0891\n",
            "Epoch 832/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0891\n",
            "Epoch 833/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0890\n",
            "Epoch 834/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0890\n",
            "Epoch 835/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0890\n",
            "Epoch 836/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0890\n",
            "Epoch 837/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0889\n",
            "Epoch 838/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0889\n",
            "Epoch 839/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0889\n",
            "Epoch 840/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0888\n",
            "Epoch 841/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0888\n",
            "Epoch 842/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0888\n",
            "Epoch 843/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0887\n",
            "Epoch 844/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0887\n",
            "Epoch 845/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0887\n",
            "Epoch 846/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0887\n",
            "Epoch 847/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0886\n",
            "Epoch 848/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0886\n",
            "Epoch 849/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0886\n",
            "Epoch 850/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0886\n",
            "Epoch 851/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0885\n",
            "Epoch 852/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0885\n",
            "Epoch 853/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0885\n",
            "Epoch 854/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0885\n",
            "Epoch 855/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0884\n",
            "Epoch 856/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0884\n",
            "Epoch 857/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0884\n",
            "Epoch 858/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0883\n",
            "Epoch 859/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0883\n",
            "Epoch 860/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0883\n",
            "Epoch 861/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0883\n",
            "Epoch 862/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0882\n",
            "Epoch 863/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0882\n",
            "Epoch 864/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0882\n",
            "Epoch 865/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0882\n",
            "Epoch 866/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0882\n",
            "Epoch 867/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0881\n",
            "Epoch 868/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0881\n",
            "Epoch 869/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0881\n",
            "Epoch 870/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0881\n",
            "Epoch 871/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0880\n",
            "Epoch 872/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0880\n",
            "Epoch 873/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0880\n",
            "Epoch 874/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0879\n",
            "Epoch 875/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0879\n",
            "Epoch 876/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0879\n",
            "Epoch 877/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0879\n",
            "Epoch 878/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0878\n",
            "Epoch 879/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0878\n",
            "Epoch 880/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0878\n",
            "Epoch 881/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0878\n",
            "Epoch 882/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0877\n",
            "Epoch 883/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0877\n",
            "Epoch 884/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0877\n",
            "Epoch 885/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0877\n",
            "Epoch 886/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0877\n",
            "Epoch 887/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0876\n",
            "Epoch 888/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0876\n",
            "Epoch 889/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0876\n",
            "Epoch 890/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0876\n",
            "Epoch 891/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0875\n",
            "Epoch 892/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0875\n",
            "Epoch 893/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0875\n",
            "Epoch 894/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0875\n",
            "Epoch 895/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0875\n",
            "Epoch 896/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0874\n",
            "Epoch 897/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0874\n",
            "Epoch 898/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0874\n",
            "Epoch 899/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0874\n",
            "Epoch 900/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0873\n",
            "Epoch 901/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0873\n",
            "Epoch 902/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0873\n",
            "Epoch 903/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0873\n",
            "Epoch 904/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0873\n",
            "Epoch 905/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0872\n",
            "Epoch 906/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0872\n",
            "Epoch 907/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0872\n",
            "Epoch 908/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0872\n",
            "Epoch 909/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0871\n",
            "Epoch 910/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0871\n",
            "Epoch 911/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0871\n",
            "Epoch 912/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0871\n",
            "Epoch 913/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0871\n",
            "Epoch 914/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0870\n",
            "Epoch 915/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0870\n",
            "Epoch 916/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0870\n",
            "Epoch 917/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0870\n",
            "Epoch 918/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0870\n",
            "Epoch 919/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0869\n",
            "Epoch 920/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0869\n",
            "Epoch 921/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0869\n",
            "Epoch 922/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0869\n",
            "Epoch 923/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0868\n",
            "Epoch 924/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0868\n",
            "Epoch 925/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0868\n",
            "Epoch 926/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0868\n",
            "Epoch 927/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0868\n",
            "Epoch 928/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0868\n",
            "Epoch 929/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0867\n",
            "Epoch 930/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0867\n",
            "Epoch 931/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0867\n",
            "Epoch 932/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0867\n",
            "Epoch 933/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0867\n",
            "Epoch 934/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0866\n",
            "Epoch 935/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0866\n",
            "Epoch 936/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0866\n",
            "Epoch 937/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0866\n",
            "Epoch 938/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0866\n",
            "Epoch 939/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0865\n",
            "Epoch 940/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0865\n",
            "Epoch 941/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0865\n",
            "Epoch 942/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0865\n",
            "Epoch 943/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0864\n",
            "Epoch 944/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0864\n",
            "Epoch 945/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0864\n",
            "Epoch 946/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0864\n",
            "Epoch 947/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0864\n",
            "Epoch 948/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0864\n",
            "Epoch 949/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 950/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 951/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 952/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 953/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 954/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 955/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0863\n",
            "Epoch 956/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0862\n",
            "Epoch 957/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0862\n",
            "Epoch 958/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0862\n",
            "Epoch 959/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0862\n",
            "Epoch 960/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0861\n",
            "Epoch 961/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0861\n",
            "Epoch 962/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0861\n",
            "Epoch 963/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0861\n",
            "Epoch 964/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0861\n",
            "Epoch 965/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0861\n",
            "Epoch 966/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0861\n",
            "Epoch 967/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0860\n",
            "Epoch 968/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0860\n",
            "Epoch 969/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0860\n",
            "Epoch 970/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0860\n",
            "Epoch 971/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0860\n",
            "Epoch 972/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0859\n",
            "Epoch 973/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0859\n",
            "Epoch 974/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0859\n",
            "Epoch 975/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0859\n",
            "Epoch 976/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0859\n",
            "Epoch 977/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0859\n",
            "Epoch 978/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0858\n",
            "Epoch 979/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0858\n",
            "Epoch 980/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0858\n",
            "Epoch 981/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0858\n",
            "Epoch 982/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0857\n",
            "Epoch 983/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0857\n",
            "Epoch 984/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0857\n",
            "Epoch 985/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0857\n",
            "Epoch 986/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0857\n",
            "Epoch 987/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0856\n",
            "Epoch 988/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0856\n",
            "Epoch 989/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0856\n",
            "Epoch 990/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0856\n",
            "Epoch 991/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0856\n",
            "Epoch 992/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0855\n",
            "Epoch 993/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0855\n",
            "Epoch 994/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0855\n",
            "Epoch 995/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0855\n",
            "Epoch 996/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0855\n",
            "Epoch 997/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0855\n",
            "Epoch 998/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0854\n",
            "Epoch 999/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0854\n",
            "Epoch 1000/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0854\n",
            "Epoch 1001/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0853\n",
            "Epoch 1002/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0853\n",
            "Epoch 1003/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0853\n",
            "Epoch 1004/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0853\n",
            "Epoch 1005/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0853\n",
            "Epoch 1006/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0853\n",
            "Epoch 1007/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0852\n",
            "Epoch 1008/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0852\n",
            "Epoch 1009/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0852\n",
            "Epoch 1010/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0852\n",
            "Epoch 1011/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0851\n",
            "Epoch 1012/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0851\n",
            "Epoch 1013/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0851\n",
            "Epoch 1014/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0851\n",
            "Epoch 1015/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0851\n",
            "Epoch 1016/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1017/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0850\n",
            "Epoch 1018/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1019/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1020/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1021/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1022/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1023/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1024/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1025/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 1026/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0849\n",
            "Epoch 1027/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0849\n",
            "Epoch 1028/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0849\n",
            "Epoch 1029/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0849\n",
            "Epoch 1030/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0849\n",
            "Epoch 1031/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0849\n",
            "Epoch 1032/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0848\n",
            "Epoch 1033/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0848\n",
            "Epoch 1034/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0848\n",
            "Epoch 1035/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0848\n",
            "Epoch 1036/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0848\n",
            "Epoch 1037/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0847\n",
            "Epoch 1038/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0847\n",
            "Epoch 1039/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0847\n",
            "Epoch 1040/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0847\n",
            "Epoch 1041/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0847\n",
            "Epoch 1042/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0846\n",
            "Epoch 1043/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0846\n",
            "Epoch 1044/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0846\n",
            "Epoch 1045/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 1046/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0845\n",
            "Epoch 1047/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0845\n",
            "Epoch 1048/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 1049/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 1050/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0845\n",
            "Epoch 1051/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 1052/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 1053/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0844\n",
            "Epoch 1054/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0844\n",
            "Epoch 1055/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0844\n",
            "Epoch 1056/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0844\n",
            "Epoch 1057/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0844\n",
            "Epoch 1058/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0844\n",
            "Epoch 1059/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0844\n",
            "Epoch 1060/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0844\n",
            "Epoch 1061/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0843\n",
            "Epoch 1062/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0843\n",
            "Epoch 1063/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0843\n",
            "Epoch 1064/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0843\n",
            "Epoch 1065/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0843\n",
            "Epoch 1066/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0843\n",
            "Epoch 1067/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0842\n",
            "Epoch 1068/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0842\n",
            "Epoch 1069/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0842\n",
            "Epoch 1070/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0842\n",
            "Epoch 1071/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0842\n",
            "Epoch 1072/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0842\n",
            "Epoch 1073/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1074/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1075/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1076/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1077/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0841\n",
            "Epoch 1078/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1079/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 1080/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0841\n",
            "Epoch 1081/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0841\n",
            "Epoch 1082/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0840\n",
            "Epoch 1083/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0840\n",
            "Epoch 1084/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0840\n",
            "Epoch 1085/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0840\n",
            "Epoch 1086/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0840\n",
            "Epoch 1087/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0840\n",
            "Epoch 1088/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0840\n",
            "Epoch 1089/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0839\n",
            "Epoch 1090/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0839\n",
            "Epoch 1091/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0839\n",
            "Epoch 1092/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0839\n",
            "Epoch 1093/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0839\n",
            "Epoch 1094/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0839\n",
            "Epoch 1095/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0839\n",
            "Epoch 1096/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0839\n",
            "Epoch 1097/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0838\n",
            "Epoch 1098/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0838\n",
            "Epoch 1099/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0838\n",
            "Epoch 1100/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0838\n",
            "Epoch 1101/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0838\n",
            "Epoch 1102/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0838\n",
            "Epoch 1103/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0838\n",
            "Epoch 1104/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0838\n",
            "Epoch 1105/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0837\n",
            "Epoch 1106/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0837\n",
            "Epoch 1107/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0837\n",
            "Epoch 1108/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0837\n",
            "Epoch 1109/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1110/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1111/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1112/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0836\n",
            "Epoch 1113/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0836\n",
            "Epoch 1114/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1115/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1116/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0836\n",
            "Epoch 1117/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0836\n",
            "Epoch 1118/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0835\n",
            "Epoch 1119/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0835\n",
            "Epoch 1120/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0835\n",
            "Epoch 1121/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0835\n",
            "Epoch 1122/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0834\n",
            "Epoch 1123/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0834\n",
            "Epoch 1124/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0834\n",
            "Epoch 1125/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0834\n",
            "Epoch 1126/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0834\n",
            "Epoch 1127/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0834\n",
            "Epoch 1128/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0834\n",
            "Epoch 1129/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0834\n",
            "Epoch 1130/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0834\n",
            "Epoch 1131/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0834\n",
            "Epoch 1132/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0833\n",
            "Epoch 1133/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0833\n",
            "Epoch 1134/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0833\n",
            "Epoch 1135/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0833\n",
            "Epoch 1136/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0833\n",
            "Epoch 1137/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0833\n",
            "Epoch 1138/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0833\n",
            "Epoch 1139/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0832\n",
            "Epoch 1140/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0832\n",
            "Epoch 1141/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0832\n",
            "Epoch 1142/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0832\n",
            "Epoch 1143/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0832\n",
            "Epoch 1144/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0832\n",
            "Epoch 1145/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1146/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1147/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1148/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0831\n",
            "Epoch 1149/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0831\n",
            "Epoch 1150/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0831\n",
            "Epoch 1151/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1152/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1153/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1154/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0831\n",
            "Epoch 1155/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0830\n",
            "Epoch 1156/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0830\n",
            "Epoch 1157/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0830\n",
            "Epoch 1158/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0830\n",
            "Epoch 1159/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0830\n",
            "Epoch 1160/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0829\n",
            "Epoch 1161/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0829\n",
            "Epoch 1162/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0829\n",
            "Epoch 1163/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0829\n",
            "Epoch 1164/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0829\n",
            "Epoch 1165/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0829\n",
            "Epoch 1166/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0828\n",
            "Epoch 1167/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0828\n",
            "Epoch 1168/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0828\n",
            "Epoch 1169/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0828\n",
            "Epoch 1170/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0828\n",
            "Epoch 1171/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0828\n",
            "Epoch 1172/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0828\n",
            "Epoch 1173/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0828\n",
            "Epoch 1174/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0827\n",
            "Epoch 1175/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0827\n",
            "Epoch 1176/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0827\n",
            "Epoch 1177/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0827\n",
            "Epoch 1178/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0827\n",
            "Epoch 1179/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0827\n",
            "Epoch 1180/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0827\n",
            "Epoch 1181/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0827\n",
            "Epoch 1182/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0827\n",
            "Epoch 1183/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0826\n",
            "Epoch 1184/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0826\n",
            "Epoch 1185/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0826\n",
            "Epoch 1186/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0826\n",
            "Epoch 1187/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0826\n",
            "Epoch 1188/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0826\n",
            "Epoch 1189/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0826\n",
            "Epoch 1190/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0825\n",
            "Epoch 1191/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0825\n",
            "Epoch 1192/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0825\n",
            "Epoch 1193/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0825\n",
            "Epoch 1194/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0825\n",
            "Epoch 1195/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0825\n",
            "Epoch 1196/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0825\n",
            "Epoch 1197/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0825\n",
            "Epoch 1198/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0825\n",
            "Epoch 1199/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0824\n",
            "Epoch 1200/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0824\n",
            "Epoch 1201/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0824\n",
            "Epoch 1202/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0824\n",
            "Epoch 1203/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1204/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1205/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0823\n",
            "Epoch 1206/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1207/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1208/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1209/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0823\n",
            "Epoch 1210/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1211/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1212/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1213/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1214/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0823\n",
            "Epoch 1215/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1216/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1217/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1218/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1219/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0822\n",
            "Epoch 1220/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1221/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0822\n",
            "Epoch 1222/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1223/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0822\n",
            "Epoch 1224/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0821\n",
            "Epoch 1225/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0821\n",
            "Epoch 1226/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0821\n",
            "Epoch 1227/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0821\n",
            "Epoch 1228/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1229/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1230/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1231/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1232/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1233/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1234/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1235/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 1236/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0819\n",
            "Epoch 1237/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0819\n",
            "Epoch 1238/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0819\n",
            "Epoch 1239/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0819\n",
            "Epoch 1240/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0819\n",
            "Epoch 1241/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0819\n",
            "Epoch 1242/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1243/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0818\n",
            "Epoch 1244/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1245/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1246/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818: 0s - loss: 0.08\n",
            "Epoch 1247/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1248/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1249/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1250/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0818\n",
            "Epoch 1251/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1252/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1253/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1254/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0817\n",
            "Epoch 1255/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1256/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1257/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1258/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1259/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 1260/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0816\n",
            "Epoch 1261/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0816\n",
            "Epoch 1262/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0816\n",
            "Epoch 1263/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0816\n",
            "Epoch 1264/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0816\n",
            "Epoch 1265/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0816\n",
            "Epoch 1266/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0816\n",
            "Epoch 1267/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0815\n",
            "Epoch 1268/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0815\n",
            "Epoch 1269/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0815\n",
            "Epoch 1270/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0815\n",
            "Epoch 1271/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0815\n",
            "Epoch 1272/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0815\n",
            "Epoch 1273/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0815\n",
            "Epoch 1274/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 1275/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 1276/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 1277/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0814\n",
            "Epoch 1278/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0814\n",
            "Epoch 1279/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 1280/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 1281/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0813\n",
            "Epoch 1282/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0813\n",
            "Epoch 1283/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0813\n",
            "Epoch 1284/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0813\n",
            "Epoch 1285/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0813\n",
            "Epoch 1286/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0813\n",
            "Epoch 1287/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0812\n",
            "Epoch 1288/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0812\n",
            "Epoch 1289/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0812\n",
            "Epoch 1290/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0812\n",
            "Epoch 1291/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0812\n",
            "Epoch 1292/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0812\n",
            "Epoch 1293/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0812\n",
            "Epoch 1294/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1295/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1296/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1297/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1298/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1299/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1300/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1301/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0811\n",
            "Epoch 1302/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0811\n",
            "Epoch 1303/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0811\n",
            "Epoch 1304/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0811\n",
            "Epoch 1305/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0811\n",
            "Epoch 1306/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0811\n",
            "Epoch 1307/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 1308/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 1309/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0810\n",
            "Epoch 1310/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 1311/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0810\n",
            "Epoch 1312/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0810\n",
            "Epoch 1313/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0810\n",
            "Epoch 1314/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0809\n",
            "Epoch 1315/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0809\n",
            "Epoch 1316/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0809\n",
            "Epoch 1317/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0809\n",
            "Epoch 1318/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0809\n",
            "Epoch 1319/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0809\n",
            "Epoch 1320/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0809\n",
            "Epoch 1321/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0809\n",
            "Epoch 1322/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1323/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1324/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0808\n",
            "Epoch 1325/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1326/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0808\n",
            "Epoch 1327/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0808\n",
            "Epoch 1328/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1329/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1330/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0807\n",
            "Epoch 1331/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0807\n",
            "Epoch 1332/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0807\n",
            "Epoch 1333/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0807\n",
            "Epoch 1334/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0807\n",
            "Epoch 1335/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0807\n",
            "Epoch 1336/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0806\n",
            "Epoch 1337/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0806\n",
            "Epoch 1338/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0806\n",
            "Epoch 1339/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0806\n",
            "Epoch 1340/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0805\n",
            "Epoch 1341/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0805\n",
            "Epoch 1342/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0805\n",
            "Epoch 1343/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0805\n",
            "Epoch 1344/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0805\n",
            "Epoch 1345/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0805\n",
            "Epoch 1346/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0805\n",
            "Epoch 1347/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1348/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0804\n",
            "Epoch 1349/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1350/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1351/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1352/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 1353/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1354/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1355/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 1356/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0804\n",
            "Epoch 1357/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 1358/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 1359/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0803\n",
            "Epoch 1360/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 1361/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0803\n",
            "Epoch 1362/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0803\n",
            "Epoch 1363/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0803\n",
            "Epoch 1364/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0803\n",
            "Epoch 1365/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0803\n",
            "Epoch 1366/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0803\n",
            "Epoch 1367/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0803\n",
            "Epoch 1368/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0803\n",
            "Epoch 1369/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0802\n",
            "Epoch 1370/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0802\n",
            "Epoch 1371/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0802\n",
            "Epoch 1372/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0802\n",
            "Epoch 1373/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0802\n",
            "Epoch 1374/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0802\n",
            "Epoch 1375/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0802\n",
            "Epoch 1376/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0801\n",
            "Epoch 1377/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0801\n",
            "Epoch 1378/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0801\n",
            "Epoch 1379/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0801\n",
            "Epoch 1380/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0801\n",
            "Epoch 1381/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0800\n",
            "Epoch 1382/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0800\n",
            "Epoch 1383/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0800\n",
            "Epoch 1384/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0800\n",
            "Epoch 1385/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0800\n",
            "Epoch 1386/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0800\n",
            "Epoch 1387/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1388/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0799\n",
            "Epoch 1389/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1390/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1391/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1392/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1393/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0799\n",
            "Epoch 1394/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1395/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0799\n",
            "Epoch 1396/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1397/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0799\n",
            "Epoch 1398/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1399/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0799\n",
            "Epoch 1400/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0799\n",
            "Epoch 1401/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1402/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0799\n",
            "Epoch 1403/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0799\n",
            "Epoch 1404/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0799\n",
            "Epoch 1405/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0798\n",
            "Epoch 1406/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0798\n",
            "Epoch 1407/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0798\n",
            "Epoch 1408/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0798\n",
            "Epoch 1409/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0798\n",
            "Epoch 1410/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0797\n",
            "Epoch 1411/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0797\n",
            "Epoch 1412/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0797\n",
            "Epoch 1413/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0797\n",
            "Epoch 1414/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0797\n",
            "Epoch 1415/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1416/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1417/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1418/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1419/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1420/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0796\n",
            "Epoch 1421/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0795\n",
            "Epoch 1422/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0795\n",
            "Epoch 1423/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0795\n",
            "Epoch 1424/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0794\n",
            "Epoch 1425/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0794\n",
            "Epoch 1426/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0794\n",
            "Epoch 1427/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0794\n",
            "Epoch 1428/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0794\n",
            "Epoch 1429/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0793\n",
            "Epoch 1430/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0793\n",
            "Epoch 1431/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0793\n",
            "Epoch 1432/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0793\n",
            "Epoch 1433/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0793\n",
            "Epoch 1434/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0793\n",
            "Epoch 1435/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0793\n",
            "Epoch 1436/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0793\n",
            "Epoch 1437/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1438/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1439/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1440/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1441/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1442/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1443/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1444/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1445/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1446/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1447/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1448/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1449/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1450/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1451/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1452/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1453/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1454/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1455/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1456/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0792\n",
            "Epoch 1457/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0792\n",
            "Epoch 1458/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0791\n",
            "Epoch 1459/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0791\n",
            "Epoch 1460/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0791\n",
            "Epoch 1461/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0791\n",
            "Epoch 1462/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0791\n",
            "Epoch 1463/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0791\n",
            "Epoch 1464/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0791\n",
            "Epoch 1465/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1466/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1467/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1468/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1469/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1470/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0790\n",
            "Epoch 1471/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0789\n",
            "Epoch 1472/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0789\n",
            "Epoch 1473/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0789\n",
            "Epoch 1474/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0789\n",
            "Epoch 1475/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0788\n",
            "Epoch 1476/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 1477/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 1478/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 1479/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0788\n",
            "Epoch 1480/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0788\n",
            "Epoch 1481/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 1482/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0787\n",
            "Epoch 1483/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 1484/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0787\n",
            "Epoch 1485/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0787\n",
            "Epoch 1486/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0787\n",
            "Epoch 1487/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0787\n",
            "Epoch 1488/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0787\n",
            "Epoch 1489/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0787\n",
            "Epoch 1490/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0787\n",
            "Epoch 1491/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0786\n",
            "Epoch 1492/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0786\n",
            "Epoch 1493/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0786\n",
            "Epoch 1494/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0786\n",
            "Epoch 1495/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0786\n",
            "Epoch 1496/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0785\n",
            "Epoch 1497/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0785\n",
            "Epoch 1498/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0785\n",
            "Epoch 1499/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0785\n",
            "Epoch 1500/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0785\n",
            "Epoch 1501/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0785\n",
            "Epoch 1502/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0785\n",
            "Epoch 1503/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0784\n",
            "Epoch 1504/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1505/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0784\n",
            "Epoch 1506/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1507/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1508/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1509/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1510/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 1511/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1512/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0783\n",
            "Epoch 1513/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1514/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1515/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1516/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0783\n",
            "Epoch 1517/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1518/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1519/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0783\n",
            "Epoch 1520/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1521/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0783\n",
            "Epoch 1522/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0782\n",
            "Epoch 1523/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0782\n",
            "Epoch 1524/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0782\n",
            "Epoch 1525/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0782\n",
            "Epoch 1526/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0782\n",
            "Epoch 1527/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0782\n",
            "Epoch 1528/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0781\n",
            "Epoch 1529/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0781\n",
            "Epoch 1530/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0781\n",
            "Epoch 1531/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0781\n",
            "Epoch 1532/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0781\n",
            "Epoch 1533/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0781\n",
            "Epoch 1534/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0781\n",
            "Epoch 1535/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1536/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1537/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1538/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1539/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1540/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1541/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0780\n",
            "Epoch 1542/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1543/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1544/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1545/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1546/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1547/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1548/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1549/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1550/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0780\n",
            "Epoch 1551/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1552/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0779\n",
            "Epoch 1553/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1554/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1555/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1556/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0779\n",
            "Epoch 1557/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1558/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1559/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1560/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0778\n",
            "Epoch 1561/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 1562/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0779\n",
            "Epoch 1563/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0779\n",
            "Epoch 1564/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0779\n",
            "Epoch 1565/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0778\n",
            "Epoch 1566/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1567/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1568/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1569/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1570/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1571/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1572/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0777\n",
            "Epoch 1573/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0777\n",
            "Epoch 1574/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1575/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1576/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1577/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1578/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1579/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1580/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1581/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0775\n",
            "Epoch 1582/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1583/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0775\n",
            "Epoch 1584/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1585/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1586/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1587/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1588/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1589/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1590/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1591/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1592/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1593/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1594/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1595/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1596/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1597/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1598/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1599/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1600/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1601/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1602/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1603/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1604/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1605/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1606/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1607/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1608/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1609/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1610/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0773\n",
            "Epoch 1611/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1612/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1613/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0774\n",
            "Epoch 1614/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0774\n",
            "Epoch 1615/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1616/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1617/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1618/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1619/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0774\n",
            "Epoch 1620/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1621/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1622/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1623/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1624/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1625/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1626/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1627/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0776\n",
            "Epoch 1628/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1629/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0778\n",
            "Epoch 1630/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0780\n",
            "Epoch 1631/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0785\n",
            "Epoch 1632/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0793\n",
            "Epoch 1633/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0808\n",
            "Epoch 1634/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0828\n",
            "Epoch 1635/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0845\n",
            "Epoch 1636/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0843\n",
            "Epoch 1637/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0829\n",
            "Epoch 1638/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 1639/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0807\n",
            "Epoch 1640/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0792\n",
            "Epoch 1641/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0781\n",
            "Epoch 1642/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0775\n",
            "Epoch 1643/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0772\n",
            "Epoch 1644/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0771\n",
            "Epoch 1645/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1646/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1647/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1648/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0774\n",
            "Epoch 1649/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0775\n",
            "Epoch 1650/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0776\n",
            "Epoch 1651/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0777\n",
            "Epoch 1652/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0777\n",
            "Epoch 1653/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1654/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1655/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0777\n",
            "Epoch 1656/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0776\n",
            "Epoch 1657/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0776\n",
            "Epoch 1658/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0775\n",
            "Epoch 1659/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0775\n",
            "Epoch 1660/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 1661/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1662/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1663/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0773\n",
            "Epoch 1664/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0773\n",
            "Epoch 1665/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1666/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1667/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0772\n",
            "Epoch 1668/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0772\n",
            "Epoch 1669/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0772\n",
            "Epoch 1670/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1671/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1672/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1673/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0772\n",
            "Epoch 1674/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1675/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0771\n",
            "Epoch 1676/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1677/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0771\n",
            "Epoch 1678/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1679/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0771\n",
            "Epoch 1680/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0771\n",
            "Epoch 1681/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1682/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0771\n",
            "Epoch 1683/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1684/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0771\n",
            "Epoch 1685/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0770\n",
            "Epoch 1686/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0770\n",
            "Epoch 1687/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0770\n",
            "Epoch 1688/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0770\n",
            "Epoch 1689/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0770\n",
            "Epoch 1690/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0770\n",
            "Epoch 1691/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0770\n",
            "Epoch 1692/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1693/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1694/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1695/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1696/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1697/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1698/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1699/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1700/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1701/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1702/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1703/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1704/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1705/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1706/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0769\n",
            "Epoch 1707/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1708/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0769\n",
            "Epoch 1709/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0768\n",
            "Epoch 1710/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0768\n",
            "Epoch 1711/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0768\n",
            "Epoch 1712/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0768\n",
            "Epoch 1713/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0768\n",
            "Epoch 1714/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0768\n",
            "Epoch 1715/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0767\n",
            "Epoch 1716/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0767\n",
            "Epoch 1717/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0767\n",
            "Epoch 1718/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0767\n",
            "Epoch 1719/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0767\n",
            "Epoch 1720/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0767\n",
            "Epoch 1721/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0767\n",
            "Epoch 1722/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0766\n",
            "Epoch 1723/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0766\n",
            "Epoch 1724/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0766\n",
            "Epoch 1725/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0766\n",
            "Epoch 1726/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0766\n",
            "Epoch 1727/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0766\n",
            "Epoch 1728/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 1729/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0765\n",
            "Epoch 1730/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 1731/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 1732/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 1733/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0764\n",
            "Epoch 1734/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0764\n",
            "Epoch 1735/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0764\n",
            "Epoch 1736/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0764\n",
            "Epoch 1737/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0764\n",
            "Epoch 1738/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0763\n",
            "Epoch 1739/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0763\n",
            "Epoch 1740/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0763\n",
            "Epoch 1741/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0763\n",
            "Epoch 1742/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0763\n",
            "Epoch 1743/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1744/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0763\n",
            "Epoch 1745/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0763\n",
            "Epoch 1746/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1747/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0763\n",
            "Epoch 1748/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1749/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1750/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0764\n",
            "Epoch 1751/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1752/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0763\n",
            "Epoch 1753/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0763\n",
            "Epoch 1754/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0763\n",
            "Epoch 1755/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0763\n",
            "Epoch 1756/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0763\n",
            "Epoch 1757/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0763\n",
            "Epoch 1758/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0762\n",
            "Epoch 1759/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1760/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0762\n",
            "Epoch 1761/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1762/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1763/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1764/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1765/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0762\n",
            "Epoch 1766/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0762\n",
            "Epoch 1767/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0762\n",
            "Epoch 1768/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0762\n",
            "Epoch 1769/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0762\n",
            "Epoch 1770/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0762\n",
            "Epoch 1771/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0761\n",
            "Epoch 1772/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1773/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1774/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0761\n",
            "Epoch 1775/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1776/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0761\n",
            "Epoch 1777/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1778/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1779/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1780/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1781/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0761\n",
            "Epoch 1782/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 1783/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 1784/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0759\n",
            "Epoch 1785/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1786/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0759\n",
            "Epoch 1787/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0759\n",
            "Epoch 1788/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0759\n",
            "Epoch 1789/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0759\n",
            "Epoch 1790/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0759\n",
            "Epoch 1791/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1792/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1793/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1794/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0759\n",
            "Epoch 1795/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1796/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 1797/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0760\n",
            "Epoch 1798/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0760\n",
            "Epoch 1799/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0760\n",
            "Epoch 1800/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0760\n",
            "Epoch 1801/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 1802/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0760\n",
            "Epoch 1803/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 1804/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1805/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0759\n",
            "Epoch 1806/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1807/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0759\n",
            "Epoch 1808/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0758\n",
            "Epoch 1809/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1810/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1811/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0757\n",
            "Epoch 1812/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0757\n",
            "Epoch 1813/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0757\n",
            "Epoch 1814/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0757\n",
            "Epoch 1815/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0757\n",
            "Epoch 1816/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0757\n",
            "Epoch 1817/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0757\n",
            "Epoch 1818/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0757\n",
            "Epoch 1819/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0757\n",
            "Epoch 1820/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0757\n",
            "Epoch 1821/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1822/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1823/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0758\n",
            "Epoch 1824/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1825/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0758\n",
            "Epoch 1826/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0758\n",
            "Epoch 1827/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0758\n",
            "Epoch 1828/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0758\n",
            "Epoch 1829/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0757\n",
            "Epoch 1830/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0757\n",
            "Epoch 1831/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0757\n",
            "Epoch 1832/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0756\n",
            "Epoch 1833/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0756\n",
            "Epoch 1834/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0756\n",
            "Epoch 1835/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0756\n",
            "Epoch 1836/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0756\n",
            "Epoch 1837/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0756\n",
            "Epoch 1838/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0756\n",
            "Epoch 1839/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0756\n",
            "Epoch 1840/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1841/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1842/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0755\n",
            "Epoch 1843/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1844/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0755\n",
            "Epoch 1845/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1846/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0755\n",
            "Epoch 1847/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1848/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0755\n",
            "Epoch 1849/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0755\n",
            "Epoch 1850/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0755\n",
            "Epoch 1851/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0755\n",
            "Epoch 1852/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0755\n",
            "Epoch 1853/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0755\n",
            "Epoch 1854/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1855/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1856/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0755\n",
            "Epoch 1857/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0754\n",
            "Epoch 1858/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0754\n",
            "Epoch 1859/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0754\n",
            "Epoch 1860/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0754\n",
            "Epoch 1861/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0754\n",
            "Epoch 1862/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0754\n",
            "Epoch 1863/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0754\n",
            "Epoch 1864/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0754\n",
            "Epoch 1865/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1866/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1867/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1868/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1869/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0753\n",
            "Epoch 1870/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1871/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 1872/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0753\n",
            "Epoch 1873/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1874/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1875/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1876/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1877/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1878/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1879/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1880/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1881/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1882/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0752\n",
            "Epoch 1883/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0752\n",
            "Epoch 1884/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0752\n",
            "Epoch 1885/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0752\n",
            "Epoch 1886/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0752\n",
            "Epoch 1887/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0752\n",
            "Epoch 1888/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0752\n",
            "Epoch 1889/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0751\n",
            "Epoch 1890/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0751\n",
            "Epoch 1891/2000\n",
            "15746/15746 [==============================] - 0s 31us/step - loss: 0.0751\n",
            "Epoch 1892/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0751\n",
            "Epoch 1893/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0751\n",
            "Epoch 1894/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0751\n",
            "Epoch 1895/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0751\n",
            "Epoch 1896/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0751\n",
            "Epoch 1897/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0751\n",
            "Epoch 1898/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0751\n",
            "Epoch 1899/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0751\n",
            "Epoch 1900/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0751\n",
            "Epoch 1901/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0751\n",
            "Epoch 1902/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0751\n",
            "Epoch 1903/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0751\n",
            "Epoch 1904/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0751\n",
            "Epoch 1905/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0751\n",
            "Epoch 1906/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0750\n",
            "Epoch 1907/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0750\n",
            "Epoch 1908/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0750\n",
            "Epoch 1909/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0750\n",
            "Epoch 1910/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0750\n",
            "Epoch 1911/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0750\n",
            "Epoch 1912/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0749\n",
            "Epoch 1913/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0749\n",
            "Epoch 1914/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0749\n",
            "Epoch 1915/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0749\n",
            "Epoch 1916/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0749\n",
            "Epoch 1917/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0749\n",
            "Epoch 1918/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0749\n",
            "Epoch 1919/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0748\n",
            "Epoch 1920/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0748\n",
            "Epoch 1921/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.0748\n",
            "Epoch 1922/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0748\n",
            "Epoch 1923/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0748\n",
            "Epoch 1924/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0748\n",
            "Epoch 1925/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0748\n",
            "Epoch 1926/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0748\n",
            "Epoch 1927/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0747\n",
            "Epoch 1928/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0747\n",
            "Epoch 1929/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1930/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1931/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1932/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1933/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1934/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1935/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1936/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0747\n",
            "Epoch 1937/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1938/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1939/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1940/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0747\n",
            "Epoch 1941/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1942/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0747\n",
            "Epoch 1943/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1944/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1945/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1946/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1947/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1948/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0747\n",
            "Epoch 1949/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1950/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 1951/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1952/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0747\n",
            "Epoch 1953/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0746\n",
            "Epoch 1954/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0746\n",
            "Epoch 1955/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0746\n",
            "Epoch 1956/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0746\n",
            "Epoch 1957/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0746\n",
            "Epoch 1958/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0746\n",
            "Epoch 1959/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0746\n",
            "Epoch 1960/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1961/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0745\n",
            "Epoch 1962/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1963/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1964/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1965/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1966/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1967/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1968/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0745\n",
            "Epoch 1969/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1970/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0745\n",
            "Epoch 1971/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1972/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1973/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1974/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1975/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1976/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1977/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1978/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0744\n",
            "Epoch 1979/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0744\n",
            "Epoch 1980/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0743\n",
            "Epoch 1981/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1982/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1983/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1984/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1985/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1986/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1987/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1988/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1989/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1990/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0743\n",
            "Epoch 1991/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1992/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1993/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0743\n",
            "Epoch 1994/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1995/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 1996/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1997/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1998/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 1999/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 2000/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x14a652dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oecaEqGFriLZ",
        "colab": {}
      },
      "source": [
        "lstm = LSTM_(nodes_alpharnn,0)\n",
        "lstm.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8TGqr8m4QN2",
        "outputId": "105e4a64-3e35-4edc-8cdd-fe6e86a99994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rs = RS_(20,0,0)\n",
        "rs.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "3192/3192 [==============================] - 5s 2ms/step - loss: 1.1736\n",
            "Epoch 2/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 1.0945\n",
            "Epoch 3/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.9603\n",
            "Epoch 4/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.7080\n",
            "Epoch 5/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.5818\n",
            "Epoch 6/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.5146\n",
            "Epoch 7/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.4606\n",
            "Epoch 8/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.4343\n",
            "Epoch 9/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.4103\n",
            "Epoch 10/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.4014\n",
            "Epoch 11/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.3765\n",
            "Epoch 12/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.3533\n",
            "Epoch 13/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.3379\n",
            "Epoch 14/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.3286\n",
            "Epoch 15/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.3049\n",
            "Epoch 16/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.3124\n",
            "Epoch 17/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2978\n",
            "Epoch 18/2000\n",
            "3192/3192 [==============================] - 0s 92us/step - loss: 0.3770\n",
            "Epoch 19/2000\n",
            "3192/3192 [==============================] - 0s 107us/step - loss: 0.3680\n",
            "Epoch 20/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.3120\n",
            "Epoch 21/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2802\n",
            "Epoch 22/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2676\n",
            "Epoch 23/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2623\n",
            "Epoch 24/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2576\n",
            "Epoch 25/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2540\n",
            "Epoch 26/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2487\n",
            "Epoch 27/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2460\n",
            "Epoch 28/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2422\n",
            "Epoch 29/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2442\n",
            "Epoch 30/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2415\n",
            "Epoch 31/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2452\n",
            "Epoch 32/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2496\n",
            "Epoch 33/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2457\n",
            "Epoch 34/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2421\n",
            "Epoch 35/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2393\n",
            "Epoch 36/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2359\n",
            "Epoch 37/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2339\n",
            "Epoch 38/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2320\n",
            "Epoch 39/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2329\n",
            "Epoch 40/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2303\n",
            "Epoch 41/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2295\n",
            "Epoch 42/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2236\n",
            "Epoch 43/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2253\n",
            "Epoch 44/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2203\n",
            "Epoch 45/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2324\n",
            "Epoch 46/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2269\n",
            "Epoch 47/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2256\n",
            "Epoch 48/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2229\n",
            "Epoch 49/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2278\n",
            "Epoch 50/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2238\n",
            "Epoch 51/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2231\n",
            "Epoch 52/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2217\n",
            "Epoch 53/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2212\n",
            "Epoch 54/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2194\n",
            "Epoch 55/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2190\n",
            "Epoch 56/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2190\n",
            "Epoch 57/2000\n",
            "3192/3192 [==============================] - 0s 93us/step - loss: 0.2176\n",
            "Epoch 58/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2174\n",
            "Epoch 59/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2171\n",
            "Epoch 60/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2166\n",
            "Epoch 61/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2162\n",
            "Epoch 62/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2160\n",
            "Epoch 63/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2156\n",
            "Epoch 64/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2148\n",
            "Epoch 65/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2155\n",
            "Epoch 66/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2143\n",
            "Epoch 67/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2137\n",
            "Epoch 68/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2146\n",
            "Epoch 69/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2134\n",
            "Epoch 70/2000\n",
            "3192/3192 [==============================] - 0s 104us/step - loss: 0.2130\n",
            "Epoch 71/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2141\n",
            "Epoch 72/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2121\n",
            "Epoch 73/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2127\n",
            "Epoch 74/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2127\n",
            "Epoch 75/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2138\n",
            "Epoch 76/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2112\n",
            "Epoch 77/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2133\n",
            "Epoch 78/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2115\n",
            "Epoch 79/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2130\n",
            "Epoch 80/2000\n",
            "3192/3192 [==============================] - 0s 106us/step - loss: 0.2120\n",
            "Epoch 81/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2115\n",
            "Epoch 82/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2110\n",
            "Epoch 83/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2123\n",
            "Epoch 84/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2116\n",
            "Epoch 85/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2119\n",
            "Epoch 86/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2101\n",
            "Epoch 87/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2134\n",
            "Epoch 88/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2106\n",
            "Epoch 89/2000\n",
            "3192/3192 [==============================] - 0s 104us/step - loss: 0.2108\n",
            "Epoch 90/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2138\n",
            "Epoch 91/2000\n",
            "3192/3192 [==============================] - 0s 108us/step - loss: 0.2137\n",
            "Epoch 92/2000\n",
            "3192/3192 [==============================] - 0s 106us/step - loss: 0.2113\n",
            "Epoch 93/2000\n",
            "3192/3192 [==============================] - 0s 104us/step - loss: 0.2120\n",
            "Epoch 94/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2145\n",
            "Epoch 95/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2114\n",
            "Epoch 96/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2103\n",
            "Epoch 97/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2130\n",
            "Epoch 98/2000\n",
            "3192/3192 [==============================] - 0s 106us/step - loss: 0.2109\n",
            "Epoch 99/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2105\n",
            "Epoch 100/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2096\n",
            "Epoch 101/2000\n",
            "3192/3192 [==============================] - 0s 111us/step - loss: 0.2133\n",
            "Epoch 102/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2091\n",
            "Epoch 103/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2110\n",
            "Epoch 104/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2094\n",
            "Epoch 105/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2100\n",
            "Epoch 106/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2098\n",
            "Epoch 107/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2105\n",
            "Epoch 108/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2100\n",
            "Epoch 109/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2089\n",
            "Epoch 110/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2103\n",
            "Epoch 111/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2096\n",
            "Epoch 112/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2078\n",
            "Epoch 113/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2134\n",
            "Epoch 114/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2091\n",
            "Epoch 115/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2092\n",
            "Epoch 116/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2093\n",
            "Epoch 117/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2092\n",
            "Epoch 118/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2092\n",
            "Epoch 119/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2092\n",
            "Epoch 120/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2091\n",
            "Epoch 121/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2091\n",
            "Epoch 122/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2091\n",
            "Epoch 123/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2091\n",
            "Epoch 124/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2088\n",
            "Epoch 125/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2096\n",
            "Epoch 126/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2090\n",
            "Epoch 127/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2090\n",
            "Epoch 128/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2090\n",
            "Epoch 129/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2089\n",
            "Epoch 130/2000\n",
            "3192/3192 [==============================] - 0s 103us/step - loss: 0.2089\n",
            "Epoch 131/2000\n",
            "3192/3192 [==============================] - 0s 115us/step - loss: 0.2088\n",
            "Epoch 132/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2088\n",
            "Epoch 133/2000\n",
            "3192/3192 [==============================] - 0s 108us/step - loss: 0.2087\n",
            "Epoch 134/2000\n",
            "3192/3192 [==============================] - 0s 106us/step - loss: 0.2088\n",
            "Epoch 135/2000\n",
            "3192/3192 [==============================] - 0s 115us/step - loss: 0.2084\n",
            "Epoch 136/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2105\n",
            "Epoch 137/2000\n",
            "3192/3192 [==============================] - 0s 104us/step - loss: 0.2058\n",
            "Epoch 138/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2110\n",
            "Epoch 139/2000\n",
            "3192/3192 [==============================] - 0s 126us/step - loss: 0.2084\n",
            "Epoch 140/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2084\n",
            "Epoch 141/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2080\n",
            "Epoch 142/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2077\n",
            "Epoch 143/2000\n",
            "3192/3192 [==============================] - 0s 122us/step - loss: 0.2087\n",
            "Epoch 144/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2082\n",
            "Epoch 145/2000\n",
            "3192/3192 [==============================] - 0s 113us/step - loss: 0.2074\n",
            "Epoch 146/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2075\n",
            "Epoch 147/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2086\n",
            "Epoch 148/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2082\n",
            "Epoch 149/2000\n",
            "3192/3192 [==============================] - 0s 111us/step - loss: 0.2068\n",
            "Epoch 150/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2088\n",
            "Epoch 151/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2081\n",
            "Epoch 152/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2065\n",
            "Epoch 153/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2092\n",
            "Epoch 154/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2077\n",
            "Epoch 155/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2117\n",
            "Epoch 156/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2106\n",
            "Epoch 157/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2074\n",
            "Epoch 158/2000\n",
            "3192/3192 [==============================] - 0s 108us/step - loss: 0.2063\n",
            "Epoch 159/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2088\n",
            "Epoch 160/2000\n",
            "3192/3192 [==============================] - 0s 104us/step - loss: 0.2092\n",
            "Epoch 161/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2066\n",
            "Epoch 162/2000\n",
            "3192/3192 [==============================] - 0s 108us/step - loss: 0.2077\n",
            "Epoch 163/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2078\n",
            "Epoch 164/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2067\n",
            "Epoch 165/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2080\n",
            "Epoch 166/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2061\n",
            "Epoch 167/2000\n",
            "3192/3192 [==============================] - 0s 103us/step - loss: 0.2094\n",
            "Epoch 168/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2090\n",
            "Epoch 169/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2086\n",
            "Epoch 170/2000\n",
            "3192/3192 [==============================] - 0s 98us/step - loss: 0.2071\n",
            "Epoch 171/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2062\n",
            "Epoch 172/2000\n",
            "3192/3192 [==============================] - 0s 111us/step - loss: 0.2064\n",
            "Epoch 173/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2062\n",
            "Epoch 174/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2080\n",
            "Epoch 175/2000\n",
            "3192/3192 [==============================] - 0s 124us/step - loss: 0.2081\n",
            "Epoch 176/2000\n",
            "3192/3192 [==============================] - 0s 133us/step - loss: 0.2081\n",
            "Epoch 177/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2076\n",
            "Epoch 178/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2064\n",
            "Epoch 179/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2066\n",
            "Epoch 180/2000\n",
            "3192/3192 [==============================] - 0s 107us/step - loss: 0.2057\n",
            "Epoch 181/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2058\n",
            "Epoch 182/2000\n",
            "3192/3192 [==============================] - 0s 133us/step - loss: 0.2055\n",
            "Epoch 183/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2067\n",
            "Epoch 184/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2057\n",
            "Epoch 185/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2059\n",
            "Epoch 186/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2058\n",
            "Epoch 187/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2058\n",
            "Epoch 188/2000\n",
            "3192/3192 [==============================] - 0s 117us/step - loss: 0.2058\n",
            "Epoch 189/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2057\n",
            "Epoch 190/2000\n",
            "3192/3192 [==============================] - 0s 112us/step - loss: 0.2057\n",
            "Epoch 191/2000\n",
            "3192/3192 [==============================] - 0s 105us/step - loss: 0.2056\n",
            "Epoch 192/2000\n",
            "3192/3192 [==============================] - 0s 121us/step - loss: 0.2055\n",
            "Epoch 193/2000\n",
            "3192/3192 [==============================] - 0s 148us/step - loss: 0.2060\n",
            "Epoch 194/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2056\n",
            "Epoch 195/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2072\n",
            "Epoch 196/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2061\n",
            "Epoch 197/2000\n",
            "3192/3192 [==============================] - 0s 119us/step - loss: 0.2057\n",
            "Epoch 198/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2073\n",
            "Epoch 199/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2057\n",
            "Epoch 200/2000\n",
            "3192/3192 [==============================] - 0s 107us/step - loss: 0.2061\n",
            "Epoch 201/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2057\n",
            "Epoch 202/2000\n",
            "3192/3192 [==============================] - 0s 122us/step - loss: 0.2087\n",
            "Epoch 203/2000\n",
            "3192/3192 [==============================] - 0s 124us/step - loss: 0.2057\n",
            "Epoch 204/2000\n",
            "3192/3192 [==============================] - 0s 103us/step - loss: 0.2069\n",
            "Epoch 205/2000\n",
            "3192/3192 [==============================] - 0s 119us/step - loss: 0.2064\n",
            "Epoch 206/2000\n",
            "3192/3192 [==============================] - 0s 93us/step - loss: 0.2064\n",
            "Epoch 207/2000\n",
            "3192/3192 [==============================] - 0s 121us/step - loss: 0.2064\n",
            "Epoch 208/2000\n",
            "3192/3192 [==============================] - 0s 107us/step - loss: 0.2063\n",
            "Epoch 209/2000\n",
            "3192/3192 [==============================] - 0s 124us/step - loss: 0.2063\n",
            "Epoch 210/2000\n",
            "3192/3192 [==============================] - 0s 128us/step - loss: 0.2063\n",
            "Epoch 211/2000\n",
            "3192/3192 [==============================] - 0s 102us/step - loss: 0.2063\n",
            "Epoch 212/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2065\n",
            "Epoch 213/2000\n",
            "3192/3192 [==============================] - 0s 110us/step - loss: 0.2069\n",
            "Epoch 214/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2065\n",
            "Epoch 215/2000\n",
            "3192/3192 [==============================] - 0s 112us/step - loss: 0.2066\n",
            "Epoch 216/2000\n",
            "3192/3192 [==============================] - 0s 124us/step - loss: 0.2058\n",
            "Epoch 217/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2105\n",
            "Epoch 218/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2076\n",
            "Epoch 219/2000\n",
            "3192/3192 [==============================] - 0s 96us/step - loss: 0.2083\n",
            "Epoch 220/2000\n",
            "3192/3192 [==============================] - 0s 123us/step - loss: 0.2079\n",
            "Epoch 221/2000\n",
            "3192/3192 [==============================] - 0s 94us/step - loss: 0.2078\n",
            "Epoch 222/2000\n",
            "3192/3192 [==============================] - 0s 91us/step - loss: 0.2077\n",
            "Epoch 223/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2076\n",
            "Epoch 224/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2076\n",
            "Epoch 225/2000\n",
            "3192/3192 [==============================] - 0s 99us/step - loss: 0.2075\n",
            "Epoch 226/2000\n",
            "3192/3192 [==============================] - 0s 100us/step - loss: 0.2075\n",
            "Epoch 227/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2073\n",
            "Epoch 228/2000\n",
            "3192/3192 [==============================] - 0s 128us/step - loss: 0.2073\n",
            "Epoch 229/2000\n",
            "3192/3192 [==============================] - 0s 97us/step - loss: 0.2072\n",
            "Epoch 230/2000\n",
            "3192/3192 [==============================] - 0s 101us/step - loss: 0.2072\n",
            "Epoch 231/2000\n",
            "3192/3192 [==============================] - 0s 95us/step - loss: 0.2071\n",
            "Epoch 232/2000\n",
            "3192/3192 [==============================] - 0s 103us/step - loss: 0.2071\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00232: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15294ac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OEqibAqj5tUQ",
        "colab": {}
      },
      "source": [
        "nodes_alpharnnt=1 # 4.8867e-04"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2u2b2qShzD_",
        "outputId": "171272f7-a58b-481d-9537-1c82fea9f66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "alpharnnt = AlphaRNNt(10,0)\n",
        "alpharnnt.fit(x_train_reg,y_train_reg,epochs=2000,batch_size=1500,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "15746/15746 [==============================] - 3s 206us/step - loss: 1.1339\n",
            "Epoch 2/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 1.1256\n",
            "Epoch 3/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.1181\n",
            "Epoch 4/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.1109\n",
            "Epoch 5/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 1.1052\n",
            "Epoch 6/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.1023\n",
            "Epoch 7/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.0992\n",
            "Epoch 8/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 1.0954\n",
            "Epoch 9/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.0896\n",
            "Epoch 10/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 1.0771\n",
            "Epoch 11/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 1.0452\n",
            "Epoch 12/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.9582\n",
            "Epoch 13/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.7690\n",
            "Epoch 14/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.5562\n",
            "Epoch 15/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.4630\n",
            "Epoch 16/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.4243\n",
            "Epoch 17/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.4025\n",
            "Epoch 18/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.3887\n",
            "Epoch 19/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.3758\n",
            "Epoch 20/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.3640\n",
            "Epoch 21/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.3526\n",
            "Epoch 22/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.3414\n",
            "Epoch 23/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.3305\n",
            "Epoch 24/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.3194\n",
            "Epoch 25/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.3088\n",
            "Epoch 26/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2981\n",
            "Epoch 27/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2880\n",
            "Epoch 28/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2775\n",
            "Epoch 29/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2679\n",
            "Epoch 30/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.2585\n",
            "Epoch 31/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2493\n",
            "Epoch 32/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2410\n",
            "Epoch 33/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2335\n",
            "Epoch 34/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2264\n",
            "Epoch 35/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2202\n",
            "Epoch 36/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2149\n",
            "Epoch 37/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2099\n",
            "Epoch 38/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.2058\n",
            "Epoch 39/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.2015\n",
            "Epoch 40/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1982\n",
            "Epoch 41/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1953\n",
            "Epoch 42/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1921\n",
            "Epoch 43/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1894\n",
            "Epoch 44/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1870\n",
            "Epoch 45/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1851\n",
            "Epoch 46/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1828\n",
            "Epoch 47/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1805\n",
            "Epoch 48/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1784\n",
            "Epoch 49/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1772\n",
            "Epoch 50/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1749\n",
            "Epoch 51/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1730\n",
            "Epoch 52/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1712\n",
            "Epoch 53/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1697\n",
            "Epoch 54/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1680\n",
            "Epoch 55/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1660\n",
            "Epoch 56/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1643\n",
            "Epoch 57/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1626\n",
            "Epoch 58/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1611\n",
            "Epoch 59/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1595\n",
            "Epoch 60/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1576\n",
            "Epoch 61/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1559\n",
            "Epoch 62/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1539\n",
            "Epoch 63/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1527\n",
            "Epoch 64/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1515\n",
            "Epoch 65/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1500\n",
            "Epoch 66/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1493\n",
            "Epoch 67/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1479\n",
            "Epoch 68/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1464\n",
            "Epoch 69/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1449\n",
            "Epoch 70/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1442\n",
            "Epoch 71/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1431\n",
            "Epoch 72/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1430\n",
            "Epoch 73/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1417\n",
            "Epoch 74/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1408\n",
            "Epoch 75/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1403\n",
            "Epoch 76/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1398\n",
            "Epoch 77/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1401\n",
            "Epoch 78/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1381\n",
            "Epoch 79/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1376\n",
            "Epoch 80/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1369\n",
            "Epoch 81/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1359\n",
            "Epoch 82/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1355\n",
            "Epoch 83/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1347\n",
            "Epoch 84/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1339\n",
            "Epoch 85/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1336\n",
            "Epoch 86/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1328\n",
            "Epoch 87/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1327\n",
            "Epoch 88/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1320\n",
            "Epoch 89/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1313\n",
            "Epoch 90/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1302\n",
            "Epoch 91/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1299\n",
            "Epoch 92/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1292\n",
            "Epoch 93/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1286\n",
            "Epoch 94/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1282\n",
            "Epoch 95/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1276\n",
            "Epoch 96/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1270\n",
            "Epoch 97/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1272\n",
            "Epoch 98/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1262\n",
            "Epoch 99/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1255\n",
            "Epoch 100/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1251\n",
            "Epoch 101/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1246\n",
            "Epoch 102/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1243\n",
            "Epoch 103/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1235\n",
            "Epoch 104/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1232\n",
            "Epoch 105/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1228\n",
            "Epoch 106/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1221\n",
            "Epoch 107/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1215\n",
            "Epoch 108/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1205\n",
            "Epoch 109/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1201\n",
            "Epoch 110/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1199\n",
            "Epoch 111/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1191\n",
            "Epoch 112/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1184\n",
            "Epoch 113/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1181\n",
            "Epoch 114/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1175\n",
            "Epoch 115/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1168\n",
            "Epoch 116/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1166\n",
            "Epoch 117/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1153\n",
            "Epoch 118/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1149\n",
            "Epoch 119/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1144\n",
            "Epoch 120/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1140\n",
            "Epoch 121/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1133\n",
            "Epoch 122/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1126\n",
            "Epoch 123/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1123\n",
            "Epoch 124/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1120\n",
            "Epoch 125/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1113\n",
            "Epoch 126/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1114\n",
            "Epoch 127/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1113\n",
            "Epoch 128/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1104\n",
            "Epoch 129/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1101\n",
            "Epoch 130/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1098\n",
            "Epoch 131/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1094\n",
            "Epoch 132/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1115\n",
            "Epoch 133/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1097\n",
            "Epoch 134/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1086\n",
            "Epoch 135/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1091\n",
            "Epoch 136/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1093\n",
            "Epoch 137/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1079\n",
            "Epoch 138/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1071\n",
            "Epoch 139/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1071\n",
            "Epoch 140/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1070\n",
            "Epoch 141/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1065\n",
            "Epoch 142/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1061\n",
            "Epoch 143/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1057\n",
            "Epoch 144/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1057\n",
            "Epoch 145/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1055\n",
            "Epoch 146/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1055\n",
            "Epoch 147/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1050\n",
            "Epoch 148/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1045\n",
            "Epoch 149/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.1044\n",
            "Epoch 150/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1046\n",
            "Epoch 151/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1044\n",
            "Epoch 152/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1034\n",
            "Epoch 153/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1044\n",
            "Epoch 154/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1036\n",
            "Epoch 155/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1033\n",
            "Epoch 156/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1035\n",
            "Epoch 157/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1029\n",
            "Epoch 158/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1027\n",
            "Epoch 159/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1028\n",
            "Epoch 160/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1023\n",
            "Epoch 161/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.1026\n",
            "Epoch 162/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1024\n",
            "Epoch 163/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1020\n",
            "Epoch 164/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1015\n",
            "Epoch 165/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.1017\n",
            "Epoch 166/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1013\n",
            "Epoch 167/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1017\n",
            "Epoch 168/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1015\n",
            "Epoch 169/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1010\n",
            "Epoch 170/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1011\n",
            "Epoch 171/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1013\n",
            "Epoch 172/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1009\n",
            "Epoch 173/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.1011\n",
            "Epoch 174/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1010\n",
            "Epoch 175/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.1009\n",
            "Epoch 176/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.1005\n",
            "Epoch 177/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0998\n",
            "Epoch 178/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0997\n",
            "Epoch 179/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.1009\n",
            "Epoch 180/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.1004\n",
            "Epoch 181/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0998\n",
            "Epoch 182/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0993\n",
            "Epoch 183/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0998\n",
            "Epoch 184/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0994\n",
            "Epoch 185/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0997\n",
            "Epoch 186/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0992\n",
            "Epoch 187/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0987\n",
            "Epoch 188/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0985\n",
            "Epoch 189/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0997\n",
            "Epoch 190/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0994\n",
            "Epoch 191/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0988\n",
            "Epoch 192/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0986\n",
            "Epoch 193/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0983\n",
            "Epoch 194/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0981\n",
            "Epoch 195/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0985\n",
            "Epoch 196/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0978\n",
            "Epoch 197/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0979\n",
            "Epoch 198/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0993\n",
            "Epoch 199/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.1000\n",
            "Epoch 200/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0992\n",
            "Epoch 201/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0977\n",
            "Epoch 202/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0975\n",
            "Epoch 203/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0973\n",
            "Epoch 204/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0969\n",
            "Epoch 205/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0969\n",
            "Epoch 206/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0969\n",
            "Epoch 207/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0968\n",
            "Epoch 208/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0979\n",
            "Epoch 209/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0981\n",
            "Epoch 210/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0975\n",
            "Epoch 211/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0964\n",
            "Epoch 212/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0961\n",
            "Epoch 213/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0959\n",
            "Epoch 214/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0964\n",
            "Epoch 215/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0964\n",
            "Epoch 216/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0962\n",
            "Epoch 217/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0956\n",
            "Epoch 218/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0956\n",
            "Epoch 219/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0954\n",
            "Epoch 220/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0960\n",
            "Epoch 221/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0958\n",
            "Epoch 222/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0953\n",
            "Epoch 223/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0951\n",
            "Epoch 224/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0955\n",
            "Epoch 225/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0952\n",
            "Epoch 226/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0963\n",
            "Epoch 227/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0956\n",
            "Epoch 228/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0950\n",
            "Epoch 229/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0947\n",
            "Epoch 230/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0944\n",
            "Epoch 231/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0948\n",
            "Epoch 232/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0945\n",
            "Epoch 233/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0944\n",
            "Epoch 234/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0950\n",
            "Epoch 235/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0941\n",
            "Epoch 236/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0941\n",
            "Epoch 237/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0944\n",
            "Epoch 238/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0942\n",
            "Epoch 239/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0941\n",
            "Epoch 240/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0937\n",
            "Epoch 241/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0937\n",
            "Epoch 242/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0937\n",
            "Epoch 243/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0939\n",
            "Epoch 244/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0937\n",
            "Epoch 245/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0936\n",
            "Epoch 246/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0946\n",
            "Epoch 247/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0939\n",
            "Epoch 248/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0938\n",
            "Epoch 249/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0932\n",
            "Epoch 250/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0930\n",
            "Epoch 251/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0938\n",
            "Epoch 252/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0935\n",
            "Epoch 253/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0932\n",
            "Epoch 254/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0927\n",
            "Epoch 255/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0927\n",
            "Epoch 256/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0923\n",
            "Epoch 257/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0927\n",
            "Epoch 258/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0929\n",
            "Epoch 259/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0929\n",
            "Epoch 260/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0932\n",
            "Epoch 261/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0930\n",
            "Epoch 262/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0930\n",
            "Epoch 263/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0921\n",
            "Epoch 264/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0921\n",
            "Epoch 265/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0922\n",
            "Epoch 266/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0920\n",
            "Epoch 267/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0926\n",
            "Epoch 268/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0924\n",
            "Epoch 269/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0930\n",
            "Epoch 270/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0927\n",
            "Epoch 271/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0925\n",
            "Epoch 272/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0919\n",
            "Epoch 273/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0932\n",
            "Epoch 274/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0915\n",
            "Epoch 275/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0925\n",
            "Epoch 276/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0919\n",
            "Epoch 277/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0915\n",
            "Epoch 278/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0912\n",
            "Epoch 279/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0911\n",
            "Epoch 280/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0909\n",
            "Epoch 281/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0916\n",
            "Epoch 282/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0909\n",
            "Epoch 283/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0911\n",
            "Epoch 284/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0909\n",
            "Epoch 285/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0914\n",
            "Epoch 286/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0908\n",
            "Epoch 287/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0908\n",
            "Epoch 288/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0906\n",
            "Epoch 289/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0907\n",
            "Epoch 290/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0903\n",
            "Epoch 291/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0905\n",
            "Epoch 292/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0903\n",
            "Epoch 293/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0901\n",
            "Epoch 294/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.0903\n",
            "Epoch 295/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0899\n",
            "Epoch 296/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0904\n",
            "Epoch 297/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0911\n",
            "Epoch 298/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0903\n",
            "Epoch 299/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0904\n",
            "Epoch 300/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0899\n",
            "Epoch 301/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0899\n",
            "Epoch 302/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0896\n",
            "Epoch 303/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0895\n",
            "Epoch 304/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0900\n",
            "Epoch 305/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0906\n",
            "Epoch 306/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0897\n",
            "Epoch 307/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0905\n",
            "Epoch 308/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0896\n",
            "Epoch 309/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0895\n",
            "Epoch 310/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0897\n",
            "Epoch 311/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0897\n",
            "Epoch 312/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0891\n",
            "Epoch 313/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0890\n",
            "Epoch 314/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0889\n",
            "Epoch 315/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0890\n",
            "Epoch 316/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0890\n",
            "Epoch 317/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0892\n",
            "Epoch 318/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0896\n",
            "Epoch 319/2000\n",
            "15746/15746 [==============================] - 1s 34us/step - loss: 0.0888\n",
            "Epoch 320/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0893\n",
            "Epoch 321/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0893\n",
            "Epoch 322/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0890\n",
            "Epoch 323/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0889\n",
            "Epoch 324/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0888\n",
            "Epoch 325/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0895\n",
            "Epoch 326/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0884\n",
            "Epoch 327/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0886\n",
            "Epoch 328/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0885\n",
            "Epoch 329/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0882\n",
            "Epoch 330/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0882\n",
            "Epoch 331/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0881\n",
            "Epoch 332/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0879\n",
            "Epoch 333/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0879\n",
            "Epoch 334/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0880\n",
            "Epoch 335/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0881\n",
            "Epoch 336/2000\n",
            "15746/15746 [==============================] - 1s 35us/step - loss: 0.0878\n",
            "Epoch 337/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0886\n",
            "Epoch 338/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0885\n",
            "Epoch 339/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0876\n",
            "Epoch 340/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0877\n",
            "Epoch 341/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0875\n",
            "Epoch 342/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0878\n",
            "Epoch 343/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0877\n",
            "Epoch 344/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0877\n",
            "Epoch 345/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0876\n",
            "Epoch 346/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0882\n",
            "Epoch 347/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0882\n",
            "Epoch 348/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0879\n",
            "Epoch 349/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0874\n",
            "Epoch 350/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0871\n",
            "Epoch 351/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0871\n",
            "Epoch 352/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0872\n",
            "Epoch 353/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0869\n",
            "Epoch 354/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.0869\n",
            "Epoch 355/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0871\n",
            "Epoch 356/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0877\n",
            "Epoch 357/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0873\n",
            "Epoch 358/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0877\n",
            "Epoch 359/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0873\n",
            "Epoch 360/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0872\n",
            "Epoch 361/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0872\n",
            "Epoch 362/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0868\n",
            "Epoch 363/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0867\n",
            "Epoch 364/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0871\n",
            "Epoch 365/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0866\n",
            "Epoch 366/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0869\n",
            "Epoch 367/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0873\n",
            "Epoch 368/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0878\n",
            "Epoch 369/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0862\n",
            "Epoch 370/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0866\n",
            "Epoch 371/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0865\n",
            "Epoch 372/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0860\n",
            "Epoch 373/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0865\n",
            "Epoch 374/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0869\n",
            "Epoch 375/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0869\n",
            "Epoch 376/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0861\n",
            "Epoch 377/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0862\n",
            "Epoch 378/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0865\n",
            "Epoch 379/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0861\n",
            "Epoch 380/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0860\n",
            "Epoch 381/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0858\n",
            "Epoch 382/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0860\n",
            "Epoch 383/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0856\n",
            "Epoch 384/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0857\n",
            "Epoch 385/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0857\n",
            "Epoch 386/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0855\n",
            "Epoch 387/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0854\n",
            "Epoch 388/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0857\n",
            "Epoch 389/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0856\n",
            "Epoch 390/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0854\n",
            "Epoch 391/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0859\n",
            "Epoch 392/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0860\n",
            "Epoch 393/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0854\n",
            "Epoch 394/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0857\n",
            "Epoch 395/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0853\n",
            "Epoch 396/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0851\n",
            "Epoch 397/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0851\n",
            "Epoch 398/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0853\n",
            "Epoch 399/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0852\n",
            "Epoch 400/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0857\n",
            "Epoch 401/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0850\n",
            "Epoch 402/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0853\n",
            "Epoch 403/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0853\n",
            "Epoch 404/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0856\n",
            "Epoch 405/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0847\n",
            "Epoch 406/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0849\n",
            "Epoch 407/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0854\n",
            "Epoch 408/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0847\n",
            "Epoch 409/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0848\n",
            "Epoch 410/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0845\n",
            "Epoch 411/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0849\n",
            "Epoch 412/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0855\n",
            "Epoch 413/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0850\n",
            "Epoch 414/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0848\n",
            "Epoch 415/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0844\n",
            "Epoch 416/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0844\n",
            "Epoch 417/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0843\n",
            "Epoch 418/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0844\n",
            "Epoch 419/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0851\n",
            "Epoch 420/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0845\n",
            "Epoch 421/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0847\n",
            "Epoch 422/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0844\n",
            "Epoch 423/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0844\n",
            "Epoch 424/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0849\n",
            "Epoch 425/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0843\n",
            "Epoch 426/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0843\n",
            "Epoch 427/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0841\n",
            "Epoch 428/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0842\n",
            "Epoch 429/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0847\n",
            "Epoch 430/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0843\n",
            "Epoch 431/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0841\n",
            "Epoch 432/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0837\n",
            "Epoch 433/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0839\n",
            "Epoch 434/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0838\n",
            "Epoch 435/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0839\n",
            "Epoch 436/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0837\n",
            "Epoch 437/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0841\n",
            "Epoch 438/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0842\n",
            "Epoch 439/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0850\n",
            "Epoch 440/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0847\n",
            "Epoch 441/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0839\n",
            "Epoch 442/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0838\n",
            "Epoch 443/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0838\n",
            "Epoch 444/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0833\n",
            "Epoch 445/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0835\n",
            "Epoch 446/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0835\n",
            "Epoch 447/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0835\n",
            "Epoch 448/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0836\n",
            "Epoch 449/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0836\n",
            "Epoch 450/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0835\n",
            "Epoch 451/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0834\n",
            "Epoch 452/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0834\n",
            "Epoch 453/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0836\n",
            "Epoch 454/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0837\n",
            "Epoch 455/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0844\n",
            "Epoch 456/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0839\n",
            "Epoch 457/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0834\n",
            "Epoch 458/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0835\n",
            "Epoch 459/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0836\n",
            "Epoch 460/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0834\n",
            "Epoch 461/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0832\n",
            "Epoch 462/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0836\n",
            "Epoch 463/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0835\n",
            "Epoch 464/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0831\n",
            "Epoch 465/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0835\n",
            "Epoch 466/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0834\n",
            "Epoch 467/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0828\n",
            "Epoch 468/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0827\n",
            "Epoch 469/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0829\n",
            "Epoch 470/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0828\n",
            "Epoch 471/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0829\n",
            "Epoch 472/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0832\n",
            "Epoch 473/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0837\n",
            "Epoch 474/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0828\n",
            "Epoch 475/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0827\n",
            "Epoch 476/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0825\n",
            "Epoch 477/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0826\n",
            "Epoch 478/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0827\n",
            "Epoch 479/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0829\n",
            "Epoch 480/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0834\n",
            "Epoch 481/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0826\n",
            "Epoch 482/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0822\n",
            "Epoch 483/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0823\n",
            "Epoch 484/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0824\n",
            "Epoch 485/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0825\n",
            "Epoch 486/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0826\n",
            "Epoch 487/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0841\n",
            "Epoch 488/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0842\n",
            "Epoch 489/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0842\n",
            "Epoch 490/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0826\n",
            "Epoch 491/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0829\n",
            "Epoch 492/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0824\n",
            "Epoch 493/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0823\n",
            "Epoch 494/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0825\n",
            "Epoch 495/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0823\n",
            "Epoch 496/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0821\n",
            "Epoch 497/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0821\n",
            "Epoch 498/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0820\n",
            "Epoch 499/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0822\n",
            "Epoch 500/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0823\n",
            "Epoch 501/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0822\n",
            "Epoch 502/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0819\n",
            "Epoch 503/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0822\n",
            "Epoch 504/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0822\n",
            "Epoch 505/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0821\n",
            "Epoch 506/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0824\n",
            "Epoch 507/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0820\n",
            "Epoch 508/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0818\n",
            "Epoch 509/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0818\n",
            "Epoch 510/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0823\n",
            "Epoch 511/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0817\n",
            "Epoch 512/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0819\n",
            "Epoch 513/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0814\n",
            "Epoch 514/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 515/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0818\n",
            "Epoch 516/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0818\n",
            "Epoch 517/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0818\n",
            "Epoch 518/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0816\n",
            "Epoch 519/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0815\n",
            "Epoch 520/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0815\n",
            "Epoch 521/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0816\n",
            "Epoch 522/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0830\n",
            "Epoch 523/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0816\n",
            "Epoch 524/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0815\n",
            "Epoch 525/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0825\n",
            "Epoch 526/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0829\n",
            "Epoch 527/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0822\n",
            "Epoch 528/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0823\n",
            "Epoch 529/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0816\n",
            "Epoch 530/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0814\n",
            "Epoch 531/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0814\n",
            "Epoch 532/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 533/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0812\n",
            "Epoch 534/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0814\n",
            "Epoch 535/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0819\n",
            "Epoch 536/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0817\n",
            "Epoch 537/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0814\n",
            "Epoch 538/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0813\n",
            "Epoch 539/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0813\n",
            "Epoch 540/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0815\n",
            "Epoch 541/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0811\n",
            "Epoch 542/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 543/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0811\n",
            "Epoch 544/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0808\n",
            "Epoch 545/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0811\n",
            "Epoch 546/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0814\n",
            "Epoch 547/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0808\n",
            "Epoch 548/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0807\n",
            "Epoch 549/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0804\n",
            "Epoch 550/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0806\n",
            "Epoch 551/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0811\n",
            "Epoch 552/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0808\n",
            "Epoch 553/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0805\n",
            "Epoch 554/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0807\n",
            "Epoch 555/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0810\n",
            "Epoch 556/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0804\n",
            "Epoch 557/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0807\n",
            "Epoch 558/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0808\n",
            "Epoch 559/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0807\n",
            "Epoch 560/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0804\n",
            "Epoch 561/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0806\n",
            "Epoch 562/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0803\n",
            "Epoch 563/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0802\n",
            "Epoch 564/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0809\n",
            "Epoch 565/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0812\n",
            "Epoch 566/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0801\n",
            "Epoch 567/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0799\n",
            "Epoch 568/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0799\n",
            "Epoch 569/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0802\n",
            "Epoch 570/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0801\n",
            "Epoch 571/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0802\n",
            "Epoch 572/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0799\n",
            "Epoch 573/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0799\n",
            "Epoch 574/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0800\n",
            "Epoch 575/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0799\n",
            "Epoch 576/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0800\n",
            "Epoch 577/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0798\n",
            "Epoch 578/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0796\n",
            "Epoch 579/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0795\n",
            "Epoch 580/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0804\n",
            "Epoch 581/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0809\n",
            "Epoch 582/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0801\n",
            "Epoch 583/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0797\n",
            "Epoch 584/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0809\n",
            "Epoch 585/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0797\n",
            "Epoch 586/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0799\n",
            "Epoch 587/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0796\n",
            "Epoch 588/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0795\n",
            "Epoch 589/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0797\n",
            "Epoch 590/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0798\n",
            "Epoch 591/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0797\n",
            "Epoch 592/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0793\n",
            "Epoch 593/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0796\n",
            "Epoch 594/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0798\n",
            "Epoch 595/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0791\n",
            "Epoch 596/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0791\n",
            "Epoch 597/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0795\n",
            "Epoch 598/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0796\n",
            "Epoch 599/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0797\n",
            "Epoch 600/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0798\n",
            "Epoch 601/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0793\n",
            "Epoch 602/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0792\n",
            "Epoch 603/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0798\n",
            "Epoch 604/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0791\n",
            "Epoch 605/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0789\n",
            "Epoch 606/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0791\n",
            "Epoch 607/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0788\n",
            "Epoch 608/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0789\n",
            "Epoch 609/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0786\n",
            "Epoch 610/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0787\n",
            "Epoch 611/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0796\n",
            "Epoch 612/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0788\n",
            "Epoch 613/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0787\n",
            "Epoch 614/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0789\n",
            "Epoch 615/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0793\n",
            "Epoch 616/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0788\n",
            "Epoch 617/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0785\n",
            "Epoch 618/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0785\n",
            "Epoch 619/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0784\n",
            "Epoch 620/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0794\n",
            "Epoch 621/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0784\n",
            "Epoch 622/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0785\n",
            "Epoch 623/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0784\n",
            "Epoch 624/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0782\n",
            "Epoch 625/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0784\n",
            "Epoch 626/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0783\n",
            "Epoch 627/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0783\n",
            "Epoch 628/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0784\n",
            "Epoch 629/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0788\n",
            "Epoch 630/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0783\n",
            "Epoch 631/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0784\n",
            "Epoch 632/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0794\n",
            "Epoch 633/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0786\n",
            "Epoch 634/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0782\n",
            "Epoch 635/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0780\n",
            "Epoch 636/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0784\n",
            "Epoch 637/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0799\n",
            "Epoch 638/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0789\n",
            "Epoch 639/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0785\n",
            "Epoch 640/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0779\n",
            "Epoch 641/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0781\n",
            "Epoch 642/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0784\n",
            "Epoch 643/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0787\n",
            "Epoch 644/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0781\n",
            "Epoch 645/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0783\n",
            "Epoch 646/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0788\n",
            "Epoch 647/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0791\n",
            "Epoch 648/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0783\n",
            "Epoch 649/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0785\n",
            "Epoch 650/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0786\n",
            "Epoch 651/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0789\n",
            "Epoch 652/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0784\n",
            "Epoch 653/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0779\n",
            "Epoch 654/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0776\n",
            "Epoch 655/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0779\n",
            "Epoch 656/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0775\n",
            "Epoch 657/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0777\n",
            "Epoch 658/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0775\n",
            "Epoch 659/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0777\n",
            "Epoch 660/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0775\n",
            "Epoch 661/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0777\n",
            "Epoch 662/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0779\n",
            "Epoch 663/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0776\n",
            "Epoch 664/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0774\n",
            "Epoch 665/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0775\n",
            "Epoch 666/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0785\n",
            "Epoch 667/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0788\n",
            "Epoch 668/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.0775\n",
            "Epoch 669/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0771\n",
            "Epoch 670/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0770\n",
            "Epoch 671/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0773\n",
            "Epoch 672/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0778\n",
            "Epoch 673/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0779\n",
            "Epoch 674/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0776\n",
            "Epoch 675/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0773\n",
            "Epoch 676/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0772\n",
            "Epoch 677/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0769\n",
            "Epoch 678/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0770\n",
            "Epoch 679/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0768\n",
            "Epoch 680/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0768\n",
            "Epoch 681/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0770\n",
            "Epoch 682/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0777\n",
            "Epoch 683/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0772\n",
            "Epoch 684/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0770\n",
            "Epoch 685/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0767\n",
            "Epoch 686/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0769\n",
            "Epoch 687/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0766\n",
            "Epoch 688/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0765\n",
            "Epoch 689/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0769\n",
            "Epoch 690/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0767\n",
            "Epoch 691/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0774\n",
            "Epoch 692/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0764\n",
            "Epoch 693/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0769\n",
            "Epoch 694/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0770\n",
            "Epoch 695/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0764\n",
            "Epoch 696/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0766\n",
            "Epoch 697/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0764\n",
            "Epoch 698/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0764\n",
            "Epoch 699/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0763\n",
            "Epoch 700/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0769\n",
            "Epoch 701/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0767\n",
            "Epoch 702/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0770\n",
            "Epoch 703/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0771\n",
            "Epoch 704/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 705/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0763\n",
            "Epoch 706/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0764\n",
            "Epoch 707/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0763\n",
            "Epoch 708/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0767\n",
            "Epoch 709/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0768\n",
            "Epoch 710/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0760\n",
            "Epoch 711/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0760\n",
            "Epoch 712/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0765\n",
            "Epoch 713/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0763\n",
            "Epoch 714/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0765\n",
            "Epoch 715/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0765\n",
            "Epoch 716/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0761\n",
            "Epoch 717/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0760\n",
            "Epoch 718/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0763\n",
            "Epoch 719/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 720/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0761\n",
            "Epoch 721/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0761\n",
            "Epoch 722/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0760\n",
            "Epoch 723/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0766\n",
            "Epoch 724/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0770\n",
            "Epoch 725/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0779\n",
            "Epoch 726/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0759\n",
            "Epoch 727/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0757\n",
            "Epoch 728/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0765\n",
            "Epoch 729/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0764\n",
            "Epoch 730/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0762\n",
            "Epoch 731/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0760\n",
            "Epoch 732/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0753\n",
            "Epoch 733/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0754\n",
            "Epoch 734/2000\n",
            "15746/15746 [==============================] - 0s 14us/step - loss: 0.0752\n",
            "Epoch 735/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0759\n",
            "Epoch 736/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0771\n",
            "Epoch 737/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0768\n",
            "Epoch 738/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0763\n",
            "Epoch 739/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0751\n",
            "Epoch 740/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0756\n",
            "Epoch 741/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0758\n",
            "Epoch 742/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0755\n",
            "Epoch 743/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0753\n",
            "Epoch 744/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0756\n",
            "Epoch 745/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0756\n",
            "Epoch 746/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0750\n",
            "Epoch 747/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0768\n",
            "Epoch 748/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0752\n",
            "Epoch 749/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.0751\n",
            "Epoch 750/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0755\n",
            "Epoch 751/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0755\n",
            "Epoch 752/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0752\n",
            "Epoch 753/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0751\n",
            "Epoch 754/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0746\n",
            "Epoch 755/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0748\n",
            "Epoch 756/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0762\n",
            "Epoch 757/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0768\n",
            "Epoch 758/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0754\n",
            "Epoch 759/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0748\n",
            "Epoch 760/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0746\n",
            "Epoch 761/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0753\n",
            "Epoch 762/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0749\n",
            "Epoch 763/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0747\n",
            "Epoch 764/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0746\n",
            "Epoch 765/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0746\n",
            "Epoch 766/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0749\n",
            "Epoch 767/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0745\n",
            "Epoch 768/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0753\n",
            "Epoch 769/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0760\n",
            "Epoch 770/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0759\n",
            "Epoch 771/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0757\n",
            "Epoch 772/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0749\n",
            "Epoch 773/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0745\n",
            "Epoch 774/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0745\n",
            "Epoch 775/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 776/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 777/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0741\n",
            "Epoch 778/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0742\n",
            "Epoch 779/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0739\n",
            "Epoch 780/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0742\n",
            "Epoch 781/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0741\n",
            "Epoch 782/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0751\n",
            "Epoch 783/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0747\n",
            "Epoch 784/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0745\n",
            "Epoch 785/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0743\n",
            "Epoch 786/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0740\n",
            "Epoch 787/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0744\n",
            "Epoch 788/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0743\n",
            "Epoch 789/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0741\n",
            "Epoch 790/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0738\n",
            "Epoch 791/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 792/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0739\n",
            "Epoch 793/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0739\n",
            "Epoch 794/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0741\n",
            "Epoch 795/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0748\n",
            "Epoch 796/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0746\n",
            "Epoch 797/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0741\n",
            "Epoch 798/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0744\n",
            "Epoch 799/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0738\n",
            "Epoch 800/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0742\n",
            "Epoch 801/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0737\n",
            "Epoch 802/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0743\n",
            "Epoch 803/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0736\n",
            "Epoch 804/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0740\n",
            "Epoch 805/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0743\n",
            "Epoch 806/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0738\n",
            "Epoch 807/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0739\n",
            "Epoch 808/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0742\n",
            "Epoch 809/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0737\n",
            "Epoch 810/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0735\n",
            "Epoch 811/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0748\n",
            "Epoch 812/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0734\n",
            "Epoch 813/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0738\n",
            "Epoch 814/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0731\n",
            "Epoch 815/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0735\n",
            "Epoch 816/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0740\n",
            "Epoch 817/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0739\n",
            "Epoch 818/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 819/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0733\n",
            "Epoch 820/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 821/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0735\n",
            "Epoch 822/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0737\n",
            "Epoch 823/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0736\n",
            "Epoch 824/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 825/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0730\n",
            "Epoch 826/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0734\n",
            "Epoch 827/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0728\n",
            "Epoch 828/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0733\n",
            "Epoch 829/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0733\n",
            "Epoch 830/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0735\n",
            "Epoch 831/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0733\n",
            "Epoch 832/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 833/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0731\n",
            "Epoch 834/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0724\n",
            "Epoch 835/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 836/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0727\n",
            "Epoch 837/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0737\n",
            "Epoch 838/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0730\n",
            "Epoch 839/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0734\n",
            "Epoch 840/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0736\n",
            "Epoch 841/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0743\n",
            "Epoch 842/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0732\n",
            "Epoch 843/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0730\n",
            "Epoch 844/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0730\n",
            "Epoch 845/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0725\n",
            "Epoch 846/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0727\n",
            "Epoch 847/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0724\n",
            "Epoch 848/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0729\n",
            "Epoch 849/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0725\n",
            "Epoch 850/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0725\n",
            "Epoch 851/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0734\n",
            "Epoch 852/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0745\n",
            "Epoch 853/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0770\n",
            "Epoch 854/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0723\n",
            "Epoch 855/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0722\n",
            "Epoch 856/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0722\n",
            "Epoch 857/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0729\n",
            "Epoch 858/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0727\n",
            "Epoch 859/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0735\n",
            "Epoch 860/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0726\n",
            "Epoch 861/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0725\n",
            "Epoch 862/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0727\n",
            "Epoch 863/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0742\n",
            "Epoch 864/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0734\n",
            "Epoch 865/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0723\n",
            "Epoch 866/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0718\n",
            "Epoch 867/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0723\n",
            "Epoch 868/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0720\n",
            "Epoch 869/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0717\n",
            "Epoch 870/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0717\n",
            "Epoch 871/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0717\n",
            "Epoch 872/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0723\n",
            "Epoch 873/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0719\n",
            "Epoch 874/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0716\n",
            "Epoch 875/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0718\n",
            "Epoch 876/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0728\n",
            "Epoch 877/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0722\n",
            "Epoch 878/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0720\n",
            "Epoch 879/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0717\n",
            "Epoch 880/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0721\n",
            "Epoch 881/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0716\n",
            "Epoch 882/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0717\n",
            "Epoch 883/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0717\n",
            "Epoch 884/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0713\n",
            "Epoch 885/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0713\n",
            "Epoch 886/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0715\n",
            "Epoch 887/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0720\n",
            "Epoch 888/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0743\n",
            "Epoch 889/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0731\n",
            "Epoch 890/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0734\n",
            "Epoch 891/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0721\n",
            "Epoch 892/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0718\n",
            "Epoch 893/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0715\n",
            "Epoch 894/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0713\n",
            "Epoch 895/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0714\n",
            "Epoch 896/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0712\n",
            "Epoch 897/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0720\n",
            "Epoch 898/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0717\n",
            "Epoch 899/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0713\n",
            "Epoch 900/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0714\n",
            "Epoch 901/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0722\n",
            "Epoch 902/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0710\n",
            "Epoch 903/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0716\n",
            "Epoch 904/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0712\n",
            "Epoch 905/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0721: 0s - loss: 0.07\n",
            "Epoch 906/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0710\n",
            "Epoch 907/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0710\n",
            "Epoch 908/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0714\n",
            "Epoch 909/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0708\n",
            "Epoch 910/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0711\n",
            "Epoch 911/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0713\n",
            "Epoch 912/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0711\n",
            "Epoch 913/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0716\n",
            "Epoch 914/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0712\n",
            "Epoch 915/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0712\n",
            "Epoch 916/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0720\n",
            "Epoch 917/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0730\n",
            "Epoch 918/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0735\n",
            "Epoch 919/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0732\n",
            "Epoch 920/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0725\n",
            "Epoch 921/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0723\n",
            "Epoch 922/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0708\n",
            "Epoch 923/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0709\n",
            "Epoch 924/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0708\n",
            "Epoch 925/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0710\n",
            "Epoch 926/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0712\n",
            "Epoch 927/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0709\n",
            "Epoch 928/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0707\n",
            "Epoch 929/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0715\n",
            "Epoch 930/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0711\n",
            "Epoch 931/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0706\n",
            "Epoch 932/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0706\n",
            "Epoch 933/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0706\n",
            "Epoch 934/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0705\n",
            "Epoch 935/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0703\n",
            "Epoch 936/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0705\n",
            "Epoch 937/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0708\n",
            "Epoch 938/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0707\n",
            "Epoch 939/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0709\n",
            "Epoch 940/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0705\n",
            "Epoch 941/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0709\n",
            "Epoch 942/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0704\n",
            "Epoch 943/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0707\n",
            "Epoch 944/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0705\n",
            "Epoch 945/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0703\n",
            "Epoch 946/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0715\n",
            "Epoch 947/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0711\n",
            "Epoch 948/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0718\n",
            "Epoch 949/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0717\n",
            "Epoch 950/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0715\n",
            "Epoch 951/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0706\n",
            "Epoch 952/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0704\n",
            "Epoch 953/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0709\n",
            "Epoch 954/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0707\n",
            "Epoch 955/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0699\n",
            "Epoch 956/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0702\n",
            "Epoch 957/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0697\n",
            "Epoch 958/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0706\n",
            "Epoch 959/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0715\n",
            "Epoch 960/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0700\n",
            "Epoch 961/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0700\n",
            "Epoch 962/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0701\n",
            "Epoch 963/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0704\n",
            "Epoch 964/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0699\n",
            "Epoch 965/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0697\n",
            "Epoch 966/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0716\n",
            "Epoch 967/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0709\n",
            "Epoch 968/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0703\n",
            "Epoch 969/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0699\n",
            "Epoch 970/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0703\n",
            "Epoch 971/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0695\n",
            "Epoch 972/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0696\n",
            "Epoch 973/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0696\n",
            "Epoch 974/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0699\n",
            "Epoch 975/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0700\n",
            "Epoch 976/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0713\n",
            "Epoch 977/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0724\n",
            "Epoch 978/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0705\n",
            "Epoch 979/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0696\n",
            "Epoch 980/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0695\n",
            "Epoch 981/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0697\n",
            "Epoch 982/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0693\n",
            "Epoch 983/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0702\n",
            "Epoch 984/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0696\n",
            "Epoch 985/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0695\n",
            "Epoch 986/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0695\n",
            "Epoch 987/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0695\n",
            "Epoch 988/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0698\n",
            "Epoch 989/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0702\n",
            "Epoch 990/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0693\n",
            "Epoch 991/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0693\n",
            "Epoch 992/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0693\n",
            "Epoch 993/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0698\n",
            "Epoch 994/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0709\n",
            "Epoch 995/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0706\n",
            "Epoch 996/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0701\n",
            "Epoch 997/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0694\n",
            "Epoch 998/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0699\n",
            "Epoch 999/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0693\n",
            "Epoch 1000/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0696\n",
            "Epoch 1001/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0691\n",
            "Epoch 1002/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0689\n",
            "Epoch 1003/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0692\n",
            "Epoch 1004/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0689\n",
            "Epoch 1005/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0688\n",
            "Epoch 1006/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0695\n",
            "Epoch 1007/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0693\n",
            "Epoch 1008/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0689\n",
            "Epoch 1009/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0691\n",
            "Epoch 1010/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0686\n",
            "Epoch 1011/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0689\n",
            "Epoch 1012/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0689\n",
            "Epoch 1013/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0694\n",
            "Epoch 1014/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0685\n",
            "Epoch 1015/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0689\n",
            "Epoch 1016/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0699\n",
            "Epoch 1017/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0703\n",
            "Epoch 1018/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0703\n",
            "Epoch 1019/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0692\n",
            "Epoch 1020/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0697\n",
            "Epoch 1021/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0689\n",
            "Epoch 1022/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0689\n",
            "Epoch 1023/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0695\n",
            "Epoch 1024/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0683\n",
            "Epoch 1025/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0684\n",
            "Epoch 1026/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0686\n",
            "Epoch 1027/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0686\n",
            "Epoch 1028/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0684\n",
            "Epoch 1029/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0691\n",
            "Epoch 1030/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0686\n",
            "Epoch 1031/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0687\n",
            "Epoch 1032/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0683\n",
            "Epoch 1033/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0685\n",
            "Epoch 1034/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0687\n",
            "Epoch 1035/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0685\n",
            "Epoch 1036/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0682\n",
            "Epoch 1037/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0681\n",
            "Epoch 1038/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0682\n",
            "Epoch 1039/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0682\n",
            "Epoch 1040/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0680\n",
            "Epoch 1041/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0685\n",
            "Epoch 1042/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0693\n",
            "Epoch 1043/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0682\n",
            "Epoch 1044/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0683\n",
            "Epoch 1045/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0697\n",
            "Epoch 1046/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0689\n",
            "Epoch 1047/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0682\n",
            "Epoch 1048/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0681\n",
            "Epoch 1049/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0683\n",
            "Epoch 1050/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0681\n",
            "Epoch 1051/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0684\n",
            "Epoch 1052/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0677\n",
            "Epoch 1053/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0677\n",
            "Epoch 1054/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0678\n",
            "Epoch 1055/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0678\n",
            "Epoch 1056/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1057/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0679\n",
            "Epoch 1058/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0679\n",
            "Epoch 1059/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0695\n",
            "Epoch 1060/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0698\n",
            "Epoch 1061/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0686\n",
            "Epoch 1062/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0691\n",
            "Epoch 1063/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0678\n",
            "Epoch 1064/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0681\n",
            "Epoch 1065/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0679\n",
            "Epoch 1066/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0678\n",
            "Epoch 1067/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0677\n",
            "Epoch 1068/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0680\n",
            "Epoch 1069/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1070/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0677\n",
            "Epoch 1071/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0677\n",
            "Epoch 1072/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0682\n",
            "Epoch 1073/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0682\n",
            "Epoch 1074/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0675\n",
            "Epoch 1075/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0684\n",
            "Epoch 1076/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0681\n",
            "Epoch 1077/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0688\n",
            "Epoch 1078/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0679\n",
            "Epoch 1079/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0674\n",
            "Epoch 1080/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1081/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0671\n",
            "Epoch 1082/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0671\n",
            "Epoch 1083/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0674\n",
            "Epoch 1084/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0670\n",
            "Epoch 1085/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1086/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0679\n",
            "Epoch 1087/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1088/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0685\n",
            "Epoch 1089/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1090/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0674\n",
            "Epoch 1091/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0675\n",
            "Epoch 1092/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0675\n",
            "Epoch 1093/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0676\n",
            "Epoch 1094/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0674\n",
            "Epoch 1095/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0671\n",
            "Epoch 1096/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1097/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0670\n",
            "Epoch 1098/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0670\n",
            "Epoch 1099/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0669\n",
            "Epoch 1100/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1101/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0671\n",
            "Epoch 1102/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0670\n",
            "Epoch 1103/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0673\n",
            "Epoch 1104/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0669\n",
            "Epoch 1105/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0673\n",
            "Epoch 1106/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0667\n",
            "Epoch 1107/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0668\n",
            "Epoch 1108/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0673\n",
            "Epoch 1109/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0668\n",
            "Epoch 1110/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0669\n",
            "Epoch 1111/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0666\n",
            "Epoch 1112/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0671\n",
            "Epoch 1113/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0671\n",
            "Epoch 1114/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0670\n",
            "Epoch 1115/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1116/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0682\n",
            "Epoch 1117/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0668\n",
            "Epoch 1118/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1119/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0676\n",
            "Epoch 1120/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0671\n",
            "Epoch 1121/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0668\n",
            "Epoch 1122/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1123/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0670\n",
            "Epoch 1124/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0687\n",
            "Epoch 1125/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0672\n",
            "Epoch 1126/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0672\n",
            "Epoch 1127/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0669\n",
            "Epoch 1128/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0667\n",
            "Epoch 1129/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0679\n",
            "Epoch 1130/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0680\n",
            "Epoch 1131/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0666\n",
            "Epoch 1132/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0664\n",
            "Epoch 1133/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0664\n",
            "Epoch 1134/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0661\n",
            "Epoch 1135/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0665\n",
            "Epoch 1136/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0662\n",
            "Epoch 1137/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0663\n",
            "Epoch 1138/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0660\n",
            "Epoch 1139/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0661\n",
            "Epoch 1140/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0669\n",
            "Epoch 1141/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0665\n",
            "Epoch 1142/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0665\n",
            "Epoch 1143/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0666\n",
            "Epoch 1144/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0675\n",
            "Epoch 1145/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0669\n",
            "Epoch 1146/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0660\n",
            "Epoch 1147/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0667\n",
            "Epoch 1148/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0664\n",
            "Epoch 1149/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0663\n",
            "Epoch 1150/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0674\n",
            "Epoch 1151/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0660\n",
            "Epoch 1152/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0667\n",
            "Epoch 1153/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0656\n",
            "Epoch 1154/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0659\n",
            "Epoch 1155/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0660\n",
            "Epoch 1156/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0670\n",
            "Epoch 1157/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0672\n",
            "Epoch 1158/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0664\n",
            "Epoch 1159/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0656\n",
            "Epoch 1160/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0658\n",
            "Epoch 1161/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0660\n",
            "Epoch 1162/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0659\n",
            "Epoch 1163/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0659\n",
            "Epoch 1164/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0654\n",
            "Epoch 1165/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0658\n",
            "Epoch 1166/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0659\n",
            "Epoch 1167/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0660\n",
            "Epoch 1168/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0663\n",
            "Epoch 1169/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0656\n",
            "Epoch 1170/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0654\n",
            "Epoch 1171/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0654\n",
            "Epoch 1172/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0660\n",
            "Epoch 1173/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0660\n",
            "Epoch 1174/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0661\n",
            "Epoch 1175/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0659\n",
            "Epoch 1176/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0672\n",
            "Epoch 1177/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0668\n",
            "Epoch 1178/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0658\n",
            "Epoch 1179/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0655\n",
            "Epoch 1180/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0663\n",
            "Epoch 1181/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0663\n",
            "Epoch 1182/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0664\n",
            "Epoch 1183/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0661\n",
            "Epoch 1184/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0652\n",
            "Epoch 1185/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0653\n",
            "Epoch 1186/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0655\n",
            "Epoch 1187/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0654\n",
            "Epoch 1188/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0659\n",
            "Epoch 1189/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0663\n",
            "Epoch 1190/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0651\n",
            "Epoch 1191/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0651\n",
            "Epoch 1192/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0658\n",
            "Epoch 1193/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0652\n",
            "Epoch 1194/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0651\n",
            "Epoch 1195/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0656\n",
            "Epoch 1196/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0654\n",
            "Epoch 1197/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0657\n",
            "Epoch 1198/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0648\n",
            "Epoch 1199/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0653\n",
            "Epoch 1200/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0659\n",
            "Epoch 1201/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0661\n",
            "Epoch 1202/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0658\n",
            "Epoch 1203/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0660\n",
            "Epoch 1204/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0652\n",
            "Epoch 1205/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0662\n",
            "Epoch 1206/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0665\n",
            "Epoch 1207/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0648\n",
            "Epoch 1208/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0650\n",
            "Epoch 1209/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0649\n",
            "Epoch 1210/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0650\n",
            "Epoch 1211/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0656\n",
            "Epoch 1212/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0656\n",
            "Epoch 1213/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0649\n",
            "Epoch 1214/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0657\n",
            "Epoch 1215/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0657\n",
            "Epoch 1216/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0666\n",
            "Epoch 1217/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0647\n",
            "Epoch 1218/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0648\n",
            "Epoch 1219/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0651\n",
            "Epoch 1220/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0647\n",
            "Epoch 1221/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0658\n",
            "Epoch 1222/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0650\n",
            "Epoch 1223/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0647\n",
            "Epoch 1224/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0646\n",
            "Epoch 1225/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0650\n",
            "Epoch 1226/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0643\n",
            "Epoch 1227/2000\n",
            "15746/15746 [==============================] - 0s 15us/step - loss: 0.0643\n",
            "Epoch 1228/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0649\n",
            "Epoch 1229/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0645\n",
            "Epoch 1230/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0642\n",
            "Epoch 1231/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0646\n",
            "Epoch 1232/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0650\n",
            "Epoch 1233/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0644\n",
            "Epoch 1234/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0645\n",
            "Epoch 1235/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0649\n",
            "Epoch 1236/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0649\n",
            "Epoch 1237/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0658\n",
            "Epoch 1238/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0647\n",
            "Epoch 1239/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0646\n",
            "Epoch 1240/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0651\n",
            "Epoch 1241/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0640\n",
            "Epoch 1242/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0640\n",
            "Epoch 1243/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0640\n",
            "Epoch 1244/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0643\n",
            "Epoch 1245/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0641\n",
            "Epoch 1246/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0646\n",
            "Epoch 1247/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0643\n",
            "Epoch 1248/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0644\n",
            "Epoch 1249/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0643\n",
            "Epoch 1250/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0645\n",
            "Epoch 1251/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0643\n",
            "Epoch 1252/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0641\n",
            "Epoch 1253/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0641\n",
            "Epoch 1254/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0643\n",
            "Epoch 1255/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0642\n",
            "Epoch 1256/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0637\n",
            "Epoch 1257/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0638\n",
            "Epoch 1258/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0638\n",
            "Epoch 1259/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0645\n",
            "Epoch 1260/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0639\n",
            "Epoch 1261/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0643\n",
            "Epoch 1262/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0644\n",
            "Epoch 1263/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0640\n",
            "Epoch 1264/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0644\n",
            "Epoch 1265/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0635\n",
            "Epoch 1266/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0641\n",
            "Epoch 1267/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0649\n",
            "Epoch 1268/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0651\n",
            "Epoch 1269/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0655\n",
            "Epoch 1270/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0638\n",
            "Epoch 1271/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0641\n",
            "Epoch 1272/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0636\n",
            "Epoch 1273/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0636\n",
            "Epoch 1274/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0636\n",
            "Epoch 1275/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0635\n",
            "Epoch 1276/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0637\n",
            "Epoch 1277/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0640\n",
            "Epoch 1278/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0640\n",
            "Epoch 1279/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0644\n",
            "Epoch 1280/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0637\n",
            "Epoch 1281/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0634\n",
            "Epoch 1282/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0637\n",
            "Epoch 1283/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0636\n",
            "Epoch 1284/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0648\n",
            "Epoch 1285/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0653\n",
            "Epoch 1286/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0645\n",
            "Epoch 1287/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0642\n",
            "Epoch 1288/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0635\n",
            "Epoch 1289/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0635\n",
            "Epoch 1290/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0643\n",
            "Epoch 1291/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0648\n",
            "Epoch 1292/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0642\n",
            "Epoch 1293/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0634\n",
            "Epoch 1294/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0631\n",
            "Epoch 1295/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0634\n",
            "Epoch 1296/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1297/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0631\n",
            "Epoch 1298/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0630\n",
            "Epoch 1299/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1300/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0638\n",
            "Epoch 1301/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0632\n",
            "Epoch 1302/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1303/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0636\n",
            "Epoch 1304/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0632\n",
            "Epoch 1305/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0641\n",
            "Epoch 1306/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1307/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0630\n",
            "Epoch 1308/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0630\n",
            "Epoch 1309/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0631\n",
            "Epoch 1310/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0632\n",
            "Epoch 1311/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0633\n",
            "Epoch 1312/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0631\n",
            "Epoch 1313/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0630\n",
            "Epoch 1314/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0632\n",
            "Epoch 1315/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0640\n",
            "Epoch 1316/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0644\n",
            "Epoch 1317/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0635\n",
            "Epoch 1318/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0639\n",
            "Epoch 1319/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0627\n",
            "Epoch 1320/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0635\n",
            "Epoch 1321/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0628\n",
            "Epoch 1322/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0633\n",
            "Epoch 1323/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0656\n",
            "Epoch 1324/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0638\n",
            "Epoch 1325/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0634\n",
            "Epoch 1326/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0638\n",
            "Epoch 1327/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0629\n",
            "Epoch 1328/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0628\n",
            "Epoch 1329/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0631\n",
            "Epoch 1330/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0632\n",
            "Epoch 1331/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0629\n",
            "Epoch 1332/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0631\n",
            "Epoch 1333/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0628\n",
            "Epoch 1334/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1335/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1336/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0636\n",
            "Epoch 1337/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0628\n",
            "Epoch 1338/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0641\n",
            "Epoch 1339/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0640\n",
            "Epoch 1340/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0643\n",
            "Epoch 1341/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0629\n",
            "Epoch 1342/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0627\n",
            "Epoch 1343/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0635\n",
            "Epoch 1344/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0630\n",
            "Epoch 1345/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0628\n",
            "Epoch 1346/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0628\n",
            "Epoch 1347/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0625\n",
            "Epoch 1348/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0624\n",
            "Epoch 1349/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1350/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0628\n",
            "Epoch 1351/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0626\n",
            "Epoch 1352/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0631\n",
            "Epoch 1353/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1354/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0624\n",
            "Epoch 1355/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0636\n",
            "Epoch 1356/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0624\n",
            "Epoch 1357/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1358/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0626\n",
            "Epoch 1359/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0631\n",
            "Epoch 1360/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0630\n",
            "Epoch 1361/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0626\n",
            "Epoch 1362/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0625\n",
            "Epoch 1363/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0626\n",
            "Epoch 1364/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0626\n",
            "Epoch 1365/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1366/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0620\n",
            "Epoch 1367/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0620\n",
            "Epoch 1368/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0621\n",
            "Epoch 1369/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0626\n",
            "Epoch 1370/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1371/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1372/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0621\n",
            "Epoch 1373/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0626\n",
            "Epoch 1374/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1375/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1376/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1377/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0629\n",
            "Epoch 1378/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0625\n",
            "Epoch 1379/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0621\n",
            "Epoch 1380/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0629\n",
            "Epoch 1381/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0626\n",
            "Epoch 1382/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1383/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0625\n",
            "Epoch 1384/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0625\n",
            "Epoch 1385/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0623\n",
            "Epoch 1386/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0618\n",
            "Epoch 1387/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0619\n",
            "Epoch 1388/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1389/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0621\n",
            "Epoch 1390/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0619\n",
            "Epoch 1391/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1392/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0620\n",
            "Epoch 1393/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0621\n",
            "Epoch 1394/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0631\n",
            "Epoch 1395/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0630\n",
            "Epoch 1396/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0625\n",
            "Epoch 1397/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0624\n",
            "Epoch 1398/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0626\n",
            "Epoch 1399/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0621\n",
            "Epoch 1400/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0618\n",
            "Epoch 1401/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0625\n",
            "Epoch 1402/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0633\n",
            "Epoch 1403/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1404/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1405/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0624\n",
            "Epoch 1406/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1407/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0623\n",
            "Epoch 1408/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0621\n",
            "Epoch 1409/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0625\n",
            "Epoch 1410/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0617\n",
            "Epoch 1411/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1412/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0621\n",
            "Epoch 1413/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0623\n",
            "Epoch 1414/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0621\n",
            "Epoch 1415/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0614\n",
            "Epoch 1416/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1417/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0612\n",
            "Epoch 1418/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0615\n",
            "Epoch 1419/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0621\n",
            "Epoch 1420/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1421/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0617\n",
            "Epoch 1422/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0614\n",
            "Epoch 1423/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0613\n",
            "Epoch 1424/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0612\n",
            "Epoch 1425/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0613\n",
            "Epoch 1426/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1427/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0612\n",
            "Epoch 1428/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0611\n",
            "Epoch 1429/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0616\n",
            "Epoch 1430/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0631\n",
            "Epoch 1431/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0618\n",
            "Epoch 1432/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0620\n",
            "Epoch 1433/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0615\n",
            "Epoch 1434/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0612\n",
            "Epoch 1435/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0617\n",
            "Epoch 1436/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0615\n",
            "Epoch 1437/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0610\n",
            "Epoch 1438/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0612\n",
            "Epoch 1439/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0612\n",
            "Epoch 1440/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0617\n",
            "Epoch 1441/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0613\n",
            "Epoch 1442/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0615\n",
            "Epoch 1443/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0615\n",
            "Epoch 1444/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0620\n",
            "Epoch 1445/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0615\n",
            "Epoch 1446/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0626\n",
            "Epoch 1447/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0643\n",
            "Epoch 1448/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0627\n",
            "Epoch 1449/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0611\n",
            "Epoch 1450/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1451/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0622\n",
            "Epoch 1452/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0618\n",
            "Epoch 1453/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0614\n",
            "Epoch 1454/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0617\n",
            "Epoch 1455/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0633\n",
            "Epoch 1456/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0619\n",
            "Epoch 1457/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0612\n",
            "Epoch 1458/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0617\n",
            "Epoch 1459/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0618\n",
            "Epoch 1460/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0612\n",
            "Epoch 1461/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0617\n",
            "Epoch 1462/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0611\n",
            "Epoch 1463/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0610\n",
            "Epoch 1464/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0607\n",
            "Epoch 1465/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0608\n",
            "Epoch 1466/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0619\n",
            "Epoch 1467/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1468/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0611\n",
            "Epoch 1469/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0613\n",
            "Epoch 1470/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0615\n",
            "Epoch 1471/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0614\n",
            "Epoch 1472/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1473/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0610\n",
            "Epoch 1474/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1475/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0611\n",
            "Epoch 1476/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1477/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1478/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0619\n",
            "Epoch 1479/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1480/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0610\n",
            "Epoch 1481/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0622\n",
            "Epoch 1482/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0621\n",
            "Epoch 1483/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0610\n",
            "Epoch 1484/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0611\n",
            "Epoch 1485/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1486/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1487/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1488/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1489/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0610\n",
            "Epoch 1490/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1491/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0608\n",
            "Epoch 1492/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1493/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0616\n",
            "Epoch 1494/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0622\n",
            "Epoch 1495/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0610\n",
            "Epoch 1496/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0617\n",
            "Epoch 1497/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0611\n",
            "Epoch 1498/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1499/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0612\n",
            "Epoch 1500/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1501/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0617\n",
            "Epoch 1502/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0608\n",
            "Epoch 1503/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1504/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1505/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1506/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1507/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0613\n",
            "Epoch 1508/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1509/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1510/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0601\n",
            "Epoch 1511/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1512/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0607\n",
            "Epoch 1513/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0606\n",
            "Epoch 1514/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0604\n",
            "Epoch 1515/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1516/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0604\n",
            "Epoch 1517/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0600\n",
            "Epoch 1518/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0608\n",
            "Epoch 1519/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1520/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0600\n",
            "Epoch 1521/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0601\n",
            "Epoch 1522/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0603\n",
            "Epoch 1523/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0611\n",
            "Epoch 1524/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0611\n",
            "Epoch 1525/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0605\n",
            "Epoch 1526/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1527/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0609\n",
            "Epoch 1528/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0611\n",
            "Epoch 1529/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1530/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0611\n",
            "Epoch 1531/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0607\n",
            "Epoch 1532/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0610\n",
            "Epoch 1533/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1534/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0601\n",
            "Epoch 1535/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1536/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0600\n",
            "Epoch 1537/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0598\n",
            "Epoch 1538/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0608\n",
            "Epoch 1539/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1540/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0604\n",
            "Epoch 1541/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1542/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0598\n",
            "Epoch 1543/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1544/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1545/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0600\n",
            "Epoch 1546/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0598\n",
            "Epoch 1547/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0599\n",
            "Epoch 1548/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0597\n",
            "Epoch 1549/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0597\n",
            "Epoch 1550/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1551/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1552/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1553/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1554/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1555/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1556/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1557/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1558/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1559/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0604\n",
            "Epoch 1560/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0598\n",
            "Epoch 1561/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0598\n",
            "Epoch 1562/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0597\n",
            "Epoch 1563/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0600\n",
            "Epoch 1564/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0596\n",
            "Epoch 1565/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0598\n",
            "Epoch 1566/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0597\n",
            "Epoch 1567/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0596\n",
            "Epoch 1568/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0595\n",
            "Epoch 1569/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1570/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0605\n",
            "Epoch 1571/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0601\n",
            "Epoch 1572/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0607\n",
            "Epoch 1573/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0607\n",
            "Epoch 1574/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0609\n",
            "Epoch 1575/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0599\n",
            "Epoch 1576/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1577/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0596\n",
            "Epoch 1578/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1579/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1580/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1581/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0597\n",
            "Epoch 1582/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0606\n",
            "Epoch 1583/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0616\n",
            "Epoch 1584/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0603\n",
            "Epoch 1585/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0597\n",
            "Epoch 1586/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0596\n",
            "Epoch 1587/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0599\n",
            "Epoch 1588/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0615\n",
            "Epoch 1589/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0616\n",
            "Epoch 1590/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0613\n",
            "Epoch 1591/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0593\n",
            "Epoch 1592/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1593/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0594\n",
            "Epoch 1594/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0601\n",
            "Epoch 1595/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0599\n",
            "Epoch 1596/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0600\n",
            "Epoch 1597/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0591\n",
            "Epoch 1598/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1599/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1600/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0604\n",
            "Epoch 1601/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1602/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0595\n",
            "Epoch 1603/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0595\n",
            "Epoch 1604/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1605/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0604\n",
            "Epoch 1606/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0605\n",
            "Epoch 1607/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1608/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0595\n",
            "Epoch 1609/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0597\n",
            "Epoch 1610/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0596\n",
            "Epoch 1611/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0592\n",
            "Epoch 1612/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0595\n",
            "Epoch 1613/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0606\n",
            "Epoch 1614/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0605\n",
            "Epoch 1615/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0594\n",
            "Epoch 1616/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1617/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0593\n",
            "Epoch 1618/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0592\n",
            "Epoch 1619/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0593\n",
            "Epoch 1620/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0593\n",
            "Epoch 1621/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0590\n",
            "Epoch 1622/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0590\n",
            "Epoch 1623/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1624/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0592\n",
            "Epoch 1625/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0592\n",
            "Epoch 1626/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0589\n",
            "Epoch 1627/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0591\n",
            "Epoch 1628/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1629/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0590\n",
            "Epoch 1630/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0589\n",
            "Epoch 1631/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1632/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0589\n",
            "Epoch 1633/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0595\n",
            "Epoch 1634/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1635/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0590\n",
            "Epoch 1636/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1637/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0595\n",
            "Epoch 1638/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0602\n",
            "Epoch 1639/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1640/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0586\n",
            "Epoch 1641/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0589\n",
            "Epoch 1642/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0590\n",
            "Epoch 1643/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1644/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1645/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1646/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0596\n",
            "Epoch 1647/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1648/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1649/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0594\n",
            "Epoch 1650/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0593\n",
            "Epoch 1651/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1652/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1653/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0589\n",
            "Epoch 1654/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0589\n",
            "Epoch 1655/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0594\n",
            "Epoch 1656/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0605\n",
            "Epoch 1657/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0596\n",
            "Epoch 1658/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1659/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0590\n",
            "Epoch 1660/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0589\n",
            "Epoch 1661/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0585\n",
            "Epoch 1662/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0587\n",
            "Epoch 1663/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0590\n",
            "Epoch 1664/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0601\n",
            "Epoch 1665/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0598\n",
            "Epoch 1666/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0598\n",
            "Epoch 1667/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1668/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0586\n",
            "Epoch 1669/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0594\n",
            "Epoch 1670/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0588\n",
            "Epoch 1671/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0589\n",
            "Epoch 1672/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0585\n",
            "Epoch 1673/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1674/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0589\n",
            "Epoch 1675/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0584\n",
            "Epoch 1676/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0583\n",
            "Epoch 1677/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0585\n",
            "Epoch 1678/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0587\n",
            "Epoch 1679/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0586\n",
            "Epoch 1680/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0587\n",
            "Epoch 1681/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0595\n",
            "Epoch 1682/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0593\n",
            "Epoch 1683/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0584\n",
            "Epoch 1684/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1685/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1686/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0584\n",
            "Epoch 1687/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0584\n",
            "Epoch 1688/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0585\n",
            "Epoch 1689/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0582\n",
            "Epoch 1690/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0587\n",
            "Epoch 1691/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1692/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1693/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0583\n",
            "Epoch 1694/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0587\n",
            "Epoch 1695/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1696/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0587\n",
            "Epoch 1697/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0581\n",
            "Epoch 1698/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0582\n",
            "Epoch 1699/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0583\n",
            "Epoch 1700/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0585\n",
            "Epoch 1701/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0581\n",
            "Epoch 1702/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0580\n",
            "Epoch 1703/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1704/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0596\n",
            "Epoch 1705/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0588\n",
            "Epoch 1706/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1707/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0595\n",
            "Epoch 1708/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0585\n",
            "Epoch 1709/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0591\n",
            "Epoch 1710/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0586\n",
            "Epoch 1711/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0585\n",
            "Epoch 1712/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0590\n",
            "Epoch 1713/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0582\n",
            "Epoch 1714/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1715/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0578\n",
            "Epoch 1716/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0578\n",
            "Epoch 1717/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0596\n",
            "Epoch 1718/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0586\n",
            "Epoch 1719/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0585\n",
            "Epoch 1720/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0582\n",
            "Epoch 1721/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0582\n",
            "Epoch 1722/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0579\n",
            "Epoch 1723/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1724/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0579\n",
            "Epoch 1725/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0579\n",
            "Epoch 1726/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0582\n",
            "Epoch 1727/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0577\n",
            "Epoch 1728/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0582\n",
            "Epoch 1729/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1730/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0586\n",
            "Epoch 1731/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0596\n",
            "Epoch 1732/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0597\n",
            "Epoch 1733/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0590\n",
            "Epoch 1734/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0584\n",
            "Epoch 1735/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1736/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0577\n",
            "Epoch 1737/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0578\n",
            "Epoch 1738/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1739/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0583\n",
            "Epoch 1740/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0577\n",
            "Epoch 1741/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0580\n",
            "Epoch 1742/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0580\n",
            "Epoch 1743/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0592\n",
            "Epoch 1744/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0583\n",
            "Epoch 1745/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0579\n",
            "Epoch 1746/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0576\n",
            "Epoch 1747/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1748/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0578\n",
            "Epoch 1749/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0577\n",
            "Epoch 1750/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0580\n",
            "Epoch 1751/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0580\n",
            "Epoch 1752/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0579\n",
            "Epoch 1753/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0577\n",
            "Epoch 1754/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0574\n",
            "Epoch 1755/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0578\n",
            "Epoch 1756/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0578\n",
            "Epoch 1757/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0575\n",
            "Epoch 1758/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0584\n",
            "Epoch 1759/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0591\n",
            "Epoch 1760/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0579\n",
            "Epoch 1761/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0577\n",
            "Epoch 1762/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0588\n",
            "Epoch 1763/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1764/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0579\n",
            "Epoch 1765/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0574\n",
            "Epoch 1766/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0574\n",
            "Epoch 1767/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0575\n",
            "Epoch 1768/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0578\n",
            "Epoch 1769/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0581\n",
            "Epoch 1770/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0580\n",
            "Epoch 1771/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0585\n",
            "Epoch 1772/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0574\n",
            "Epoch 1773/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0579\n",
            "Epoch 1774/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0580\n",
            "Epoch 1775/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0582\n",
            "Epoch 1776/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0578\n",
            "Epoch 1777/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0583\n",
            "Epoch 1778/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0575\n",
            "Epoch 1779/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0578\n",
            "Epoch 1780/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0580\n",
            "Epoch 1781/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0576\n",
            "Epoch 1782/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0573\n",
            "Epoch 1783/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0575\n",
            "Epoch 1784/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0574\n",
            "Epoch 1785/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0573\n",
            "Epoch 1786/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0575\n",
            "Epoch 1787/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0574\n",
            "Epoch 1788/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0576\n",
            "Epoch 1789/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0578\n",
            "Epoch 1790/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0576\n",
            "Epoch 1791/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0577\n",
            "Epoch 1792/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0578\n",
            "Epoch 1793/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0577\n",
            "Epoch 1794/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0577\n",
            "Epoch 1795/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0575\n",
            "Epoch 1796/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.0574\n",
            "Epoch 1797/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0579\n",
            "Epoch 1798/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0578\n",
            "Epoch 1799/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0578\n",
            "Epoch 1800/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0576\n",
            "Epoch 1801/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0572\n",
            "Epoch 1802/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0575\n",
            "Epoch 1803/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0582\n",
            "Epoch 1804/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0572\n",
            "Epoch 1805/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.0577\n",
            "Epoch 1806/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0577\n",
            "Epoch 1807/2000\n",
            "15746/15746 [==============================] - 0s 31us/step - loss: 0.0573\n",
            "Epoch 1808/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0574\n",
            "Epoch 1809/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0573\n",
            "Epoch 1810/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0571\n",
            "Epoch 1811/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0570\n",
            "Epoch 1812/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.0571\n",
            "Epoch 1813/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0569\n",
            "Epoch 1814/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0573\n",
            "Epoch 1815/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0570\n",
            "Epoch 1816/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0579\n",
            "Epoch 1817/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0577\n",
            "Epoch 1818/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0573\n",
            "Epoch 1819/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0570\n",
            "Epoch 1820/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0572\n",
            "Epoch 1821/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0576\n",
            "Epoch 1822/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0581\n",
            "Epoch 1823/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0578\n",
            "Epoch 1824/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1825/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0572\n",
            "Epoch 1826/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0568\n",
            "Epoch 1827/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1828/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0571\n",
            "Epoch 1829/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1830/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0569\n",
            "Epoch 1831/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0568\n",
            "Epoch 1832/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0572\n",
            "Epoch 1833/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1834/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0571\n",
            "Epoch 1835/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1836/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1837/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0566\n",
            "Epoch 1838/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1839/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0572\n",
            "Epoch 1840/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1841/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0567\n",
            "Epoch 1842/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1843/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0574\n",
            "Epoch 1844/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0571\n",
            "Epoch 1845/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0571\n",
            "Epoch 1846/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0573\n",
            "Epoch 1847/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0574\n",
            "Epoch 1848/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0568\n",
            "Epoch 1849/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0565\n",
            "Epoch 1850/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0567\n",
            "Epoch 1851/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0569\n",
            "Epoch 1852/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0575\n",
            "Epoch 1853/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0573\n",
            "Epoch 1854/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1855/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1856/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1857/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0568\n",
            "Epoch 1858/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0570\n",
            "Epoch 1859/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0569\n",
            "Epoch 1860/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0574\n",
            "Epoch 1861/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0569\n",
            "Epoch 1862/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1863/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0568\n",
            "Epoch 1864/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1865/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1866/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0568\n",
            "Epoch 1867/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0569\n",
            "Epoch 1868/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0565\n",
            "Epoch 1869/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0569\n",
            "Epoch 1870/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0574\n",
            "Epoch 1871/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0567\n",
            "Epoch 1872/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0568\n",
            "Epoch 1873/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1874/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0563\n",
            "Epoch 1875/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0571\n",
            "Epoch 1876/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0567\n",
            "Epoch 1877/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0565\n",
            "Epoch 1878/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0570\n",
            "Epoch 1879/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0563\n",
            "Epoch 1880/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0564\n",
            "Epoch 1881/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0565\n",
            "Epoch 1882/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0564\n",
            "Epoch 1883/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0572\n",
            "Epoch 1884/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1885/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1886/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0564\n",
            "Epoch 1887/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0562\n",
            "Epoch 1888/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0565\n",
            "Epoch 1889/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0581\n",
            "Epoch 1890/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0586\n",
            "Epoch 1891/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0583\n",
            "Epoch 1892/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0573\n",
            "Epoch 1893/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0563\n",
            "Epoch 1894/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0578\n",
            "Epoch 1895/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0570\n",
            "Epoch 1896/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0572\n",
            "Epoch 1897/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0578\n",
            "Epoch 1898/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0572\n",
            "Epoch 1899/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0569\n",
            "Epoch 1900/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0563\n",
            "Epoch 1901/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0561\n",
            "Epoch 1902/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0563\n",
            "Epoch 1903/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0571\n",
            "Epoch 1904/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0561\n",
            "Epoch 1905/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0565\n",
            "Epoch 1906/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0570\n",
            "Epoch 1907/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0576\n",
            "Epoch 1908/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0565\n",
            "Epoch 1909/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0567\n",
            "Epoch 1910/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.0564\n",
            "Epoch 1911/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.0562\n",
            "Epoch 1912/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0561\n",
            "Epoch 1913/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0565\n",
            "Epoch 1914/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0562\n",
            "Epoch 1915/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0559\n",
            "Epoch 1916/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.0559\n",
            "Epoch 1917/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0560\n",
            "Epoch 1918/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0563\n",
            "Epoch 1919/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0561\n",
            "Epoch 1920/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0570\n",
            "Epoch 1921/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0586\n",
            "Epoch 1922/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1923/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0566\n",
            "Epoch 1924/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0562\n",
            "Epoch 1925/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0569\n",
            "Epoch 1926/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0562\n",
            "Epoch 1927/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0559\n",
            "Epoch 1928/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0559\n",
            "Epoch 1929/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0564\n",
            "Epoch 1930/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0562\n",
            "Epoch 1931/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0563\n",
            "Epoch 1932/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0559\n",
            "Epoch 1933/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0562\n",
            "Epoch 1934/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0562\n",
            "Epoch 1935/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0566\n",
            "Epoch 1936/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0569\n",
            "Epoch 1937/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0562\n",
            "Epoch 1938/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0563\n",
            "Epoch 1939/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0560\n",
            "Epoch 1940/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0559\n",
            "Epoch 1941/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.0561\n",
            "Epoch 1942/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0557\n",
            "Epoch 1943/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0557\n",
            "Epoch 1944/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0562\n",
            "Epoch 1945/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0568\n",
            "Epoch 1946/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0567\n",
            "Epoch 1947/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0563\n",
            "Epoch 1948/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0563\n",
            "Epoch 1949/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0578\n",
            "Epoch 1950/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0583\n",
            "Epoch 1951/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0563\n",
            "Epoch 1952/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0558\n",
            "Epoch 1953/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0558\n",
            "Epoch 1954/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0568\n",
            "Epoch 1955/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0572\n",
            "Epoch 1956/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0563\n",
            "Epoch 1957/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0556\n",
            "Epoch 1958/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.0557\n",
            "Epoch 1959/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0565\n",
            "Epoch 1960/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0558\n",
            "Epoch 1961/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0564\n",
            "Epoch 1962/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0564\n",
            "Epoch 1963/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0577\n",
            "Epoch 1964/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0572\n",
            "Epoch 1965/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0563\n",
            "Epoch 1966/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0563\n",
            "Epoch 1967/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0560\n",
            "Epoch 1968/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0563\n",
            "Epoch 1969/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0561\n",
            "Epoch 1970/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0557\n",
            "Epoch 1971/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0564\n",
            "Epoch 1972/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0555\n",
            "Epoch 1973/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0555\n",
            "Epoch 1974/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0554\n",
            "Epoch 1975/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0557\n",
            "Epoch 1976/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0579\n",
            "Epoch 1977/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0560\n",
            "Epoch 1978/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0558\n",
            "Epoch 1979/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0563\n",
            "Epoch 1980/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0562\n",
            "Epoch 1981/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0561\n",
            "Epoch 1982/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0558\n",
            "Epoch 1983/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0556\n",
            "Epoch 1984/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0556\n",
            "Epoch 1985/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0563\n",
            "Epoch 1986/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0565\n",
            "Epoch 1987/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0558\n",
            "Epoch 1988/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0559\n",
            "Epoch 1989/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0554\n",
            "Epoch 1990/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0554\n",
            "Epoch 1991/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0555\n",
            "Epoch 1992/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0559\n",
            "Epoch 1993/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0556\n",
            "Epoch 1994/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0559\n",
            "Epoch 1995/2000\n",
            "15746/15746 [==============================] - 0s 18us/step - loss: 0.0554\n",
            "Epoch 1996/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0554\n",
            "Epoch 1997/2000\n",
            "15746/15746 [==============================] - 0s 17us/step - loss: 0.0557\n",
            "Epoch 1998/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0558\n",
            "Epoch 1999/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0562\n",
            "Epoch 2000/2000\n",
            "15746/15746 [==============================] - 0s 16us/step - loss: 0.0560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x150e0ce10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0qoBByLfT44",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7d3cdbe-1a59-4de3-d8e2-b63bb367c8fa"
      },
      "source": [
        "alpharnn = Alpha_Rnn(10,0)\n",
        "alpharnn.fit(x_train_reg,y_train_reg,epochs=2000,batch_size=100,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "3192/3192 [==============================] - 2s 704us/step - loss: 1.0725\n",
            "Epoch 2/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.9052\n",
            "Epoch 3/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.5969\n",
            "Epoch 4/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.4956\n",
            "Epoch 5/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.4414\n",
            "Epoch 6/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.4004\n",
            "Epoch 7/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.3679\n",
            "Epoch 8/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.3425\n",
            "Epoch 9/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.3211\n",
            "Epoch 10/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.3046\n",
            "Epoch 11/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2907\n",
            "Epoch 12/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2794\n",
            "Epoch 13/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2697\n",
            "Epoch 14/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2623\n",
            "Epoch 15/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2547\n",
            "Epoch 16/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2490\n",
            "Epoch 17/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2436\n",
            "Epoch 18/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2391\n",
            "Epoch 19/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2355\n",
            "Epoch 20/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2319\n",
            "Epoch 21/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2291\n",
            "Epoch 22/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2261\n",
            "Epoch 23/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2235\n",
            "Epoch 24/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2222\n",
            "Epoch 25/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2200\n",
            "Epoch 26/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2188\n",
            "Epoch 27/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2168\n",
            "Epoch 28/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2157\n",
            "Epoch 29/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2144\n",
            "Epoch 30/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2137\n",
            "Epoch 31/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2129\n",
            "Epoch 32/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2129\n",
            "Epoch 33/2000\n",
            "3192/3192 [==============================] - 0s 51us/step - loss: 0.2114\n",
            "Epoch 34/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2107\n",
            "Epoch 35/2000\n",
            "3192/3192 [==============================] - 0s 58us/step - loss: 0.2108\n",
            "Epoch 36/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2096\n",
            "Epoch 37/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2092\n",
            "Epoch 38/2000\n",
            "3192/3192 [==============================] - 0s 50us/step - loss: 0.2088\n",
            "Epoch 39/2000\n",
            "3192/3192 [==============================] - 0s 51us/step - loss: 0.2082\n",
            "Epoch 40/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2078\n",
            "Epoch 41/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2075\n",
            "Epoch 42/2000\n",
            "3192/3192 [==============================] - 0s 51us/step - loss: 0.2072\n",
            "Epoch 43/2000\n",
            "3192/3192 [==============================] - 0s 50us/step - loss: 0.2075\n",
            "Epoch 44/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2069\n",
            "Epoch 45/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2062\n",
            "Epoch 46/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2063\n",
            "Epoch 47/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2057\n",
            "Epoch 48/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2061\n",
            "Epoch 49/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2052\n",
            "Epoch 50/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2050\n",
            "Epoch 51/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2049\n",
            "Epoch 52/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2047\n",
            "Epoch 53/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2044\n",
            "Epoch 54/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2044\n",
            "Epoch 55/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2042\n",
            "Epoch 56/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2038\n",
            "Epoch 57/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2038\n",
            "Epoch 58/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2039\n",
            "Epoch 59/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2036\n",
            "Epoch 60/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2036\n",
            "Epoch 61/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2034\n",
            "Epoch 62/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2050\n",
            "Epoch 63/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2043\n",
            "Epoch 64/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2031\n",
            "Epoch 65/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2037\n",
            "Epoch 66/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2029\n",
            "Epoch 67/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2027\n",
            "Epoch 68/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2029\n",
            "Epoch 69/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2030\n",
            "Epoch 70/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2027\n",
            "Epoch 71/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2024\n",
            "Epoch 72/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2027\n",
            "Epoch 73/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2020\n",
            "Epoch 74/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2027\n",
            "Epoch 75/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2025\n",
            "Epoch 76/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2022\n",
            "Epoch 77/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2028\n",
            "Epoch 78/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2025\n",
            "Epoch 79/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2028\n",
            "Epoch 80/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2029\n",
            "Epoch 81/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2021\n",
            "Epoch 82/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2030\n",
            "Epoch 83/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2020\n",
            "Epoch 84/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2022\n",
            "Epoch 85/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2018\n",
            "Epoch 86/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2021\n",
            "Epoch 87/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2023\n",
            "Epoch 88/2000\n",
            "3192/3192 [==============================] - 0s 38us/step - loss: 0.2017\n",
            "Epoch 89/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2018\n",
            "Epoch 90/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2018\n",
            "Epoch 91/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2020\n",
            "Epoch 92/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2024\n",
            "Epoch 93/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2015\n",
            "Epoch 94/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2016\n",
            "Epoch 95/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2015\n",
            "Epoch 96/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2025\n",
            "Epoch 97/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2014\n",
            "Epoch 98/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2021\n",
            "Epoch 99/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2025\n",
            "Epoch 100/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2020\n",
            "Epoch 101/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2019\n",
            "Epoch 102/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2014\n",
            "Epoch 103/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2013\n",
            "Epoch 104/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2012\n",
            "Epoch 105/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2014\n",
            "Epoch 106/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2023\n",
            "Epoch 107/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2018\n",
            "Epoch 108/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2014\n",
            "Epoch 109/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2011\n",
            "Epoch 110/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2009\n",
            "Epoch 111/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2021\n",
            "Epoch 112/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2020\n",
            "Epoch 113/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2011\n",
            "Epoch 114/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2008\n",
            "Epoch 115/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2010\n",
            "Epoch 116/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2011\n",
            "Epoch 117/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2016\n",
            "Epoch 118/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2019\n",
            "Epoch 119/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2013\n",
            "Epoch 120/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2014\n",
            "Epoch 121/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2010\n",
            "Epoch 122/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2014\n",
            "Epoch 123/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2013\n",
            "Epoch 124/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2027\n",
            "Epoch 125/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2013\n",
            "Epoch 126/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2033\n",
            "Epoch 127/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2014\n",
            "Epoch 128/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2008\n",
            "Epoch 129/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2010\n",
            "Epoch 130/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2008\n",
            "Epoch 131/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2021\n",
            "Epoch 132/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2010\n",
            "Epoch 133/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2006\n",
            "Epoch 134/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2013\n",
            "Epoch 135/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2006\n",
            "Epoch 136/2000\n",
            "3192/3192 [==============================] - 0s 51us/step - loss: 0.2007\n",
            "Epoch 137/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2019\n",
            "Epoch 138/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2009\n",
            "Epoch 139/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2013\n",
            "Epoch 140/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2021\n",
            "Epoch 141/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2009\n",
            "Epoch 142/2000\n",
            "3192/3192 [==============================] - 0s 43us/step - loss: 0.2012\n",
            "Epoch 143/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2006\n",
            "Epoch 144/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2008\n",
            "Epoch 145/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2007\n",
            "Epoch 146/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2010\n",
            "Epoch 147/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2012\n",
            "Epoch 148/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2008\n",
            "Epoch 149/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2012\n",
            "Epoch 150/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2012\n",
            "Epoch 151/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2011\n",
            "Epoch 152/2000\n",
            "3192/3192 [==============================] - 0s 59us/step - loss: 0.2005\n",
            "Epoch 153/2000\n",
            "3192/3192 [==============================] - 0s 55us/step - loss: 0.2008\n",
            "Epoch 154/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2009\n",
            "Epoch 155/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2008\n",
            "Epoch 156/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2004\n",
            "Epoch 157/2000\n",
            "3192/3192 [==============================] - 0s 52us/step - loss: 0.2010\n",
            "Epoch 158/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2013\n",
            "Epoch 159/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2007\n",
            "Epoch 160/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2001\n",
            "Epoch 161/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2007\n",
            "Epoch 162/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2020\n",
            "Epoch 163/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2008\n",
            "Epoch 164/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2006\n",
            "Epoch 165/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2006\n",
            "Epoch 166/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2009\n",
            "Epoch 167/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2002\n",
            "Epoch 168/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2003\n",
            "Epoch 169/2000\n",
            "3192/3192 [==============================] - 0s 46us/step - loss: 0.2006\n",
            "Epoch 170/2000\n",
            "3192/3192 [==============================] - 0s 51us/step - loss: 0.2007\n",
            "Epoch 171/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2005\n",
            "Epoch 172/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2005\n",
            "Epoch 173/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2005\n",
            "Epoch 174/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2004\n",
            "Epoch 175/2000\n",
            "3192/3192 [==============================] - 0s 45us/step - loss: 0.2005\n",
            "Epoch 176/2000\n",
            "3192/3192 [==============================] - 0s 52us/step - loss: 0.2011\n",
            "Epoch 177/2000\n",
            "3192/3192 [==============================] - 0s 52us/step - loss: 0.2011\n",
            "Epoch 178/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2015\n",
            "Epoch 179/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2006\n",
            "Epoch 180/2000\n",
            "3192/3192 [==============================] - 0s 44us/step - loss: 0.2004\n",
            "Epoch 181/2000\n",
            "3192/3192 [==============================] - 0s 50us/step - loss: 0.2007\n",
            "Epoch 182/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2009\n",
            "Epoch 183/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2018\n",
            "Epoch 184/2000\n",
            "3192/3192 [==============================] - 0s 53us/step - loss: 0.2012\n",
            "Epoch 185/2000\n",
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2002\n",
            "Epoch 186/2000\n",
            "3192/3192 [==============================] - 0s 48us/step - loss: 0.2005\n",
            "Epoch 187/2000\n",
            "3192/3192 [==============================] - 0s 47us/step - loss: 0.2002\n",
            "Epoch 188/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3192/3192 [==============================] - 0s 49us/step - loss: 0.2006\n",
            "Epoch 189/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2007\n",
            "Epoch 190/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2005\n",
            "Epoch 191/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2008\n",
            "Epoch 192/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2002\n",
            "Epoch 193/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2007\n",
            "Epoch 194/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2002\n",
            "Epoch 195/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2021\n",
            "Epoch 196/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2009\n",
            "Epoch 197/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2011\n",
            "Epoch 198/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2011\n",
            "Epoch 199/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2001\n",
            "Epoch 200/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2008\n",
            "Epoch 201/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2011\n",
            "Epoch 202/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2005\n",
            "Epoch 203/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2000\n",
            "Epoch 204/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2001\n",
            "Epoch 205/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 206/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2002\n",
            "Epoch 207/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2006\n",
            "Epoch 208/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2016\n",
            "Epoch 209/2000\n",
            "3192/3192 [==============================] - 0s 38us/step - loss: 0.2003\n",
            "Epoch 210/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.1997\n",
            "Epoch 211/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2006\n",
            "Epoch 212/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2005\n",
            "Epoch 213/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2011\n",
            "Epoch 214/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2002\n",
            "Epoch 215/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.1999\n",
            "Epoch 216/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 217/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2004\n",
            "Epoch 218/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2006\n",
            "Epoch 219/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2007\n",
            "Epoch 220/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2002\n",
            "Epoch 221/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2002\n",
            "Epoch 222/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2000\n",
            "Epoch 223/2000\n",
            "3192/3192 [==============================] - 0s 38us/step - loss: 0.2004\n",
            "Epoch 224/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2003\n",
            "Epoch 225/2000\n",
            "3192/3192 [==============================] - 0s 38us/step - loss: 0.2000\n",
            "Epoch 226/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2001\n",
            "Epoch 227/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2001\n",
            "Epoch 228/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2002\n",
            "Epoch 229/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2002\n",
            "Epoch 230/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2005\n",
            "Epoch 231/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.1998\n",
            "Epoch 232/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2000\n",
            "Epoch 233/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2002\n",
            "Epoch 234/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2001\n",
            "Epoch 235/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2001\n",
            "Epoch 236/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2004\n",
            "Epoch 237/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2005\n",
            "Epoch 238/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2000\n",
            "Epoch 239/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 240/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 241/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2017\n",
            "Epoch 242/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 243/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2001\n",
            "Epoch 244/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2006\n",
            "Epoch 245/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2009\n",
            "Epoch 246/2000\n",
            "3192/3192 [==============================] - 0s 42us/step - loss: 0.2000\n",
            "Epoch 247/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 248/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2001\n",
            "Epoch 249/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 250/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2007\n",
            "Epoch 251/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2010\n",
            "Epoch 252/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 253/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2003\n",
            "Epoch 254/2000\n",
            "3192/3192 [==============================] - 0s 41us/step - loss: 0.2004\n",
            "Epoch 255/2000\n",
            "3192/3192 [==============================] - 0s 39us/step - loss: 0.2004\n",
            "Epoch 256/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2003\n",
            "Epoch 257/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2001\n",
            "Epoch 258/2000\n",
            "3192/3192 [==============================] - 0s 38us/step - loss: 0.2001\n",
            "Epoch 259/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2005\n",
            "Epoch 260/2000\n",
            "3192/3192 [==============================] - 0s 40us/step - loss: 0.2005\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00260: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1464e7278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yKQHGlOfT47",
        "colab_type": "code",
        "colab": {},
        "outputId": "36f747dd-84b4-4f4d-dcfc-ce48e6bf355a"
      },
      "source": [
        "alphars = RS_(10,0)\n",
        "alphars.fit(x_train_reg,y_train_reg,epochs=2000,batch_size=1500,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "15746/15746 [==============================] - 6s 373us/step - loss: 1.1314\n",
            "Epoch 2/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 1.1226\n",
            "Epoch 3/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 1.1151\n",
            "Epoch 4/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.1076\n",
            "Epoch 5/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 1.1014\n",
            "Epoch 6/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0956\n",
            "Epoch 7/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0895\n",
            "Epoch 8/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0828\n",
            "Epoch 9/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0757\n",
            "Epoch 10/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0666\n",
            "Epoch 11/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0509\n",
            "Epoch 12/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0393\n",
            "Epoch 13/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 1.0263\n",
            "Epoch 14/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0412\n",
            "Epoch 15/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0370\n",
            "Epoch 16/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0211\n",
            "Epoch 17/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9999\n",
            "Epoch 18/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9745\n",
            "Epoch 19/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9055\n",
            "Epoch 20/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9135\n",
            "Epoch 21/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.8517\n",
            "Epoch 22/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9179\n",
            "Epoch 23/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8430\n",
            "Epoch 24/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7903\n",
            "Epoch 25/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7443\n",
            "Epoch 26/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.7226\n",
            "Epoch 27/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7058\n",
            "Epoch 28/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6590\n",
            "Epoch 29/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6466\n",
            "Epoch 30/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6270\n",
            "Epoch 31/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6882\n",
            "Epoch 32/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6518\n",
            "Epoch 33/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.5869\n",
            "Epoch 34/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6183\n",
            "Epoch 35/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.6137\n",
            "Epoch 36/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.8021\n",
            "Epoch 37/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.9219\n",
            "Epoch 38/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7841\n",
            "Epoch 39/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.7339\n",
            "Epoch 40/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.1837\n",
            "Epoch 41/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.3228\n",
            "Epoch 42/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 1.1944\n",
            "Epoch 43/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.1191\n",
            "Epoch 44/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0756\n",
            "Epoch 45/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0444\n",
            "Epoch 46/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 1.0225\n",
            "Epoch 47/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9951\n",
            "Epoch 48/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9752\n",
            "Epoch 49/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9611\n",
            "Epoch 50/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9504\n",
            "Epoch 51/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9338\n",
            "Epoch 52/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9125\n",
            "Epoch 53/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9048\n",
            "Epoch 54/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.9052\n",
            "Epoch 55/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.9137\n",
            "Epoch 56/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8896\n",
            "Epoch 57/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8726\n",
            "Epoch 58/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8767\n",
            "Epoch 59/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8441\n",
            "Epoch 60/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8575\n",
            "Epoch 61/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8387\n",
            "Epoch 62/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8211\n",
            "Epoch 63/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8022\n",
            "Epoch 64/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7931\n",
            "Epoch 65/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7839\n",
            "Epoch 66/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7727\n",
            "Epoch 67/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7561\n",
            "Epoch 68/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7371\n",
            "Epoch 69/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7122\n",
            "Epoch 70/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6891\n",
            "Epoch 71/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8058\n",
            "Epoch 72/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8908\n",
            "Epoch 73/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8668\n",
            "Epoch 74/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.8121\n",
            "Epoch 75/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7435\n",
            "Epoch 76/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.6829\n",
            "Epoch 77/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.7211\n",
            "Epoch 78/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.6559\n",
            "Epoch 79/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.6271\n",
            "Epoch 80/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.5983\n",
            "Epoch 81/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5908\n",
            "Epoch 82/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5983\n",
            "Epoch 83/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5785\n",
            "Epoch 84/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5504\n",
            "Epoch 85/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.5473\n",
            "Epoch 86/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5304\n",
            "Epoch 87/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5727\n",
            "Epoch 88/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6403\n",
            "Epoch 89/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5835\n",
            "Epoch 90/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6414\n",
            "Epoch 91/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.6294\n",
            "Epoch 92/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.7509\n",
            "Epoch 93/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.6456\n",
            "Epoch 94/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.6116\n",
            "Epoch 95/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5513\n",
            "Epoch 96/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.4986\n",
            "Epoch 97/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.4905\n",
            "Epoch 98/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.5111\n",
            "Epoch 99/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5131\n",
            "Epoch 100/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.4873\n",
            "Epoch 101/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.4682\n",
            "Epoch 102/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.4607\n",
            "Epoch 103/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.4673\n",
            "Epoch 104/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.4723\n",
            "Epoch 105/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.4912\n",
            "Epoch 106/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.5598\n",
            "Epoch 107/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5124\n",
            "Epoch 108/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4568\n",
            "Epoch 109/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.4464\n",
            "Epoch 110/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.5903\n",
            "Epoch 111/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.6706\n",
            "Epoch 112/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4940\n",
            "Epoch 113/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.4709\n",
            "Epoch 114/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.4605\n",
            "Epoch 115/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.4564\n",
            "Epoch 116/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4520\n",
            "Epoch 117/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4408\n",
            "Epoch 118/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.4284\n",
            "Epoch 119/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.4206\n",
            "Epoch 120/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.4134\n",
            "Epoch 121/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.4073\n",
            "Epoch 122/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4056\n",
            "Epoch 123/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.4002\n",
            "Epoch 124/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.3948\n",
            "Epoch 125/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.3907\n",
            "Epoch 126/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.3841\n",
            "Epoch 127/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.3773\n",
            "Epoch 128/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.3752\n",
            "Epoch 129/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3659\n",
            "Epoch 130/2000\n",
            "15746/15746 [==============================] - 0s 29us/step - loss: 0.3596\n",
            "Epoch 131/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.3548\n",
            "Epoch 132/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3510\n",
            "Epoch 133/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3474\n",
            "Epoch 134/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.3448\n",
            "Epoch 135/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.3422\n",
            "Epoch 136/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.3407\n",
            "Epoch 137/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3401\n",
            "Epoch 138/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.3548\n",
            "Epoch 139/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.3385\n",
            "Epoch 140/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.3241\n",
            "Epoch 141/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.3181\n",
            "Epoch 142/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.3122\n",
            "Epoch 143/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3058\n",
            "Epoch 144/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2991\n",
            "Epoch 145/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2930\n",
            "Epoch 146/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2909\n",
            "Epoch 147/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2869\n",
            "Epoch 148/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2828\n",
            "Epoch 149/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2799\n",
            "Epoch 150/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2758\n",
            "Epoch 151/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2744\n",
            "Epoch 152/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2740\n",
            "Epoch 153/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2731\n",
            "Epoch 154/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2725\n",
            "Epoch 155/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2721\n",
            "Epoch 156/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2698\n",
            "Epoch 157/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2682\n",
            "Epoch 158/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2665\n",
            "Epoch 159/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2671\n",
            "Epoch 160/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2673\n",
            "Epoch 161/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2647\n",
            "Epoch 162/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2639\n",
            "Epoch 163/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2624\n",
            "Epoch 164/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2720\n",
            "Epoch 165/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2800\n",
            "Epoch 166/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2710\n",
            "Epoch 167/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2621\n",
            "Epoch 168/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2583\n",
            "Epoch 169/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2569\n",
            "Epoch 170/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2550\n",
            "Epoch 171/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2539\n",
            "Epoch 172/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2524\n",
            "Epoch 173/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2518\n",
            "Epoch 174/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2511\n",
            "Epoch 175/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2849\n",
            "Epoch 176/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.3643\n",
            "Epoch 177/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.3267\n",
            "Epoch 178/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2930\n",
            "Epoch 179/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2826\n",
            "Epoch 180/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2768\n",
            "Epoch 181/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2714\n",
            "Epoch 182/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2675\n",
            "Epoch 183/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2632\n",
            "Epoch 184/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2597\n",
            "Epoch 185/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2579\n",
            "Epoch 186/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2566\n",
            "Epoch 187/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2554\n",
            "Epoch 188/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.2544\n",
            "Epoch 189/2000\n",
            "15746/15746 [==============================] - 0s 19us/step - loss: 0.2533\n",
            "Epoch 190/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2525\n",
            "Epoch 191/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2510\n",
            "Epoch 192/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2611\n",
            "Epoch 193/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2668\n",
            "Epoch 194/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2576\n",
            "Epoch 195/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2716\n",
            "Epoch 196/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2682\n",
            "Epoch 197/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2579\n",
            "Epoch 198/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2533\n",
            "Epoch 199/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2515\n",
            "Epoch 200/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2497\n",
            "Epoch 201/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2477\n",
            "Epoch 202/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2472\n",
            "Epoch 203/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2467\n",
            "Epoch 204/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2446\n",
            "Epoch 205/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2428\n",
            "Epoch 206/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2417\n",
            "Epoch 207/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2407\n",
            "Epoch 208/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2395\n",
            "Epoch 209/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2397\n",
            "Epoch 210/2000\n",
            "15746/15746 [==============================] - 1s 36us/step - loss: 0.2394\n",
            "Epoch 211/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2377\n",
            "Epoch 212/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2363\n",
            "Epoch 213/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2366\n",
            "Epoch 214/2000\n",
            "15746/15746 [==============================] - 0s 30us/step - loss: 0.2370\n",
            "Epoch 215/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2356\n",
            "Epoch 216/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2352\n",
            "Epoch 217/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2336\n",
            "Epoch 218/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2318\n",
            "Epoch 219/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2308\n",
            "Epoch 220/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2301\n",
            "Epoch 221/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2301\n",
            "Epoch 222/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2294\n",
            "Epoch 223/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2289\n",
            "Epoch 224/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2282\n",
            "Epoch 225/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2278\n",
            "Epoch 226/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2299\n",
            "Epoch 227/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2352\n",
            "Epoch 228/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.2361\n",
            "Epoch 229/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2362\n",
            "Epoch 230/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2519\n",
            "Epoch 231/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2479\n",
            "Epoch 232/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2432\n",
            "Epoch 233/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2414\n",
            "Epoch 234/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2413\n",
            "Epoch 235/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2417\n",
            "Epoch 236/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2401\n",
            "Epoch 237/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2379\n",
            "Epoch 238/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2363\n",
            "Epoch 239/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2359\n",
            "Epoch 240/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2338\n",
            "Epoch 241/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2299\n",
            "Epoch 242/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2277\n",
            "Epoch 243/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2257\n",
            "Epoch 244/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2239\n",
            "Epoch 245/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2230\n",
            "Epoch 246/2000\n",
            "15746/15746 [==============================] - 0s 29us/step - loss: 0.2223\n",
            "Epoch 247/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2209\n",
            "Epoch 248/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2203\n",
            "Epoch 249/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2194\n",
            "Epoch 250/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2177\n",
            "Epoch 251/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2168\n",
            "Epoch 252/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2160\n",
            "Epoch 253/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2154\n",
            "Epoch 254/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2164\n",
            "Epoch 255/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2156\n",
            "Epoch 256/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2142\n",
            "Epoch 257/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.2127\n",
            "Epoch 258/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2126\n",
            "Epoch 259/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2153\n",
            "Epoch 260/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2129\n",
            "Epoch 261/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2136\n",
            "Epoch 262/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2112\n",
            "Epoch 263/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2113\n",
            "Epoch 264/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2105\n",
            "Epoch 265/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2093\n",
            "Epoch 266/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2080\n",
            "Epoch 267/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2071\n",
            "Epoch 268/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2079\n",
            "Epoch 269/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2098\n",
            "Epoch 270/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.2079\n",
            "Epoch 271/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2679\n",
            "Epoch 272/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.3227\n",
            "Epoch 273/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.3006\n",
            "Epoch 274/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2650\n",
            "Epoch 275/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2504\n",
            "Epoch 276/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2416\n",
            "Epoch 277/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2325\n",
            "Epoch 278/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2263\n",
            "Epoch 279/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2211\n",
            "Epoch 280/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2162\n",
            "Epoch 281/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2149\n",
            "Epoch 282/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.5590\n",
            "Epoch 283/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3990\n",
            "Epoch 284/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.4610\n",
            "Epoch 285/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.3749\n",
            "Epoch 286/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.3336\n",
            "Epoch 287/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2941\n",
            "Epoch 288/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2786\n",
            "Epoch 289/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2807\n",
            "Epoch 290/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2782\n",
            "Epoch 291/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.3028\n",
            "Epoch 292/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.3098\n",
            "Epoch 293/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2873\n",
            "Epoch 294/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2784\n",
            "Epoch 295/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2806\n",
            "Epoch 296/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2750: 0s - loss: 0.\n",
            "Epoch 297/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2689\n",
            "Epoch 298/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2662\n",
            "Epoch 299/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2626\n",
            "Epoch 300/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2596\n",
            "Epoch 301/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2556\n",
            "Epoch 302/2000\n",
            "15746/15746 [==============================] - 0s 20us/step - loss: 0.2517\n",
            "Epoch 303/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2488\n",
            "Epoch 304/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2455\n",
            "Epoch 305/2000\n",
            "15746/15746 [==============================] - 0s 22us/step - loss: 0.2407\n",
            "Epoch 306/2000\n",
            "15746/15746 [==============================] - 0s 28us/step - loss: 0.2413\n",
            "Epoch 307/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2364\n",
            "Epoch 308/2000\n",
            "15746/15746 [==============================] - 0s 27us/step - loss: 0.2341\n",
            "Epoch 309/2000\n",
            "15746/15746 [==============================] - 0s 24us/step - loss: 0.2602\n",
            "Epoch 310/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2753\n",
            "Epoch 311/2000\n",
            "15746/15746 [==============================] - 0s 26us/step - loss: 0.2618\n",
            "Epoch 312/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2478\n",
            "Epoch 313/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2353\n",
            "Epoch 314/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2281\n",
            "Epoch 315/2000\n",
            "15746/15746 [==============================] - 0s 23us/step - loss: 0.2245\n",
            "Epoch 316/2000\n",
            "15746/15746 [==============================] - 0s 21us/step - loss: 0.2223\n",
            "Epoch 317/2000\n",
            "15746/15746 [==============================] - 0s 25us/step - loss: 0.2196\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00317: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x169f9cc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ug1RbjLEDumM",
        "colab": {}
      },
      "source": [
        "hidden_units=5\n",
        "l1_reg=0\n",
        "reg_model = Sequential()\n",
        "#reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kwHOAJzXHkLS",
        "colab": {}
      },
      "source": [
        "hidden_units=10\n",
        "l1_reg=0\n",
        "reg_model2 = Sequential()\n",
        "reg_model2.add(AlphaRNN(hidden_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model2.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model2.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--Bxweay8vNB",
        "colab": {},
        "outputId": "ddcbeabd-e5d0-4fd7-904e-3f9034d14acd"
      },
      "source": [
        "print(rs.layers[0].get_weights())\n",
        "#print(alpharnnt.layers[1].get_weights())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8e7f3978e54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(alpharnnt.layers[1].get_weights())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A-W-Lh2jhGOg",
        "outputId": "37f0148b-48fe-4e32-d983-dd828751f6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "print(sigmoid(2.3005326))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9089211390818948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JcDxpnr4vbfp",
        "outputId": "1e2f645a-cf5a-49bb-dc25-bdd5abb9591b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "gru_pred_train = gru.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 1s 92us/step\n",
            "82837/82837 [==============================] - 4s 46us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHUBCY2a6fOH",
        "outputId": "0f28b83e-b6f7-4107-fffe-dd0e8db775b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "rs_pred_train = rs.predict(x_train_reg, verbose=1)\n",
        "rs_pred_test = rs.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3cd46843bd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrs_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrs_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ZX7Q_AfT5N",
        "colab_type": "code",
        "colab": {},
        "outputId": "fcab11a9-482e-4600-a59c-dd190df02ce9"
      },
      "source": [
        "rs_pred_train = alphars.predict(x_train_reg, verbose=1)\n",
        "rs_pred_test = alphars.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 2s 138us/step\n",
            "82837/82837 [==============================] - 4s 48us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8xtr4_utp5_i",
        "outputId": "8951414d-7b2c-44f6-eae9-58b4adbba9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "rnn_pred_train = rnn.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 0s 19us/step\n",
            "82837/82837 [==============================] - 2s 18us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oE90OgzpcWB_",
        "outputId": "201c6014-bc0c-42fb-a331-07574cfdd006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "alpharnn_pred_train = alpharnn.predict(x_train_reg, verbose=1)\n",
        "alpharnn_pred_test = alpharnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 1s 61us/step\n",
            "82837/82837 [==============================] - 2s 24us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4-ohBuNfT5f",
        "colab_type": "code",
        "colab": {},
        "outputId": "b8dbe3fd-aecf-4194-f6e4-a291740c9b35"
      },
      "source": [
        "alpharnnt_pred_train = alpharnnt.predict(x_train_reg, verbose=1)\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 1s 93us/step\n",
            "82837/82837 [==============================] - 3s 39us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dpUFvv3TFKJW",
        "colab": {}
      },
      "source": [
        "#rnn_model = RNN_model2(5,0)\n",
        "alpharnn_fit = reg_model2.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "alpharnn_pred_test = reg_model2.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],alpharnn_pred_test[:,0])\n",
        "print(\"Alpha RNN test data mse = \" + str(mse))\n",
        "print(\"Alpha RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "reg_model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snIH77i4EEVq",
        "outputId": "db26620e-6300-457c-a21c-5e63e3d09484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "alphars_pred_train = alphars.predict(x_train_reg, verbose=1)\n",
        "alphars_pred_test = alphars.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15746/15746 [==============================] - 1s 51us/step\n",
            "82837/82837 [==============================] - 4s 48us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cx-acE-FFoGr",
        "colab": {}
      },
      "source": [
        "#rnn_model = RNN_model2(5,0)\n",
        "alpharnnt_fit = alpharnnt.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],alpharnnt_pred_test[:,0])\n",
        "print(\"Alpha_t RNN test data mse = \" + str(mse))\n",
        "print(\"Alpha_t RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "alpharnnt.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9kEejnbMGSJ4",
        "outputId": "3e097cd8-53a5-4202-b698-be455134c171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], alpharnnt_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], alpharnnt_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000490460512147073\n",
            "0.00044399483148641327\n",
            "0.022146343087450646\n",
            "0.021071184861948634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZ-HA-Q2cjDs",
        "outputId": "f5278549-59ee-4452-9c4c-234f43ded636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], alpharnn_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], alpharnn_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)\n",
        "alpharnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20037716908407915\n",
            "0.159784331298265\n",
            "0.44763508473317765\n",
            "0.3997303232158714\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "alpha_rnn_1 (AlphaRNN)       (None, 10)                121       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 132\n",
            "Trainable params: 132\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTIKiBet463a"
      },
      "source": [
        "## Time series cross-val\n",
        "\n",
        "Just use val_loss for model selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5t9_3cL5-kh5",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sQZXa3l1ByQa"
      },
      "source": [
        "![alt text](https://)Pick the model with the lowest val loss sum over folds. you can *not* use the test set for model selection! This would be cheating\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBuK_PhPXe5r",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoGO-YMJXhEX",
        "outputId": "72d56184-7fa1-4520-ba83-c6695401cc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# AlphaRNN: Pick the model with the lowest val_loss. Retrain it on all data and then perform prediction\n",
        "#val, idx = min((val, idx) for (idx, val) in enumerate(val_losses))\n",
        "#print(hidden_sizes[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yCKK_cJwc98C"
      },
      "source": [
        "### Train on all training data with best model and predict on test set (no rolling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWH0sGENEfAS",
        "colab": {}
      },
      "source": [
        "#session = tf.Session()\n",
        "#alpharnn = simpleRNN(1, 5) #simpleAlphaRNN(1, 5) GRU(1,5)\n",
        "#model,_=train(session, alpharnn, x_train_reg, x_test_reg, y_train_reg, y_test_reg,max_epochs=10000) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ui7qu93eXWxw",
        "outputId": "04169566-9830-425a-c1c1-efce5b66b632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "session = tf.Session()\n",
        "alpharnn_t = alphaRNN(input_dimensions, hidden_sizes[idx])\n",
        "model,_=train(session, alpharnn_t, x_train_reg, x_test_reg, y_train_reg, y_test_reg, max_epochs=5000) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-cd645fff6442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malpharnn_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphaRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpharnn_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'alphaRNN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bgxFz0a7E10q",
        "outputId": "ed8bb3be-f6ad-4f71-d9bd-90e6008ff0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "plt.plot(rs_pred_train[100:200], color='r', label='Alpha-RNN')\n",
        "plt.plot(rnn_pred_train[100:200], color='g', label='RNN')\n",
        "plt.plot(gru_pred_train[100:200], color='y', label='GRU')\n",
        "plt.plot(y_train_reg.flatten()[100:200],'b', label='Actual')\n",
        "plt.legend(loc=0)\n",
        "plt.title('Actual vs Predicted AlphaRNN_t')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAGuCAYAAABrxzvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FLQR5ROqDWgE",
        "outputId": "a2bcb442-ad56-4c63-9caf-627b89ccc167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "plt.plot(rs_pred_test[100:200], color='r', label='Alpha-RNN')\n",
        "plt.plot(rnn_pred_test[100:200], color='g', label='RNN')\n",
        "plt.plot(gru_pred_test[100:200], color='y', label='GRU')\n",
        "plt.plot(y_test_reg.flatten()[100:200],'b', label='Actual')\n",
        "plt.legend(loc=0)\n",
        "plt.title('Actual vs Predicted AlphaRNN_t')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAGuCAYAAABrxzvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTnoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BIZFqE9GlcR5",
        "outputId": "b81abbde-76f0-478d-e0de-cc4bdaee6866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "mse = mean_squared_error(y_test_reg[:,0],rnn_pred_test)\n",
        "print(mse)\n",
        "mse = mean_squared_error(y_test_reg[:,0],rs_pred_test)\n",
        "print(mse)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test)\n",
        "print(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.006125738\n",
            "0.0037169938\n",
            "0.004285496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gVXru1dJApZF",
        "outputId": "703fa91d-8f25-406a-de0b-f5cc70077600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mse_std = np.math.sqrt(mse)\n",
        "print(mse_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022216155077072364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaNbGTp4QgUP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7UbO-ooTQoz3",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(model.output, feed_dict={model.input_layer: x_test_reg})\n",
        "y_predicted_ar=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_ar[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMdbVYYdQvEo",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(model.output, feed_dict={model.input_layer: x_train_reg})\n",
        "y_predicted_train=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_train[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MVau9K7Q5sl",
        "outputId": "0bf13883-5fc2-423b-bb7a-81cb7e2c8549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\n",
        "se = mean_squared_error(y_test_reg[:,0],y_predicted_ar)\n",
        "print(mse)\n",
        "mse_std = np.math.sqrt(mse)\n",
        "print(mse_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00042334075261903065\n",
            "0.02057524611320678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VLXOjS8PVXe3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JEw4AWqxcjfF"
      },
      "source": [
        "#Comparison with Keras models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Or6iq32awi0e",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7ooyKI-wlwo",
        "colab": {}
      },
      "source": [
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0Bjom9wpT2",
        "colab": {}
      },
      "source": [
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LeXlBJgwtdy",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FImnMFUowv1P",
        "colab": {}
      },
      "source": [
        "def RNN_model2(n_units=10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(SimpleRNN(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def GRU_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(GRU(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def LSTM_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(LSTM(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    #LPNorm.build_loss(p = float('inf'))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def AlphaRNN(hidden_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  #reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNI_srXYw1yf",
        "colab": {}
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for RNN...\")\n",
        "model = KerasRegressor(build_fn=RNN_model2, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_rnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']\n",
        "#n_units = [10, 20, 30, 40, 50, 60, 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XhMb6Muy5pTg",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_omBRtqnMRS4",
        "colab": {}
      },
      "source": [
        "rnn_model = RNN_model2(nodes_rnn,0)\n",
        "rnn_fit = rnn_model.fit(x_train_reg,y_train_reg, epochs = 2000, batch_size=500, callbacks=[es])\n",
        "#rnn_fit = rnn_model.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=100, callbacks=[es])\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],rnn_pred_test[:,0])\n",
        "print(\"RNN test data mse = \" + str(mse))\n",
        "print(\"RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dq9nDKwwbyAR",
        "outputId": "a9272408-b394-486e-af2c-66ba225ebc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained RNN\n",
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 30us/step\n",
            "374/374 [==============================] - 0s 35us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DEMPQfeVb4Xt",
        "colab": {},
        "outputId": "466ec169-3d70-48e4-a93e-4807b29e4876"
      },
      "source": [
        "# calculate mean squared error of the plain RNN\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], rnn_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], rnn_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)\n",
        "rnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.19906765775208535\n",
            "0.1559363060075342\n",
            "0.4461699875070995\n",
            "0.394887713163545\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 10)                120       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 131\n",
            "Trainable params: 131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LDbGyuU5YamG",
        "colab": {}
      },
      "source": [
        "model = KerasRegressor(build_fn=LSTM_model2, epochs=2000, batch_size=100, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_lstm = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F64LeCexTn7I",
        "colab": {}
      },
      "source": [
        "lstm_model = LSTM_model2(nodes_lstm,0)\n",
        "lstm_fit = lstm_model.fit(x_train_reg,y_train_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "#score = cross_val_score(y_test_reg[:,0],g\n",
        "lstm_model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lR_ze7BcavuU",
        "colab": {}
      },
      "source": [
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "#score = cross_val_score(y_test_reg[:,0],g\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GPjEQGIaac6Q",
        "colab": {}
      },
      "source": [
        "# make predictions with the trained LSTM\n",
        "lstm_pred_train = lstm_model.predict(x_train_reg, verbose=1)\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WbYi-p7Objfe",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], lstm_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:],lstm_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1IRswaScYqi9",
        "colab": {}
      },
      "source": [
        "print(\"Hyper parameter tuning of GRU model\")\n",
        "model = KerasRegressor(build_fn=GRU_model2, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_gru = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7Od0Dpe-rPq",
        "colab": {}
      },
      "source": [
        "gru_model = GRU_model2(nodes_gru,0)\n",
        "gru_fit = gru_model.fit(x_train_reg,y_train_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "gru_pred_test = gru_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test[:,0])\n",
        "print(\"GRU test data mse = \" + str(mse))\n",
        "print(\"GRU test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VbtK2zhi63mp",
        "colab": {}
      },
      "source": [
        "# MFD: Also try GRU, SimpleRNN\n",
        "# Need to cross-validate for the best number of hidden units (i.e. hidden_size)\n",
        "#gru_model = Sequential()\n",
        "#gru_model.add(GRU(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "#gru_model.add(Dense(1))\n",
        "#gru_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "#gru_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-XAiOFZjNq",
        "outputId": "4c42a5b3-2f1d-41c2-be50-5ee5120038dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#gru_fit = gru_model.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=100, callbacks=[es])\n",
        "gru_pred_test = gru_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test[:,0])\n",
        "print(\"GRU test data mse = \" + str(mse))\n",
        "print(\"GRU test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "374/374 [==============================] - 0s 99us/step\n",
            "GRU test data mse = 0.00038023759714453945\n",
            "GRU test std mse =  0.01949968197547179\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  (None, 5)                 105       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 111\n",
            "Trainable params: 111\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bo6NSAo5aFwp",
        "colab": {}
      },
      "source": [
        "gru_pred_train = gru_model.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BjClxCUlZ0WN",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], gru_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], gru_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GHA7NNMy-tag",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "hidden_size = 5\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "rnn_model.add(Dense(1))\n",
        "rnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "rnn_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUCECDU8JEwv",
        "outputId": "0d3d248a-d955-4bc2-f15e-c79407634a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 5)                 35        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 41\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h70UAC4v-y7Z",
        "colab": {}
      },
      "source": [
        "hidden_size = 5\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "lstm_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=500, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bvcs-lveLFPU",
        "outputId": "5144b923-0caf-49a1-9f93-c9742209ceb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse = \" + str(np.sqrt(mse)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "374/374 [==============================] - 0s 87us/step\n",
            "LSTM test data mse = 0.0005170304470553611\n",
            "LSTM test std mse = 0.02273830352192883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lege7PPLInX2",
        "outputId": "d22b31a9-3d89-45e4-834e-7d48f40a6d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 5)                 140       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 146\n",
            "Trainable params: 146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16ZWjzOxxKIJ",
        "outputId": "4752e538-de5f-4d52-9949-e2b3928d7b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained LSTM\n",
        "lstm_pred_train = lstm_model.predict(x_train_reg, verbose=1)\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 67us/step\n",
            "374/374 [==============================] - 0s 95us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Imz5JXTnxS1y",
        "outputId": "bcf1963c-724b-4697-e9f9-3a94d1f67b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# make predictions with the trained RNN\n",
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 45us/step\n",
            "374/374 [==============================] - 0s 37us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJgsZCf2xVGl",
        "outputId": "116773e6-fe69-4303-d0f7-7d3aa64ea1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "gru_pred_train = gru_model.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 49us/step\n",
            "374/374 [==============================] - 0s 63us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7mbWnLlQG_0z",
        "outputId": "b0e4cfab-942e-4dbb-c92a-b5bcb87a2fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "alpharnnt_pred_train = alpharnnt.predict(x_train_reg, verbose=1)\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 46us/step\n",
            "374/374 [==============================] - 0s 47us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b2RrHtFrQ6zl",
        "outputId": "7f9520c0-ba6b-4af3-ebfc-e4de234bddfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "alpharnn_pred_train = alpharnn.predict(x_train_reg, verbose=1)\n",
        "alpharnn_pred_test = alpharnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 40us/step\n",
            "374/374 [==============================] - 0s 42us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRUzGfCRfT7R",
        "colab_type": "code",
        "colab": {},
        "outputId": "d7b1c054-9a0a-46ca-9853-f53e8feca4ee"
      },
      "source": [
        "alphars_pred_train = alphars.predict(x_train_reg, verbose=1)\n",
        "alphars_pred_test = alphars.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3192/3192 [==============================] - 0s 54us/step\n",
            "769/769 [==============================] - 0s 59us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRQEeO42DpVv",
        "outputId": "3a802909-62f1-4898-dfb1-a80252c505ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the plain RNN\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], rnn_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], rnn_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)\n",
        "rnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1399077543799677\n",
            "0.121081112246449\n",
            "0.37404244997054503\n",
            "0.3479671137427343\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 10)                120       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 131\n",
            "Trainable params: 131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7fHAc8zHDyFZ",
        "outputId": "e553f8dd-3c84-4f8b-8ad3-fbb6e4fe34b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], gru_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], gru_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)\n",
        "gru.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07424276507393676\n",
            "0.07905559690758904\n",
            "0.272475255892965\n",
            "0.28116827151652274\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_1 (GRU)                  (None, 10)                360       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 371\n",
            "Trainable params: 371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKcHJU79D_K7",
        "outputId": "77a31dbe-5ee6-4d3d-a08d-3fdc0df87409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], lstm_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:],lstm_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0004928369560054375\n",
            "0.0004456200560310017\n",
            "0.022199931441458046\n",
            "0.02110971473116114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Igbsz8AD4vi",
        "outputId": "d311ae03-0803-4a5c-f354-eb31131fbb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], alpharnnt_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], alpharnnt_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)\n",
        "alpharnnt.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.05734081049281611\n",
            "0.07950597884939051\n",
            "0.23945941303865279\n",
            "0.2819680457948923\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "alphat_rnn_1 (AlphatRNN)     (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 271\n",
            "Trainable params: 271\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mkE8rbfTVcMC",
        "outputId": "87597ff9-e1f8-4330-983b-eb6cb5eda78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], alpharnn_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], alpharnn_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)\n",
        "alpharnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10049545540500747\n",
            "0.09992646008412251\n",
            "0.31701018186330776\n",
            "0.31611146781495053\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "alpha_rnn_1 (AlphaRNN)       (None, 10)                121       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 132\n",
            "Trainable params: 132\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTmiA4KyfT7b",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f8b604e-3669-4f9c-85fc-1c4a9bc3131b"
      },
      "source": [
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:], alphars_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:], alphars_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)\n",
        "alphars.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20703221177751924\n",
            "0.18144382208230647\n",
            "0.45500792496122444\n",
            "0.4259622308166611\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rs_4 (RS)                    (None, 10)                381       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 392\n",
            "Trainable params: 392\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NyCR4Z3W-__",
        "outputId": "314f7e44-08e3-4b20-cf31-cfb0e38b2d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "#MSE_train = mean_squared_error(df_train[use_feature][n_steps:], y_predicted_train_t)  #train_losses[:, 0])\n",
        "#print(MSE_train)\n",
        "#MSE_test = mean_squared_error(df_test[use_feature][n_steps:], y_predicted_ar_t)     #validation_losses[:, 0])\n",
        "#print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4618680691666422\n",
            "0.40977381730016493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t6SkTDvIxXcV",
        "outputId": "117884d6-f2bf-4104-d266-d89d919c7333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "train_line_real = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], df_train[use_feature][n_steps+n_steps_ahead-1:], color=\"black\", label=\"Observed (Training)\")\n",
        "#train_line_pred = plt.plot(df_train.index[n_steps:], lstm_pred_train[:, 0], color=\"red\", label=\"LSTM Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], rnn_pred_train[:, 0], color=\"blue\", label=\"RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], gru_pred_train[:, 0], color=\"orange\", label=\"GRU Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], alpharnn_pred_train[:,0], color='green', label=\"alpha RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], alphars_pred_train[:,0], color=\"yellow\", label=\"alpha RS RNN Predict (Training)\" )\n",
        "train_line_pred = plt.plot(df_train.index[n_steps+n_steps_ahead-1:], alpharnnt_pred_train[:,0], color=\"red\", label=\"alpha_t RNN Predict (Training)\") \n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Training)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], df_test[use_feature][n_steps+n_steps_ahead-1:], color=\"black\", label=\"Observed (Testing)\")\n",
        "#test_line_pred = plt.plot(df_test.index[n_steps:], lstm_pred_test[:, 0], color=\"red\", label=\"LSTM Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], rnn_pred_test[:, 0], color=\"blue\", label=\"RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], gru_pred_test[:, 0], color=\"orange\", label=\"GRU Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], alpharnn_pred_test,color=\"green\", label=\"alpha RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], alphars_pred_test, color=\"yellow\", label= \"alpha RS RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps+n_steps_ahead-1:], alpharnnt_pred_test[:,0], color=\"red\", label=\"alpha_t RNN Predict (Training)\")\n",
        "# train_line_pred = plt.plot(session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTnrSPPfT7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}