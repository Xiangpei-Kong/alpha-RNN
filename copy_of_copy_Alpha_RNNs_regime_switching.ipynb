{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Alpha_RNNs_regime_switching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/copy_of_copy_Alpha_RNNs_regime_switching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDLTylUvNgsc",
        "colab_type": "code",
        "outputId": "36227712-1bd3-4ab9-c945-2827929c2ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7WLRTI-WuTTU",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0xKEzT7ITiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def heaviside(x):\n",
        "    if x>=0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9BywHfc63kb",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "#from alphaRNN import *\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "import keras.layers\n",
        "#from alphaRNN import *\n",
        "from keras import *\n",
        "from keras.legacy import interfaces\n",
        "\n",
        "\t\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "# To make this notebook's output stable across runs\n",
        "#def reset_graph(seed=42):\n",
        "#    tf.reset_default_graph()\n",
        "#    tf.set_random_seed(seed)\n",
        "#    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYzhUo7CuZLa",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, eps=0.01):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    step_length=np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      mu[i*step_length:((i*step_length)+1)]=step_size*(-1)**i\n",
        "     \n",
        "    for i in range(1, length):\n",
        "        sigma[i]=sigma[i-1] + mu[i] + eps*np.random.normal(0,1)\n",
        "        \n",
        "    return sigma   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DuOojJHWukx-",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JztXCwy5upi7",
        "colab": {}
      },
      "source": [
        "p = 30 # the number of lags (in both the data and the models)\n",
        "vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "untInWSMuxSb",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(vols, columns=['vol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7wGqBI6u0M6",
        "outputId": "7d29bb78-3418-4b5b-cc80-1f6b6ebac3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "use_features = ['vol'] \n",
        "target = 'vol'\n",
        "n_steps = 30 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(len(df)*train_weight)\n",
        "\n",
        "df_train = df[use_features].iloc[:split]\n",
        "print(df_train)\n",
        "df_test = df[use_features].iloc[split:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           vol\n",
            "0     0.232166\n",
            "1     0.233981\n",
            "2     0.236069\n",
            "3     0.238135\n",
            "4     0.240495\n",
            "...        ...\n",
            "1571  0.149793\n",
            "1572  0.153066\n",
            "1573  0.156344\n",
            "1574  0.159496\n",
            "1575  0.163017\n",
            "\n",
            "[1576 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTCv6NRBvBa4",
        "colab": {}
      },
      "source": [
        "def get_lagged_features(value, n_steps,n_steps_ahead):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps+n_steps_ahead-1, n_steps_ahead-1, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE0sdbrNvM3R",
        "colab": {}
      },
      "source": [
        "n_steps_ahead=10\n",
        "\n",
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps, n_steps_ahead).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps,n_steps_ahead).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBwt67S4vQmu",
        "outputId": "642a6f5a-89a7-4525-8311-68835350a934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1537, 30, 1) (1537, 1, 1) (355, 30, 1) (355, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AFhK-lgavTuB",
        "colab": {}
      },
      "source": [
        "train_batch_size = y_train_reg.shape[0]\n",
        "test_batch_size = y_test_reg.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COZs3KSovWsf",
        "outputId": "ab0d40be-8116-4003-a285-fed33cd5c44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_batch_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GJxDq4vyvg-4",
        "outputId": "40e5746f-fda6-4444-80e4-ac6730321226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.get_collection('alpha_t')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGb4NRQaCEOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphaRNNCell(Layer):\n",
        "    \"\"\"Cell class for AlphaRNN.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        super(AlphaRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        #self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        \n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        prev_output = states[0]\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(prev_output),\n",
        "                self.recurrent_dropout,\n",
        "                training=training)\n",
        "\n",
        "        dp_mask = self._dropout_mask\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if dp_mask is not None:\n",
        "            h = K.dot(inputs * dp_mask, self.kernel)\n",
        "        else:\n",
        "            h = K.dot(inputs, self.kernel)\n",
        "        if self.bias is not None:\n",
        "            h = K.bias_add(h, self.bias)\n",
        "\n",
        "        if rec_dp_mask is not None:\n",
        "            prev_output *= rec_dp_mask\n",
        "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        output = K.sigmoid(self.alpha)* output + (1-K.sigmoid(self.alpha))* prev_output\n",
        "        # Properly set learning phase on output tensor.\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                output._uses_learning_phase = True\n",
        "        return output, [output]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkga_VrPW3th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class AlphaRNN(keras.layers.RNN):\n",
        "    \"\"\"Fully-connected AlphaRNN where the output is to be fed back to input.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if 'implementation' in kwargs:\n",
        "            kwargs.pop('implementation')\n",
        "            warnings.warn('The `implementation` argument '\n",
        "                          'in `SimpleRNN` has been deprecated. '\n",
        "                          'Please remove it from your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphaRNNCell(units,\n",
        "                             activation=activation,\n",
        "                             use_bias=use_bias,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             recurrent_initializer=recurrent_initializer,\n",
        "                             bias_initializer=bias_initializer,\n",
        "                             kernel_regularizer=kernel_regularizer,\n",
        "                             recurrent_regularizer=recurrent_regularizer,\n",
        "                             bias_regularizer=bias_regularizer,\n",
        "                             kernel_constraint=kernel_constraint,\n",
        "                             recurrent_constraint=recurrent_constraint,\n",
        "                             bias_constraint=bias_constraint,\n",
        "                             dropout=dropout,\n",
        "                             recurrent_dropout=recurrent_dropout)\n",
        "        super(AlphaRNN, self).__init__(cell,\n",
        "                                        return_sequences=return_sequences,\n",
        "                                        return_state=return_state,\n",
        "                                        go_backwards=go_backwards,\n",
        "                                        stateful=stateful,\n",
        "                                        unroll=unroll,\n",
        "                                        **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphaRNN, self).call(inputs,\n",
        "                                           mask=mask,\n",
        "                                           training=training,\n",
        "                                           initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config:\n",
        "            config.pop('implementation')\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmXMteFc_r8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphatRNNCell(Layer):\n",
        "    \"\"\"Cell class for the AlphatRNN layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 **kwargs):\n",
        "        super(AlphatRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 2),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias_shape = (2, 2*self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            \n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # alpha\n",
        "        self.kernel_alpha = self.kernel[:, :self.units]\n",
        "        self.recurrent_kernel_alpha = self.recurrent_kernel[:, :self.units]\n",
        "        # recurrnce\n",
        "        self.kernel_h = self.kernel[:, self.units:]\n",
        "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_alpha = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units:]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "            \n",
        "            self.recurrent_bias_alpha = self.recurrent_bias[:self.units]    \n",
        "            self.recurrent_bias_h = self.recurrent_bias[self.units:]\n",
        "        else:\n",
        "            self.input_bias_alpha = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_alpha = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_alpha = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_alpha = input\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_alpha = K.dot(inputs_alpha, self.kernel_alpha)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_alpha = K.bias_add(x_alpha, self.input_bias_alpha)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_alpha = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_alpha = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_alpha = K.dot(h_tm1_alpha, self.recurrent_kernel_alpha)\n",
        "           \n",
        "            if self.use_bias:\n",
        "                recurrent_alpha = K.bias_add(recurrent_alpha, self.recurrent_bias_alpha)\n",
        "\n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "           \n",
        "            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n",
        "            if self.use_bias:\n",
        "                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
        "            x_alpha = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            \n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                  matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "            \n",
        "            recurrent_alpha = matrix_inner[:, :self.units] \n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "            recurrent_h = matrix_inner[:, self.units: 2 * self.units]  \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        h = alpha * h_tm1 + (1 - alpha) * hh\n",
        "\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphatRNN(keras.layers.RNN):\n",
        "    \"\"\"Alpha_t RNN\n",
        "    There are two variants. The default one is based on 1406.1078v3 and\n",
        "    has reset gate applied to hidden state before matrix multiplication. The\n",
        "    other one is based on original 1406.1078v1 and has the order reversed.\n",
        "    The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
        "    inference on CPU. Thus it has separate biases for `kernel` and\n",
        "    `recurrent_kernel`. Use `'reset_after'=True` and\n",
        "    `recurrent_activation='sigmoid'`.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        \n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphatRNNCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation)             \n",
        "        super(AlphatRNN, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphatRNN, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk-9KznNsHQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RSCell(Layer):\n",
        "    \"\"\"Cell class for the RS layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        reset_after: GRU convention (whether to apply reset gate after or\n",
        "            before matrix multiplication). False = \"before\" (default),\n",
        "            True = \"after\" (CuDNN compatible).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 reset_after=False,\n",
        "                 **kwargs):\n",
        "        super(RSCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.reset_after = reset_after\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 3),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)    \n",
        "\n",
        "        if self.use_bias:\n",
        "            \n",
        "            bias_shape = (2, 3 * self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            # NOTE: need to flatten, since slicing in CNTK gives 2D array\n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # kernels\n",
        "        self.kernel_s = self.kernel[:, :self.units]\n",
        "        self.kernel_h = self.kernel[:, self.units: self.units * 2]\n",
        "        \n",
        "        # recurrent kernels\n",
        "        self.recurrent_kernel_s = self.recurrent_kernel[:, :self.units]\n",
        "        # h1\n",
        "        self.recurrent_kernel_h1 = self.recurrent_kernel[:,\n",
        "                                                        self.units:\n",
        "                                                        self.units * 2]\n",
        "        #h2\n",
        "        self.recurrent_kernel_h2 = self.recurrent_kernel[:, self.units * 2:]                                                \n",
        "        \n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_s = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units: self.units * 2]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "           \n",
        "            self.recurrent_bias_s = self.recurrent_bias[:self.units]\n",
        "            self.recurrent_bias_h1 = (\n",
        "            self.recurrent_bias[self.units: self.units * 2])\n",
        "            self.recurrent_bias_h2 = self.recurrent_bias[self.units * 2:]\n",
        "        else:\n",
        "            self.input_bias_s = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_s = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=3)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=3)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_s = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_s = inputs\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_s = K.dot(inputs_s, self.kernel_s)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_s = K.bias_add(x_s, self.input_bias_s)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_s = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_s = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_s = K.dot(h_tm1_s, self.recurrent_kernel_s)\n",
        "            recurrent_h1 = K.dot(h_tm1_h, self.recurrent_kernel_h1)\n",
        "            recurrent_h2 = K.dot(h_tm1_h, self.recurrent_kernel_h2)\n",
        "            if self.use_bias:\n",
        "                recurrent_s = K.bias_add(recurrent_s, self.recurrent_bias_s)\n",
        "                recurrent_h1 = K.bias_add(recurrent_h1, self.recurrent_bias_h1)\n",
        "                recurrent_h2 = K.bias_add(recurrent_h2, self.recurrent_bias_h2)\n",
        "\n",
        "            s = self.recurrent_activation(x_s + recurrent_s)\n",
        "            #s = heaviside(x_s + recurrent_s)\n",
        "            #h = self.recurrent_activation(x_h + recurrent_h)\n",
        "\n",
        "            # reset gate applied after/before matrix multiplication\n",
        "            #if self.reset_after:\n",
        "            #    recurrent_s = K.dot(h_tm1_s, self.recurrent_kernel_s)\n",
        "            #    if self.use_bias:\n",
        "            #        recurrent_s = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            #    recurrent_h = r * recurrent_h\n",
        "            #else:\n",
        "            recurrent_h = recurrent_h1*s + recurrent_h2*(1-s)\n",
        "\n",
        "            h = self.activation(x_h + recurrent_h)\n",
        "            h = K.sigmoid(self.alpha)* h + 1-K.sigmoid(self.alpha)*h_tm1\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias[:2*self.units])\n",
        "            x_s = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            #x_h = matrix_x[:, 2 * self.units:]\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            # hidden state projected by all gate matrices at once\n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "\n",
        "            recurrent_s = matrix_inner[:, :self.units]\n",
        "            recurrent_h1 = matrix_inner[:, self.units: 2 * self.units]\n",
        "            recurrent_h2 = matrix_inner[:, 2 * self.units:]\n",
        "\n",
        "            s = self.recurrent_activation(x_s + recurrent_s)\n",
        "\n",
        "            recurrent_h = recurrent_h1*s + recurrent_h2*(1-s)\n",
        "\n",
        "            h = self.activation(x_h + recurrent_h)\n",
        "            h = K.sigmoid(self.alpha)*h + (1-K.sigmoid(self.alpha))*h_tm1\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        #h = z * h_tm1 + (1 - z) * hh\n",
        "      \n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation,\n",
        "                  'reset_after': self.reset_after}\n",
        "        base_config = super(RSCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class RS(keras.layers.RNN):\n",
        "    \"\"\"RS \n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        reset_after: GRU convention (whether to apply reset gate after or\n",
        "            before matrix multiplication). False = \"before\" (default),\n",
        "            True = \"after\" (CuDNN compatible).\n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 reset_after=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = RSCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation,\n",
        "                       reset_after=reset_after)\n",
        "        super(RS, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(RS, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    @property\n",
        "    def reset_after(self):\n",
        "        return self.cell.reset_after\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation,\n",
        "                  'reset_after': self.reset_after}\n",
        "        base_config = super(GRU, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJkBQ3XDO0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "\n",
        "\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA0onJt1FY38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=500, min_delta=1e-9, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXZrHJ9IeYmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AlphaRNNt(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  #reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(AlphatRNN(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  \n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_--_hYDVNQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Alpha_Rnn(n_units = 10, l1_reg=0):\n",
        "  reg_model2 = Sequential()\n",
        "  reg_model2.add(AlphaRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model2.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #reg_model2.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV63hsm9pOyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Plain_Rnn(n_units = 10, l1_reg=0):\n",
        "  reg_model2 = Sequential()\n",
        "  reg_model2.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model2.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBmOg6KDtpr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GRU_(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LtjCtD_rdLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM_(n_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), recurrent_initializer=keras.initializers.normal(seed=0), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=0), bias_initializer=keras.initializers.normal(seed=0), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzMO95WR3c8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RS_(n_units = 10, l1_reg=0, seed=1):\n",
        "  reg_model = Sequential()\n",
        "  reg_model.add(RS(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.normal(seed=seed), bias_initializer=keras.initializers.normal(seed=seed), recurrent_initializer=keras.initializers.normal(seed=seed), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False, implementation=2))  #activation='tanh'\n",
        "  #reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer=keras.initializers.normal(seed=seed), bias_initializer=keras.initializers.normal(seed=seed), kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW6uZAlteu5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for AlphaRNNt...\")\n",
        "model = KerasRegressor(build_fn=AlphaRNNt, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnnt = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLYwCOwmfTMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#n_units = [1,2,5,10,20]\n",
        "#l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "#tscv = TimeSeriesSplit(n_splits = 5)\n",
        "#param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for AlphaRNN...\")\n",
        "model = KerasRegressor(build_fn=Alpha_Rnn, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_alpharnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s2tRB3U-A72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nodes_alpharnn=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZmpZFAFpZ9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train optimized model\n",
        "rnn = Plain_Rnn(nodes_alpharnn,0)\n",
        "rnn.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkNiwfvihDM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train optimized model\n",
        "alpharnn = Alpha_Rnn(nodes_alpharnn,0)\n",
        "alpharnn.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOnLqiyA7Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru = GRU_(nodes_alpharnn,0)\n",
        "gru.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oecaEqGFriLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = LSTM_(nodes_alpharnn,0)\n",
        "lstm.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8TGqr8m4QN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbc42f76-2171-468c-bdc1-f8f704e18f25"
      },
      "source": [
        "rs = RS_(nodes_alpharnn,0,777)\n",
        "rs.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es],shuffle=False)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1537/1537 [==============================] - 15s 10ms/step - loss: 0.0706\n",
            "Epoch 2/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0505\n",
            "Epoch 3/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0293\n",
            "Epoch 4/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0198\n",
            "Epoch 5/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0187\n",
            "Epoch 6/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0181\n",
            "Epoch 7/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0172\n",
            "Epoch 8/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0162\n",
            "Epoch 9/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0152\n",
            "Epoch 10/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0140\n",
            "Epoch 11/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0125\n",
            "Epoch 12/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0108\n",
            "Epoch 13/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0092\n",
            "Epoch 14/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0080\n",
            "Epoch 15/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0077\n",
            "Epoch 16/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0075\n",
            "Epoch 17/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0074\n",
            "Epoch 18/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0073\n",
            "Epoch 19/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0073\n",
            "Epoch 20/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0072\n",
            "Epoch 21/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0071\n",
            "Epoch 22/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0071\n",
            "Epoch 23/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0070\n",
            "Epoch 24/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0070\n",
            "Epoch 25/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0069\n",
            "Epoch 26/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0069\n",
            "Epoch 27/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0069\n",
            "Epoch 28/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0068\n",
            "Epoch 29/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0068\n",
            "Epoch 30/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0067\n",
            "Epoch 31/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0067\n",
            "Epoch 32/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0067\n",
            "Epoch 33/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0067\n",
            "Epoch 34/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0066\n",
            "Epoch 35/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0066\n",
            "Epoch 36/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0066\n",
            "Epoch 37/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0065\n",
            "Epoch 38/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0065\n",
            "Epoch 39/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0065\n",
            "Epoch 40/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0065\n",
            "Epoch 41/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0065\n",
            "Epoch 42/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0064\n",
            "Epoch 43/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0064\n",
            "Epoch 44/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0064\n",
            "Epoch 45/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0064\n",
            "Epoch 46/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0064\n",
            "Epoch 47/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0064\n",
            "Epoch 48/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0063\n",
            "Epoch 49/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0063\n",
            "Epoch 50/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0063\n",
            "Epoch 51/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0063\n",
            "Epoch 52/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0063\n",
            "Epoch 53/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0063\n",
            "Epoch 54/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0063\n",
            "Epoch 55/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0062\n",
            "Epoch 56/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0062\n",
            "Epoch 57/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0062\n",
            "Epoch 58/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0062\n",
            "Epoch 59/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0062\n",
            "Epoch 60/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0062\n",
            "Epoch 61/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0062\n",
            "Epoch 62/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0062\n",
            "Epoch 63/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0061\n",
            "Epoch 64/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0061\n",
            "Epoch 65/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0061\n",
            "Epoch 66/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0061\n",
            "Epoch 67/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0061\n",
            "Epoch 68/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0061\n",
            "Epoch 69/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0061\n",
            "Epoch 70/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0061\n",
            "Epoch 71/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0061\n",
            "Epoch 72/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0061\n",
            "Epoch 73/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0061\n",
            "Epoch 74/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0060\n",
            "Epoch 75/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0060\n",
            "Epoch 76/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0060\n",
            "Epoch 77/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0060\n",
            "Epoch 78/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0060\n",
            "Epoch 79/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0060\n",
            "Epoch 80/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0060\n",
            "Epoch 81/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0060\n",
            "Epoch 82/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0060\n",
            "Epoch 83/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0060\n",
            "Epoch 84/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0060\n",
            "Epoch 85/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0060\n",
            "Epoch 86/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0060\n",
            "Epoch 87/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0060\n",
            "Epoch 88/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0059\n",
            "Epoch 89/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0059\n",
            "Epoch 90/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0059\n",
            "Epoch 91/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0059\n",
            "Epoch 92/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0059\n",
            "Epoch 93/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0059\n",
            "Epoch 94/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0059\n",
            "Epoch 95/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0059\n",
            "Epoch 96/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0059\n",
            "Epoch 97/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0059\n",
            "Epoch 98/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0059\n",
            "Epoch 99/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0059\n",
            "Epoch 100/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0059\n",
            "Epoch 101/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0059\n",
            "Epoch 102/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0059\n",
            "Epoch 103/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0059\n",
            "Epoch 104/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0059\n",
            "Epoch 105/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0058\n",
            "Epoch 106/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0058\n",
            "Epoch 107/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0058\n",
            "Epoch 108/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0058\n",
            "Epoch 109/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0058\n",
            "Epoch 110/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0058\n",
            "Epoch 111/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0058\n",
            "Epoch 112/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0058\n",
            "Epoch 113/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0058\n",
            "Epoch 114/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0058\n",
            "Epoch 115/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0058\n",
            "Epoch 116/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0058\n",
            "Epoch 117/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0058\n",
            "Epoch 118/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0058\n",
            "Epoch 119/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0058\n",
            "Epoch 120/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0058\n",
            "Epoch 121/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0058\n",
            "Epoch 122/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0058\n",
            "Epoch 123/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0058\n",
            "Epoch 124/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0058\n",
            "Epoch 125/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0058\n",
            "Epoch 126/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0058\n",
            "Epoch 127/2000\n",
            "1537/1537 [==============================] - 0s 126us/step - loss: 0.0057\n",
            "Epoch 128/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0057\n",
            "Epoch 129/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0057\n",
            "Epoch 130/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0057\n",
            "Epoch 131/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0057\n",
            "Epoch 132/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0057\n",
            "Epoch 133/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0057\n",
            "Epoch 134/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0057\n",
            "Epoch 135/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0057\n",
            "Epoch 136/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0057\n",
            "Epoch 137/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0057\n",
            "Epoch 138/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0057\n",
            "Epoch 139/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0057\n",
            "Epoch 140/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0057\n",
            "Epoch 141/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0057\n",
            "Epoch 142/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0057\n",
            "Epoch 143/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0057\n",
            "Epoch 144/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0057\n",
            "Epoch 145/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0057\n",
            "Epoch 146/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0057\n",
            "Epoch 147/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0057\n",
            "Epoch 148/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0057\n",
            "Epoch 149/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0057\n",
            "Epoch 150/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0057\n",
            "Epoch 151/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0057\n",
            "Epoch 152/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0057\n",
            "Epoch 153/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0057\n",
            "Epoch 154/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0056\n",
            "Epoch 155/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 156/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0056\n",
            "Epoch 157/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 158/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0056\n",
            "Epoch 159/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 160/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0056\n",
            "Epoch 161/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0056\n",
            "Epoch 162/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0056\n",
            "Epoch 163/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0056\n",
            "Epoch 164/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0056\n",
            "Epoch 165/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 166/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0056\n",
            "Epoch 167/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 168/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0056\n",
            "Epoch 169/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0056\n",
            "Epoch 170/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0056\n",
            "Epoch 171/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0056\n",
            "Epoch 172/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0056\n",
            "Epoch 173/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0056\n",
            "Epoch 174/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0056\n",
            "Epoch 175/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0056\n",
            "Epoch 176/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 177/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0056\n",
            "Epoch 178/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0056\n",
            "Epoch 179/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 180/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0056\n",
            "Epoch 181/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0056\n",
            "Epoch 182/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0056\n",
            "Epoch 183/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0055\n",
            "Epoch 184/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0055\n",
            "Epoch 185/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0055\n",
            "Epoch 186/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0055\n",
            "Epoch 187/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0055\n",
            "Epoch 188/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0055\n",
            "Epoch 189/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0055\n",
            "Epoch 190/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0055\n",
            "Epoch 191/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0055\n",
            "Epoch 192/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0055\n",
            "Epoch 193/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0055\n",
            "Epoch 194/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0055\n",
            "Epoch 195/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0055\n",
            "Epoch 196/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0055\n",
            "Epoch 197/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0055\n",
            "Epoch 198/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0055\n",
            "Epoch 199/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0055\n",
            "Epoch 200/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0055\n",
            "Epoch 201/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0055\n",
            "Epoch 202/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0055\n",
            "Epoch 203/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0055\n",
            "Epoch 204/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0054\n",
            "Epoch 205/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0054\n",
            "Epoch 206/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0054\n",
            "Epoch 207/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0054\n",
            "Epoch 208/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0054\n",
            "Epoch 209/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0054\n",
            "Epoch 210/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0054\n",
            "Epoch 211/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0054\n",
            "Epoch 212/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0053\n",
            "Epoch 213/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0053\n",
            "Epoch 214/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0053\n",
            "Epoch 215/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0053\n",
            "Epoch 216/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0052\n",
            "Epoch 217/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0051\n",
            "Epoch 218/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0051\n",
            "Epoch 219/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0050\n",
            "Epoch 220/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0050\n",
            "Epoch 221/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0050\n",
            "Epoch 222/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0050\n",
            "Epoch 223/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0050\n",
            "Epoch 224/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0050\n",
            "Epoch 225/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0050\n",
            "Epoch 226/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0049\n",
            "Epoch 227/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0049\n",
            "Epoch 228/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0049\n",
            "Epoch 229/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0049\n",
            "Epoch 230/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0048\n",
            "Epoch 231/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0047\n",
            "Epoch 232/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0046\n",
            "Epoch 233/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0047\n",
            "Epoch 234/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0046\n",
            "Epoch 235/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0046\n",
            "Epoch 236/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0047\n",
            "Epoch 237/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0045\n",
            "Epoch 238/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0043\n",
            "Epoch 239/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0047\n",
            "Epoch 240/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0047\n",
            "Epoch 241/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0044\n",
            "Epoch 242/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0041\n",
            "Epoch 243/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0047\n",
            "Epoch 244/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0049\n",
            "Epoch 245/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0043\n",
            "Epoch 246/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0041\n",
            "Epoch 247/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0047\n",
            "Epoch 248/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0043\n",
            "Epoch 249/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0043\n",
            "Epoch 250/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0044\n",
            "Epoch 251/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0042\n",
            "Epoch 252/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0043\n",
            "Epoch 253/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0045\n",
            "Epoch 254/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0041\n",
            "Epoch 255/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0041\n",
            "Epoch 256/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0045\n",
            "Epoch 257/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0041\n",
            "Epoch 258/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0040\n",
            "Epoch 259/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0043\n",
            "Epoch 260/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0043\n",
            "Epoch 261/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0040\n",
            "Epoch 262/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0039\n",
            "Epoch 263/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0044\n",
            "Epoch 264/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0045\n",
            "Epoch 265/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0041\n",
            "Epoch 266/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0039\n",
            "Epoch 267/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0039\n",
            "Epoch 268/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0042\n",
            "Epoch 269/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0043\n",
            "Epoch 270/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0040\n",
            "Epoch 271/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0037\n",
            "Epoch 272/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0040\n",
            "Epoch 273/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0045\n",
            "Epoch 274/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0040\n",
            "Epoch 275/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0037\n",
            "Epoch 276/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0039\n",
            "Epoch 277/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0044\n",
            "Epoch 278/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0038\n",
            "Epoch 279/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0037\n",
            "Epoch 280/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0041\n",
            "Epoch 281/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0044\n",
            "Epoch 282/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0041\n",
            "Epoch 283/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0037\n",
            "Epoch 284/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0037\n",
            "Epoch 285/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0040\n",
            "Epoch 286/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0042\n",
            "Epoch 287/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0037\n",
            "Epoch 288/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0035\n",
            "Epoch 289/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0036\n",
            "Epoch 290/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0037\n",
            "Epoch 291/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0040\n",
            "Epoch 292/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0045\n",
            "Epoch 293/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0039\n",
            "Epoch 294/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0036\n",
            "Epoch 295/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0035\n",
            "Epoch 296/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0039\n",
            "Epoch 297/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0043\n",
            "Epoch 298/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0037\n",
            "Epoch 299/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0034\n",
            "Epoch 300/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0033\n",
            "Epoch 301/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0032\n",
            "Epoch 302/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0032\n",
            "Epoch 303/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0030\n",
            "Epoch 304/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0031\n",
            "Epoch 305/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0033\n",
            "Epoch 306/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0034\n",
            "Epoch 307/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0034\n",
            "Epoch 308/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0031\n",
            "Epoch 309/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0033\n",
            "Epoch 310/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0033\n",
            "Epoch 311/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0030\n",
            "Epoch 312/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0035\n",
            "Epoch 313/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0030\n",
            "Epoch 314/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0031\n",
            "Epoch 315/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0032\n",
            "Epoch 316/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0030\n",
            "Epoch 317/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0035\n",
            "Epoch 318/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0031\n",
            "Epoch 319/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0032\n",
            "Epoch 320/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0030\n",
            "Epoch 321/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0030\n",
            "Epoch 322/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0029\n",
            "Epoch 323/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0031\n",
            "Epoch 324/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0032\n",
            "Epoch 325/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0029\n",
            "Epoch 326/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0036\n",
            "Epoch 327/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0034\n",
            "Epoch 328/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0033\n",
            "Epoch 329/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0031\n",
            "Epoch 330/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0028\n",
            "Epoch 331/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0028\n",
            "Epoch 332/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0030\n",
            "Epoch 333/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0028\n",
            "Epoch 334/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0029\n",
            "Epoch 335/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0035\n",
            "Epoch 336/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0029\n",
            "Epoch 337/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0029\n",
            "Epoch 338/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0029\n",
            "Epoch 339/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0028\n",
            "Epoch 340/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0030\n",
            "Epoch 341/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0029\n",
            "Epoch 342/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0029\n",
            "Epoch 343/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0029\n",
            "Epoch 344/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0027\n",
            "Epoch 345/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0026\n",
            "Epoch 346/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0026\n",
            "Epoch 347/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0027\n",
            "Epoch 348/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0027\n",
            "Epoch 349/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0030\n",
            "Epoch 350/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0028\n",
            "Epoch 351/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0031\n",
            "Epoch 352/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0030\n",
            "Epoch 353/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0030\n",
            "Epoch 354/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0029\n",
            "Epoch 355/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0028\n",
            "Epoch 356/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0028\n",
            "Epoch 357/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0029\n",
            "Epoch 358/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0028\n",
            "Epoch 359/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0030\n",
            "Epoch 360/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0029\n",
            "Epoch 361/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0027\n",
            "Epoch 362/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0033\n",
            "Epoch 363/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 364/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0026\n",
            "Epoch 365/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0025\n",
            "Epoch 366/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0026\n",
            "Epoch 367/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0027\n",
            "Epoch 368/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0034\n",
            "Epoch 369/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0026\n",
            "Epoch 370/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0026\n",
            "Epoch 371/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0028\n",
            "Epoch 372/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0029\n",
            "Epoch 373/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0032\n",
            "Epoch 374/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0030\n",
            "Epoch 375/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0034\n",
            "Epoch 376/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0027\n",
            "Epoch 377/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 378/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0025\n",
            "Epoch 379/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 380/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0027\n",
            "Epoch 381/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0029\n",
            "Epoch 382/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0025\n",
            "Epoch 383/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 384/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0028\n",
            "Epoch 385/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0028\n",
            "Epoch 386/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 387/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0029\n",
            "Epoch 388/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 389/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0032\n",
            "Epoch 390/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 391/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0027\n",
            "Epoch 392/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 393/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 394/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0026\n",
            "Epoch 395/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 396/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 397/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0026\n",
            "Epoch 398/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 399/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0032\n",
            "Epoch 400/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0035\n",
            "Epoch 401/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0031\n",
            "Epoch 402/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0030\n",
            "Epoch 403/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 404/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0026\n",
            "Epoch 405/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0030\n",
            "Epoch 406/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0034\n",
            "Epoch 407/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0029\n",
            "Epoch 408/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 409/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 410/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 411/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 412/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 413/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0025\n",
            "Epoch 414/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 415/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 416/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 417/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0025\n",
            "Epoch 418/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 419/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0032\n",
            "Epoch 420/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0034\n",
            "Epoch 421/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0045\n",
            "Epoch 422/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0059\n",
            "Epoch 423/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0055\n",
            "Epoch 424/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0033\n",
            "Epoch 425/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0028\n",
            "Epoch 426/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 427/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 428/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 429/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0031\n",
            "Epoch 430/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 431/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 432/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 433/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0031\n",
            "Epoch 434/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 435/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 436/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0030\n",
            "Epoch 437/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0035\n",
            "Epoch 438/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0032\n",
            "Epoch 439/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0028\n",
            "Epoch 440/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0026\n",
            "Epoch 441/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 442/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 443/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 444/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0026\n",
            "Epoch 445/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0034\n",
            "Epoch 446/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0028\n",
            "Epoch 447/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 448/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0030\n",
            "Epoch 449/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0025\n",
            "Epoch 450/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 451/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0028\n",
            "Epoch 452/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 453/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 454/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0027\n",
            "Epoch 455/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 456/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 457/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 458/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 459/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 460/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 461/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0028\n",
            "Epoch 462/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0031\n",
            "Epoch 463/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0030\n",
            "Epoch 464/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 465/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 466/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 467/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 468/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 469/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0026\n",
            "Epoch 470/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0024\n",
            "Epoch 471/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0024\n",
            "Epoch 472/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 473/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0027\n",
            "Epoch 474/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 475/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0024\n",
            "Epoch 476/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 477/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0032\n",
            "Epoch 478/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 479/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 480/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 481/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0031\n",
            "Epoch 482/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 483/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 484/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 485/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 486/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 487/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 488/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 489/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 490/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 491/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 492/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 493/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0024\n",
            "Epoch 494/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 495/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 496/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0024\n",
            "Epoch 497/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0027\n",
            "Epoch 498/2000\n",
            "1537/1537 [==============================] - 0s 124us/step - loss: 0.0033\n",
            "Epoch 499/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0028\n",
            "Epoch 500/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 501/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 502/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 503/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 504/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 505/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0029\n",
            "Epoch 506/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 507/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0026\n",
            "Epoch 508/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0031\n",
            "Epoch 509/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0031\n",
            "Epoch 510/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 511/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 512/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 513/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 514/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 515/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0028\n",
            "Epoch 516/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0031\n",
            "Epoch 517/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0028\n",
            "Epoch 518/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0033\n",
            "Epoch 519/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0042\n",
            "Epoch 520/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0031\n",
            "Epoch 521/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 522/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 523/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 524/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0026\n",
            "Epoch 525/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 526/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0029\n",
            "Epoch 527/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 528/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0026\n",
            "Epoch 529/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 530/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 531/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0029\n",
            "Epoch 532/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0027\n",
            "Epoch 533/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 534/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 535/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0024\n",
            "Epoch 536/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 537/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 538/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 539/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 540/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 541/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 542/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 543/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0023\n",
            "Epoch 544/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 545/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 546/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 547/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 548/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 549/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0029\n",
            "Epoch 550/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0031\n",
            "Epoch 551/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 552/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0029\n",
            "Epoch 553/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0035\n",
            "Epoch 554/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0032\n",
            "Epoch 555/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0031\n",
            "Epoch 556/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0028\n",
            "Epoch 557/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 558/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 559/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0024\n",
            "Epoch 560/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 561/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 562/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 563/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0027\n",
            "Epoch 564/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0033\n",
            "Epoch 565/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0047\n",
            "Epoch 566/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0027\n",
            "Epoch 567/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 568/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 569/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 570/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0029\n",
            "Epoch 571/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 572/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0026\n",
            "Epoch 573/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 574/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 575/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 576/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 577/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0028\n",
            "Epoch 578/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 579/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 580/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 581/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0023\n",
            "Epoch 582/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 583/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 584/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 585/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 586/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 587/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 588/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 589/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 590/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0027\n",
            "Epoch 591/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 592/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 593/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 594/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 595/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 596/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 597/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0025\n",
            "Epoch 598/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 599/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 600/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 601/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 602/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 603/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 604/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 605/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 606/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 607/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 608/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0027\n",
            "Epoch 609/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0025\n",
            "Epoch 610/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 611/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 612/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 613/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 614/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 615/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 616/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0025\n",
            "Epoch 617/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 618/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 619/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0032\n",
            "Epoch 620/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0031\n",
            "Epoch 621/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0039\n",
            "Epoch 622/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0032\n",
            "Epoch 623/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0027\n",
            "Epoch 624/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 625/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 626/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 627/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 628/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 629/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 630/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 631/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 632/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 633/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 634/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0027\n",
            "Epoch 635/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0023\n",
            "Epoch 636/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 637/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 638/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 639/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0027\n",
            "Epoch 640/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 641/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0030\n",
            "Epoch 642/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0038\n",
            "Epoch 643/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 644/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 645/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 646/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0022\n",
            "Epoch 647/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 648/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 649/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 650/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 651/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0028\n",
            "Epoch 652/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 653/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0028\n",
            "Epoch 654/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 655/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0028\n",
            "Epoch 656/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 657/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0028\n",
            "Epoch 658/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 659/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 660/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 661/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 662/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0028\n",
            "Epoch 663/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0027\n",
            "Epoch 664/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0028\n",
            "Epoch 665/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0025\n",
            "Epoch 666/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0023\n",
            "Epoch 667/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0022\n",
            "Epoch 668/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 669/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0028\n",
            "Epoch 670/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 671/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 672/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 673/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 674/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 675/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 676/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 677/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 678/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0027\n",
            "Epoch 679/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 680/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0025\n",
            "Epoch 681/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 682/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 683/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 684/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 685/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0024\n",
            "Epoch 686/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 687/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 688/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 689/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 690/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 691/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0030\n",
            "Epoch 692/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 693/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 694/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 695/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0023\n",
            "Epoch 696/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 697/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 698/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 699/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0027\n",
            "Epoch 700/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 701/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 702/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 703/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 704/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 705/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0026\n",
            "Epoch 706/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 707/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 708/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 709/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 710/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 711/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0029\n",
            "Epoch 712/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 713/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0023\n",
            "Epoch 714/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 715/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 716/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 717/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0025\n",
            "Epoch 718/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 719/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 720/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0029\n",
            "Epoch 721/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 722/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0028\n",
            "Epoch 723/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0028\n",
            "Epoch 724/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0025\n",
            "Epoch 725/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0028\n",
            "Epoch 726/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0028\n",
            "Epoch 727/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 728/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 729/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 730/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 731/2000\n",
            "1537/1537 [==============================] - 0s 130us/step - loss: 0.0026\n",
            "Epoch 732/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0028\n",
            "Epoch 733/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0027\n",
            "Epoch 734/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 735/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0024\n",
            "Epoch 736/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 737/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 738/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 739/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 740/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0023\n",
            "Epoch 741/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 742/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 743/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 744/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 745/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 746/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 747/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 748/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0028\n",
            "Epoch 749/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0023\n",
            "Epoch 750/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 751/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 752/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 753/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 754/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0029\n",
            "Epoch 755/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0026\n",
            "Epoch 756/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0026\n",
            "Epoch 757/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0023\n",
            "Epoch 758/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 759/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 760/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0023\n",
            "Epoch 761/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 762/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 763/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 764/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 765/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 766/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 767/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 768/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 769/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 770/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 771/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0026\n",
            "Epoch 772/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0032\n",
            "Epoch 773/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0029\n",
            "Epoch 774/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0026\n",
            "Epoch 775/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0025\n",
            "Epoch 776/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 777/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 778/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0026\n",
            "Epoch 779/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 780/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 781/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 782/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 783/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 784/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 785/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 786/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 787/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 788/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 789/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 790/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 791/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0030\n",
            "Epoch 792/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0030\n",
            "Epoch 793/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 794/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 795/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 796/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 797/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 798/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 799/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0022\n",
            "Epoch 800/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 801/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 802/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 803/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 804/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 805/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 806/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0028\n",
            "Epoch 807/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 808/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 809/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 810/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 811/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0027\n",
            "Epoch 812/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 813/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0025\n",
            "Epoch 814/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 815/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 816/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0025\n",
            "Epoch 817/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 818/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 819/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 820/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 821/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0026\n",
            "Epoch 822/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0026\n",
            "Epoch 823/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 824/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0024\n",
            "Epoch 825/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0026\n",
            "Epoch 826/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 827/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 828/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 829/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 830/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0027\n",
            "Epoch 831/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 832/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 833/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 834/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 835/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 836/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 837/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 838/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 839/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 840/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 841/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 842/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 843/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0028\n",
            "Epoch 844/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 845/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 846/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 847/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 848/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 849/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 850/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 851/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 852/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 853/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 854/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0029\n",
            "Epoch 855/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 856/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 857/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 858/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0024\n",
            "Epoch 859/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0024\n",
            "Epoch 860/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0025\n",
            "Epoch 861/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0022\n",
            "Epoch 862/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 863/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 864/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 865/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 866/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 867/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0029\n",
            "Epoch 868/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0023\n",
            "Epoch 869/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 870/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 871/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 872/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 873/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 874/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 875/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 876/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 877/2000\n",
            "1537/1537 [==============================] - 0s 130us/step - loss: 0.0026\n",
            "Epoch 878/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 879/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 880/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 881/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 882/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 883/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 884/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 885/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 886/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 887/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 888/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 889/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 890/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 891/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0029\n",
            "Epoch 892/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0026\n",
            "Epoch 893/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 894/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 895/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0025\n",
            "Epoch 896/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 897/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 898/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 899/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 900/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 901/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 902/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 903/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 904/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 905/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 906/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 907/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 908/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 909/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 910/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 911/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 912/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 913/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 914/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 915/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 916/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0027\n",
            "Epoch 917/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 918/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 919/2000\n",
            "1537/1537 [==============================] - 0s 128us/step - loss: 0.0022\n",
            "Epoch 920/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 921/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0028\n",
            "Epoch 922/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0027\n",
            "Epoch 923/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 924/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 925/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 926/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 927/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 928/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 929/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 930/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0023\n",
            "Epoch 931/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 932/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0022\n",
            "Epoch 933/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0022\n",
            "Epoch 934/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 935/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 936/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0023\n",
            "Epoch 937/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 938/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 939/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 940/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 941/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 942/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 943/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 944/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 945/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 946/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 947/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 948/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0027\n",
            "Epoch 949/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 950/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0032\n",
            "Epoch 951/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 952/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 953/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 954/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0024\n",
            "Epoch 955/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 956/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 957/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 958/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 959/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 960/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 961/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 962/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 963/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 964/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 965/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 966/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 967/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 968/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 969/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 970/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 971/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 972/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 973/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 974/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 975/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 976/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 977/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 978/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0025\n",
            "Epoch 979/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0024\n",
            "Epoch 980/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 981/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 982/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 983/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 984/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 985/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0023\n",
            "Epoch 986/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 987/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 988/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 989/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 990/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 991/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 992/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 993/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0025\n",
            "Epoch 994/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 995/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 996/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 997/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 998/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 999/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1000/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1001/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 1002/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1003/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1004/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1005/2000\n",
            "1537/1537 [==============================] - 0s 125us/step - loss: 0.0023\n",
            "Epoch 1006/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1007/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1008/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1009/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0022\n",
            "Epoch 1010/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1011/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0022\n",
            "Epoch 1012/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0022\n",
            "Epoch 1013/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0022\n",
            "Epoch 1014/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0022\n",
            "Epoch 1015/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0024\n",
            "Epoch 1016/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0027\n",
            "Epoch 1017/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0026\n",
            "Epoch 1018/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 1019/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0025\n",
            "Epoch 1020/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0022\n",
            "Epoch 1021/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0022\n",
            "Epoch 1022/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0022\n",
            "Epoch 1023/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0023\n",
            "Epoch 1024/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0022\n",
            "Epoch 1025/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0028\n",
            "Epoch 1026/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 1027/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 1028/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1029/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0022\n",
            "Epoch 1030/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1031/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1032/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 1033/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 1034/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1035/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1036/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1037/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1038/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0025\n",
            "Epoch 1039/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 1040/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0028\n",
            "Epoch 1041/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1042/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 1043/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 1044/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1045/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1046/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1047/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1048/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1049/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1050/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 1051/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 1052/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1053/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0026\n",
            "Epoch 1054/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 1055/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0026\n",
            "Epoch 1056/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1057/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1058/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1059/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1060/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1061/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 1062/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1063/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1064/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1065/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0025\n",
            "Epoch 1066/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1067/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 1068/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0028\n",
            "Epoch 1069/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0022\n",
            "Epoch 1070/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0024\n",
            "Epoch 1071/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0024\n",
            "Epoch 1072/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1073/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1074/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1075/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1076/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0022\n",
            "Epoch 1077/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1078/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0024\n",
            "Epoch 1079/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1080/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1081/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1082/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1083/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1084/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1085/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0027\n",
            "Epoch 1086/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0023\n",
            "Epoch 1087/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1088/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1089/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1090/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1091/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1092/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1093/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1094/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1095/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0023\n",
            "Epoch 1096/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0027\n",
            "Epoch 1097/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0027\n",
            "Epoch 1098/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0028\n",
            "Epoch 1099/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 1100/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0022\n",
            "Epoch 1101/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1102/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1103/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0022\n",
            "Epoch 1104/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1105/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1106/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0022\n",
            "Epoch 1107/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1108/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 1109/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1110/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0021\n",
            "Epoch 1111/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 1112/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1113/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1114/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 1115/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 1116/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 1117/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1118/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1119/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1120/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1121/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 1122/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1123/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1124/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1125/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0026\n",
            "Epoch 1126/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 1127/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 1128/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 1129/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1130/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 1131/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 1132/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1133/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1134/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1135/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1136/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1137/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1138/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1139/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1140/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1141/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1142/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1143/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1144/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1145/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1146/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 1147/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1148/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 1149/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0023\n",
            "Epoch 1150/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1151/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1152/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1153/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1154/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1155/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1156/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1157/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1158/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1159/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1160/2000\n",
            "1537/1537 [==============================] - 0s 126us/step - loss: 0.0022\n",
            "Epoch 1161/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0021\n",
            "Epoch 1162/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1163/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0024\n",
            "Epoch 1164/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1165/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1166/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1167/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1168/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1169/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1170/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1171/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 1172/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1173/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1174/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1175/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1176/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1177/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 1178/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 1179/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0025\n",
            "Epoch 1180/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 1181/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 1182/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1183/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 1184/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1185/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1186/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1187/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1188/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1189/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1190/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1191/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1192/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1193/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1194/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1195/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1196/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 1197/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1198/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1199/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1200/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1201/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1202/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1203/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1204/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1205/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0025\n",
            "Epoch 1206/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0024\n",
            "Epoch 1207/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0025\n",
            "Epoch 1208/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1209/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1210/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1211/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1212/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1213/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1214/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1215/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0025\n",
            "Epoch 1216/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1217/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1218/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1219/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1220/2000\n",
            "1537/1537 [==============================] - 0s 124us/step - loss: 0.0025\n",
            "Epoch 1221/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0022\n",
            "Epoch 1222/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1223/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1224/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1225/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1226/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0027\n",
            "Epoch 1227/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1228/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1229/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1230/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0022\n",
            "Epoch 1231/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1232/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1233/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 1234/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 1235/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1236/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0021\n",
            "Epoch 1237/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1238/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 1239/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0025\n",
            "Epoch 1240/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1241/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1242/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1243/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1244/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0024\n",
            "Epoch 1245/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0026\n",
            "Epoch 1246/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1247/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1248/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0021\n",
            "Epoch 1249/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0022\n",
            "Epoch 1250/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0027\n",
            "Epoch 1251/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1252/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1253/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1254/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1255/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 1256/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0024\n",
            "Epoch 1257/2000\n",
            "1537/1537 [==============================] - 0s 95us/step - loss: 0.0026\n",
            "Epoch 1258/2000\n",
            "1537/1537 [==============================] - 0s 95us/step - loss: 0.0023\n",
            "Epoch 1259/2000\n",
            "1537/1537 [==============================] - 0s 96us/step - loss: 0.0022\n",
            "Epoch 1260/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1261/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0021\n",
            "Epoch 1262/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1263/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1264/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0026\n",
            "Epoch 1265/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1266/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1267/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1268/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1269/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1270/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1271/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1272/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1273/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1274/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1275/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1276/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1277/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1278/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1279/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0028\n",
            "Epoch 1280/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 1281/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1282/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1283/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1284/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0022\n",
            "Epoch 1285/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0023\n",
            "Epoch 1286/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1287/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1288/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1289/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1290/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 1291/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 1292/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0025\n",
            "Epoch 1293/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0022\n",
            "Epoch 1294/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1295/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1296/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1297/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1298/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1299/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 1300/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1301/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1302/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1303/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1304/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0022\n",
            "Epoch 1305/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1306/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1307/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0023\n",
            "Epoch 1308/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1309/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1310/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1311/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1312/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1313/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1314/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0021\n",
            "Epoch 1315/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0026\n",
            "Epoch 1316/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1317/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1318/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1319/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1320/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0021\n",
            "Epoch 1321/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1322/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1323/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1324/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1325/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1326/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1327/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0024\n",
            "Epoch 1328/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0025\n",
            "Epoch 1329/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0021\n",
            "Epoch 1330/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1331/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1332/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1333/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1334/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1335/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1336/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1337/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1338/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1339/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1340/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1341/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0021\n",
            "Epoch 1342/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1343/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1344/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1345/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1346/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0021\n",
            "Epoch 1347/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1348/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1349/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1350/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 1351/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 1352/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0025\n",
            "Epoch 1353/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1354/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1355/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1356/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1357/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1358/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0023\n",
            "Epoch 1359/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0024\n",
            "Epoch 1360/2000\n",
            "1537/1537 [==============================] - 0s 125us/step - loss: 0.0023\n",
            "Epoch 1361/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1362/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1363/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1364/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1365/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1366/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1367/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1368/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1369/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1370/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1371/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0021\n",
            "Epoch 1372/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1373/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0026\n",
            "Epoch 1374/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1375/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1376/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1377/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1378/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1379/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1380/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1381/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1382/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1383/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1384/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0024\n",
            "Epoch 1385/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0024\n",
            "Epoch 1386/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1387/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1388/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1389/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1390/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1391/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1392/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0024\n",
            "Epoch 1393/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1394/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1395/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1396/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1397/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1398/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1399/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0023\n",
            "Epoch 1400/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1401/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1402/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0026\n",
            "Epoch 1403/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0025\n",
            "Epoch 1404/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1405/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1406/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1407/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1408/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1409/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1410/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1411/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0026\n",
            "Epoch 1412/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1413/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1414/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 1415/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1416/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1417/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1418/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1419/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1420/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1421/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0027\n",
            "Epoch 1422/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1423/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1424/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1425/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1426/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1427/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1428/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1429/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 1430/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0025\n",
            "Epoch 1431/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1432/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1433/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1434/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1435/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 1436/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1437/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1438/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1439/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 1440/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1441/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1442/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1443/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1444/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1445/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0024\n",
            "Epoch 1446/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1447/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1448/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1449/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0021\n",
            "Epoch 1450/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1451/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1452/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1453/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0023\n",
            "Epoch 1454/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1455/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1456/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1457/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1458/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1459/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1460/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 1461/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1462/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1463/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1464/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1465/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0024\n",
            "Epoch 1466/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1467/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0025\n",
            "Epoch 1468/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 1469/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1470/2000\n",
            "1537/1537 [==============================] - 0s 125us/step - loss: 0.0021\n",
            "Epoch 1471/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1472/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1473/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1474/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1475/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1476/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1477/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1478/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0021\n",
            "Epoch 1479/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1480/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1481/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0023\n",
            "Epoch 1482/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 1483/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1484/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1485/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1486/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1487/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1488/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0021\n",
            "Epoch 1489/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1490/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0025\n",
            "Epoch 1491/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0022\n",
            "Epoch 1492/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1493/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0022\n",
            "Epoch 1494/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0022\n",
            "Epoch 1495/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0025\n",
            "Epoch 1496/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1497/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0022\n",
            "Epoch 1498/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0024\n",
            "Epoch 1499/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1500/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1501/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1502/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1503/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1504/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1505/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1506/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1507/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1508/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1509/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1510/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1511/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1512/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1513/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0023\n",
            "Epoch 1514/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1515/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1516/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1517/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0020\n",
            "Epoch 1518/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1519/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1520/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1521/2000\n",
            "1537/1537 [==============================] - 0s 123us/step - loss: 0.0021\n",
            "Epoch 1522/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1523/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 1524/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1525/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1526/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1527/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1528/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1529/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1530/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1531/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0024\n",
            "Epoch 1532/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0022\n",
            "Epoch 1533/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0024\n",
            "Epoch 1534/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0020\n",
            "Epoch 1535/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0020\n",
            "Epoch 1536/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0020\n",
            "Epoch 1537/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1538/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1539/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1540/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1541/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1542/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1543/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0024\n",
            "Epoch 1544/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1545/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1546/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0020\n",
            "Epoch 1547/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0021\n",
            "Epoch 1548/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1549/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1550/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1551/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1552/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1553/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1554/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1555/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0020\n",
            "Epoch 1556/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1557/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1558/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0026\n",
            "Epoch 1559/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1560/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1561/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 1562/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1563/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1564/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1565/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1566/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1567/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1568/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1569/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1570/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1571/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1572/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1573/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1574/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1575/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0020\n",
            "Epoch 1576/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1577/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1578/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1579/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1580/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0025\n",
            "Epoch 1581/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1582/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1583/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1584/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1585/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0020\n",
            "Epoch 1586/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1587/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1588/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0019\n",
            "Epoch 1589/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1590/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1591/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1592/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1593/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1594/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0021\n",
            "Epoch 1595/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1596/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0021\n",
            "Epoch 1597/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1598/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0022\n",
            "Epoch 1599/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1600/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1601/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1602/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0020\n",
            "Epoch 1603/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1604/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1605/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1606/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1607/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1608/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1609/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1610/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0024\n",
            "Epoch 1611/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1612/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1613/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1614/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1615/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0019\n",
            "Epoch 1616/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1617/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1618/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1619/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1620/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0020\n",
            "Epoch 1621/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1622/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0024\n",
            "Epoch 1623/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1624/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1625/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1626/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1627/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1628/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0019\n",
            "Epoch 1629/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1630/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1631/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1632/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1633/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1634/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1635/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1636/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1637/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1638/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0019\n",
            "Epoch 1639/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1640/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0021\n",
            "Epoch 1641/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1642/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0020\n",
            "Epoch 1643/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1644/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1645/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1646/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1647/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0018\n",
            "Epoch 1648/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0018\n",
            "Epoch 1649/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1650/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0019\n",
            "Epoch 1651/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0019\n",
            "Epoch 1652/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0019\n",
            "Epoch 1653/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0020\n",
            "Epoch 1654/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1655/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0020\n",
            "Epoch 1656/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0019\n",
            "Epoch 1657/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1658/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1659/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1660/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1661/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1662/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1663/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0018\n",
            "Epoch 1664/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0017\n",
            "Epoch 1665/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0018\n",
            "Epoch 1666/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0018\n",
            "Epoch 1667/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1668/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1669/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1670/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0019\n",
            "Epoch 1671/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1672/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1673/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1674/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1675/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0019\n",
            "Epoch 1676/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1677/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1678/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1679/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1680/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1681/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0019\n",
            "Epoch 1682/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0022\n",
            "Epoch 1683/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1684/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1685/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0018\n",
            "Epoch 1686/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0018\n",
            "Epoch 1687/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0018\n",
            "Epoch 1688/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0019\n",
            "Epoch 1689/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0017\n",
            "Epoch 1690/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0017\n",
            "Epoch 1691/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0020\n",
            "Epoch 1692/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0026\n",
            "Epoch 1693/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1694/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1695/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1696/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1697/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1698/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1699/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0020\n",
            "Epoch 1700/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0020\n",
            "Epoch 1701/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1702/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0019\n",
            "Epoch 1703/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1704/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1705/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0017\n",
            "Epoch 1706/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0017\n",
            "Epoch 1707/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0021\n",
            "Epoch 1708/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1709/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1710/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0017\n",
            "Epoch 1711/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0016\n",
            "Epoch 1712/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0017\n",
            "Epoch 1713/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1714/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1715/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0021\n",
            "Epoch 1716/2000\n",
            "1537/1537 [==============================] - 0s 124us/step - loss: 0.0020\n",
            "Epoch 1717/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1718/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0019\n",
            "Epoch 1719/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0017\n",
            "Epoch 1720/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0017\n",
            "Epoch 1721/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0017\n",
            "Epoch 1722/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0023\n",
            "Epoch 1723/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1724/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1725/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1726/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1727/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1728/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0018\n",
            "Epoch 1729/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1730/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0021\n",
            "Epoch 1731/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1732/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1733/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0023\n",
            "Epoch 1734/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1735/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1736/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0023\n",
            "Epoch 1737/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1738/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1739/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1740/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0018\n",
            "Epoch 1741/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0016\n",
            "Epoch 1742/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0019\n",
            "Epoch 1743/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0016\n",
            "Epoch 1744/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0016\n",
            "Epoch 1745/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1746/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1747/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1748/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1749/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1750/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1751/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1752/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1753/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1754/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0017\n",
            "Epoch 1755/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0018\n",
            "Epoch 1756/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0019\n",
            "Epoch 1757/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0017\n",
            "Epoch 1758/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0018\n",
            "Epoch 1759/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1760/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1761/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1762/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0022\n",
            "Epoch 1763/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0019\n",
            "Epoch 1764/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0019\n",
            "Epoch 1765/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0017\n",
            "Epoch 1766/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0018\n",
            "Epoch 1767/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1768/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1769/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1770/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1771/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1772/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1773/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1774/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1775/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0017\n",
            "Epoch 1776/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0017\n",
            "Epoch 1777/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0017\n",
            "Epoch 1778/2000\n",
            "1537/1537 [==============================] - 0s 120us/step - loss: 0.0018\n",
            "Epoch 1779/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0018\n",
            "Epoch 1780/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0017\n",
            "Epoch 1781/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0017\n",
            "Epoch 1782/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0018\n",
            "Epoch 1783/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0017\n",
            "Epoch 1784/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1785/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1786/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1787/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1788/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0019\n",
            "Epoch 1789/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1790/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0018\n",
            "Epoch 1791/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0016\n",
            "Epoch 1792/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0016\n",
            "Epoch 1793/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0015\n",
            "Epoch 1794/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0017\n",
            "Epoch 1795/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1796/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0018\n",
            "Epoch 1797/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0017\n",
            "Epoch 1798/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0016\n",
            "Epoch 1799/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1800/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0017\n",
            "Epoch 1801/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0016\n",
            "Epoch 1802/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0019\n",
            "Epoch 1803/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0019\n",
            "Epoch 1804/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0021\n",
            "Epoch 1805/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1806/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1807/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1808/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0018\n",
            "Epoch 1809/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0020\n",
            "Epoch 1810/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1811/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1812/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0019\n",
            "Epoch 1813/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1814/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1815/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0017\n",
            "Epoch 1816/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0017\n",
            "Epoch 1817/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0016\n",
            "Epoch 1818/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0015\n",
            "Epoch 1819/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0014\n",
            "Epoch 1820/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0015\n",
            "Epoch 1821/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0017\n",
            "Epoch 1822/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0017\n",
            "Epoch 1823/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0024\n",
            "Epoch 1824/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1825/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1826/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1827/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1828/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1829/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0021\n",
            "Epoch 1830/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0021\n",
            "Epoch 1831/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0023\n",
            "Epoch 1832/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1833/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1834/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0024\n",
            "Epoch 1835/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1836/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0016\n",
            "Epoch 1837/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1838/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0018\n",
            "Epoch 1839/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0016\n",
            "Epoch 1840/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0025\n",
            "Epoch 1841/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0025\n",
            "Epoch 1842/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1843/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0021\n",
            "Epoch 1844/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1845/2000\n",
            "1537/1537 [==============================] - 0s 96us/step - loss: 0.0019\n",
            "Epoch 1846/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0019\n",
            "Epoch 1847/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0019\n",
            "Epoch 1848/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1849/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1850/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1851/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1852/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1853/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0017\n",
            "Epoch 1854/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0016\n",
            "Epoch 1855/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0014\n",
            "Epoch 1856/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0015\n",
            "Epoch 1857/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0018\n",
            "Epoch 1858/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0018\n",
            "Epoch 1859/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0015\n",
            "Epoch 1860/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0014\n",
            "Epoch 1861/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0021\n",
            "Epoch 1862/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 1863/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0022\n",
            "Epoch 1864/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0020\n",
            "Epoch 1865/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0021\n",
            "Epoch 1866/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 1867/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0024\n",
            "Epoch 1868/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1869/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0020\n",
            "Epoch 1870/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1871/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0021\n",
            "Epoch 1872/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0020\n",
            "Epoch 1873/2000\n",
            "1537/1537 [==============================] - 0s 97us/step - loss: 0.0020\n",
            "Epoch 1874/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1875/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1876/2000\n",
            "1537/1537 [==============================] - 0s 114us/step - loss: 0.0023\n",
            "Epoch 1877/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1878/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1879/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1880/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0019\n",
            "Epoch 1881/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0018\n",
            "Epoch 1882/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0016\n",
            "Epoch 1883/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0016\n",
            "Epoch 1884/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0015\n",
            "Epoch 1885/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0023\n",
            "Epoch 1886/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0025\n",
            "Epoch 1887/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0024\n",
            "Epoch 1888/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0019\n",
            "Epoch 1889/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1890/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0015\n",
            "Epoch 1891/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0021\n",
            "Epoch 1892/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0022\n",
            "Epoch 1893/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1894/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0022\n",
            "Epoch 1895/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1896/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1897/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1898/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0023\n",
            "Epoch 1899/2000\n",
            "1537/1537 [==============================] - 0s 112us/step - loss: 0.0022\n",
            "Epoch 1900/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1901/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0022\n",
            "Epoch 1902/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0023\n",
            "Epoch 1903/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0020\n",
            "Epoch 1904/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0024\n",
            "Epoch 1905/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1906/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1907/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1908/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1909/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 1910/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1911/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0019\n",
            "Epoch 1912/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1913/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1914/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1915/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0023\n",
            "Epoch 1916/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0020\n",
            "Epoch 1917/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1918/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1919/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1920/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1921/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0022\n",
            "Epoch 1922/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0021\n",
            "Epoch 1923/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1924/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1925/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0017\n",
            "Epoch 1926/2000\n",
            "1537/1537 [==============================] - 0s 116us/step - loss: 0.0018\n",
            "Epoch 1927/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0020\n",
            "Epoch 1928/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0019\n",
            "Epoch 1929/2000\n",
            "1537/1537 [==============================] - 0s 115us/step - loss: 0.0018\n",
            "Epoch 1930/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1931/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0020\n",
            "Epoch 1932/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1933/2000\n",
            "1537/1537 [==============================] - 0s 121us/step - loss: 0.0021\n",
            "Epoch 1934/2000\n",
            "1537/1537 [==============================] - 0s 117us/step - loss: 0.0019\n",
            "Epoch 1935/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0022\n",
            "Epoch 1936/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0017\n",
            "Epoch 1937/2000\n",
            "1537/1537 [==============================] - 0s 122us/step - loss: 0.0015\n",
            "Epoch 1938/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1939/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0016\n",
            "Epoch 1940/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1941/2000\n",
            "1537/1537 [==============================] - 0s 118us/step - loss: 0.0020\n",
            "Epoch 1942/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1943/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1944/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0019\n",
            "Epoch 1945/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1946/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0021\n",
            "Epoch 1947/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1948/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0019\n",
            "Epoch 1949/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0018\n",
            "Epoch 1950/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0023\n",
            "Epoch 1951/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0022\n",
            "Epoch 1952/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0018\n",
            "Epoch 1953/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1954/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1955/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0019\n",
            "Epoch 1956/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1957/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0019\n",
            "Epoch 1958/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0020\n",
            "Epoch 1959/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1960/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0018\n",
            "Epoch 1961/2000\n",
            "1537/1537 [==============================] - 0s 109us/step - loss: 0.0018\n",
            "Epoch 1962/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0023\n",
            "Epoch 1963/2000\n",
            "1537/1537 [==============================] - 0s 119us/step - loss: 0.0020\n",
            "Epoch 1964/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0016\n",
            "Epoch 1965/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0016\n",
            "Epoch 1966/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0014\n",
            "Epoch 1967/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0021\n",
            "Epoch 1968/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0021\n",
            "Epoch 1969/2000\n",
            "1537/1537 [==============================] - 0s 113us/step - loss: 0.0020\n",
            "Epoch 1970/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0021\n",
            "Epoch 1971/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0023\n",
            "Epoch 1972/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1973/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1974/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0022\n",
            "Epoch 1975/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0022\n",
            "Epoch 1976/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0017\n",
            "Epoch 1977/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0020\n",
            "Epoch 1978/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0016\n",
            "Epoch 1979/2000\n",
            "1537/1537 [==============================] - 0s 101us/step - loss: 0.0020\n",
            "Epoch 1980/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0019\n",
            "Epoch 1981/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0019\n",
            "Epoch 1982/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0018\n",
            "Epoch 1983/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0019\n",
            "Epoch 1984/2000\n",
            "1537/1537 [==============================] - 0s 102us/step - loss: 0.0021\n",
            "Epoch 1985/2000\n",
            "1537/1537 [==============================] - 0s 111us/step - loss: 0.0025\n",
            "Epoch 1986/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0020\n",
            "Epoch 1987/2000\n",
            "1537/1537 [==============================] - 0s 99us/step - loss: 0.0020\n",
            "Epoch 1988/2000\n",
            "1537/1537 [==============================] - 0s 94us/step - loss: 0.0020\n",
            "Epoch 1989/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0020\n",
            "Epoch 1990/2000\n",
            "1537/1537 [==============================] - 0s 98us/step - loss: 0.0022\n",
            "Epoch 1991/2000\n",
            "1537/1537 [==============================] - 0s 100us/step - loss: 0.0020\n",
            "Epoch 1992/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0023\n",
            "Epoch 1993/2000\n",
            "1537/1537 [==============================] - 0s 104us/step - loss: 0.0020\n",
            "Epoch 1994/2000\n",
            "1537/1537 [==============================] - 0s 103us/step - loss: 0.0019\n",
            "Epoch 1995/2000\n",
            "1537/1537 [==============================] - 0s 108us/step - loss: 0.0018\n",
            "Epoch 1996/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0018\n",
            "Epoch 1997/2000\n",
            "1537/1537 [==============================] - 0s 107us/step - loss: 0.0020\n",
            "Epoch 1998/2000\n",
            "1537/1537 [==============================] - 0s 105us/step - loss: 0.0021\n",
            "Epoch 1999/2000\n",
            "1537/1537 [==============================] - 0s 110us/step - loss: 0.0020\n",
            "Epoch 2000/2000\n",
            "1537/1537 [==============================] - 0s 106us/step - loss: 0.0015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17643916a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEqibAqj5tUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nodes_alpharnnt=1 # 4.8867e-04"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2u2b2qShzD_",
        "colab_type": "code",
        "outputId": "819595d7-1b82-440b-82b2-213caaec9df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "alpharnnt = AlphaRNNt(nodes_alpharnnt,0)\n",
        "alpharnnt.fit(x_train_reg,y_train_reg,epochs=2000,batch_size=100,callbacks=[es])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 2 and 1 for 'alphat_rnn_4/BiasAdd' (op: 'BiasAdd') with input shapes: [?,2], [1].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-11d36b47d54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpharnnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaRNNt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_alpharnnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0malpharnnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-5b6bb71655d4>\u001b[0m in \u001b[0;36mAlphaRNNt\u001b[0;34m(n_units, l1_reg)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mreg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAlphatRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m#reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-2fc14b185a5f>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    433\u001b[0m                                      \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                                      \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                                      initial_state=initial_state)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    672\u001b[0m                                              \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                                              \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                                              input_length=timesteps)\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)\u001b[0m\n\u001b[1;32m   3170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3173\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_learning_phase'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m                     \u001b[0muses_learning_phase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         last_output, outputs, states = K.rnn(step,\n",
            "\u001b[0;32m<ipython-input-101-2fc14b185a5f>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# biases: bias_z_i, bias_r_i, bias_h_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mmatrix_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mx_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mx_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   4383\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4385\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2716\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 761\u001b[0;31m         \"BiasAdd\", value=value, bias=bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    762\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3357\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   def _create_op_internal(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3424\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3427\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1768\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1769\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1770\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1771\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 2 and 1 for 'alphat_rnn_4/BiasAdd' (op: 'BiasAdd') with input shapes: [?,2], [1]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug1RbjLEDumM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_units=5\n",
        "l1_reg=0\n",
        "reg_model = Sequential()\n",
        "#reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwHOAJzXHkLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_units=20\n",
        "l1_reg=0\n",
        "reg_model2 = Sequential()\n",
        "reg_model2.add(AlphaRNN(hidden_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model2.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model2.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model2.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Bxweay8vNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(alpharnnt.layers[0].get_weights())\n",
        "print(alpharnnt.layers[1].get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-W-Lh2jhGOg",
        "colab_type": "code",
        "outputId": "89b47b0a-0e46-4060-d7b4-2bb1fff451d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.sigmoid(alpharnn.layers[0].get_weights()[3]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Sigmoid_1:0\", shape=(1,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcDxpnr4vbfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f35262d4-e3e6-474a-ba11-7c10f5749d4c"
      },
      "source": [
        "gru_pred_train = gru.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1537/1537 [==============================] - 3s 2ms/step\n",
            "355/355 [==============================] - 0s 63us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHUBCY2a6fOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "72238419-1246-42a9-95a9-549f378cf9a6"
      },
      "source": [
        "rs_pred_train = rs.predict(x_train_reg, verbose=1)\n",
        "rs_pred_test = rs.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1537/1537 [==============================] - 6s 4ms/step\n",
            "355/355 [==============================] - 0s 67us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xtr4_utp5_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5c497559-92a5-43af-d64a-b60a6ce503df"
      },
      "source": [
        "rnn_pred_train = rnn.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1537/1537 [==============================] - 0s 128us/step\n",
            "355/355 [==============================] - 0s 39us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE90OgzpcWB_",
        "colab_type": "code",
        "outputId": "201c6014-bc0c-42fb-a331-07574cfdd006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "alpharnn_pred_train = alpharnn.predict(x_train_reg, verbose=1)\n",
        "alpharnn_pred_test = alpharnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1546/1546 [==============================] - 2s 1ms/step\n",
            "364/364 [==============================] - 0s 46us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snIH77i4EEVq",
        "colab_type": "code",
        "outputId": "db26620e-6300-457c-a21c-5e63e3d09484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "alpharnnt_pred_train = alpharnnt.predict(x_train_reg, verbose=1)\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1546/1546 [==============================] - 2s 1ms/step\n",
            "364/364 [==============================] - 0s 56us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dpUFvv3TFKJW",
        "colab": {}
      },
      "source": [
        "#rnn_model = RNN_model2(5,0)\n",
        "alpharnn_fit = reg_model2.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "alpharnn_pred_test = reg_model2.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],alpharnn_pred_test[:,0])\n",
        "print(\"Alpha RNN test data mse = \" + str(mse))\n",
        "print(\"Alpha RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "reg_model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cx-acE-FFoGr",
        "colab": {}
      },
      "source": [
        "#rnn_model = RNN_model2(5,0)\n",
        "alpharnnt_fit = alpharnnt.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],alpharnnt_pred_test[:,0])\n",
        "print(\"Alpha_t RNN test data mse = \" + str(mse))\n",
        "print(\"Alpha_t RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "alpharnnt.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEejnbMGSJ4",
        "colab_type": "code",
        "outputId": "3e097cd8-53a5-4202-b698-be455134c171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], alpharnnt_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], alpharnnt_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000490460512147073\n",
            "0.00044399483148641327\n",
            "0.022146343087450646\n",
            "0.021071184861948634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ-HA-Q2cjDs",
        "colab_type": "code",
        "outputId": "f5278549-59ee-4452-9c4c-234f43ded636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], alpharnn_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], alpharnn_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0005382215566124526\n",
            "0.0004889021964658545\n",
            "0.02319960250979427\n",
            "0.022111132862561667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTIKiBet463a"
      },
      "source": [
        "## Time series cross-val\n",
        "\n",
        "Just use val_loss for model selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t9_3cL5-kh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQZXa3l1ByQa",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)Pick the model with the lowest val loss sum over folds. you can *not* use the test set for model selection! This would be cheating\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBuK_PhPXe5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "72d56184-7fa1-4520-ba83-c6695401cc83",
        "id": "GoGO-YMJXhEX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# AlphaRNN: Pick the model with the lowest val_loss. Retrain it on all data and then perform prediction\n",
        "#val, idx = min((val, idx) for (idx, val) in enumerate(val_losses))\n",
        "#print(hidden_sizes[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCKK_cJwc98C",
        "colab_type": "text"
      },
      "source": [
        "### Train on all training data with best model and predict on test set (no rolling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWH0sGENEfAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#session = tf.Session()\n",
        "#alpharnn = simpleRNN(1, 5) #simpleAlphaRNN(1, 5) GRU(1,5)\n",
        "#model,_=train(session, alpharnn, x_train_reg, x_test_reg, y_train_reg, y_test_reg,max_epochs=10000) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui7qu93eXWxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "04169566-9830-425a-c1c1-efce5b66b632"
      },
      "source": [
        "session = tf.Session()\n",
        "alpharnn_t = alphaRNN(input_dimensions, hidden_sizes[idx])\n",
        "model,_=train(session, alpharnn_t, x_train_reg, x_test_reg, y_train_reg, y_test_reg, max_epochs=5000) "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-cd645fff6442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malpharnn_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphaRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpharnn_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'alphaRNN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxFz0a7E10q",
        "colab_type": "code",
        "outputId": "75666baa-a218-45dc-98f3-936fab38bbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "plt.plot(rs_pred_test, color='r', label='Alpha-RNN')\n",
        "plt.plot(rnn_pred_test, color='g', label='RNN')\n",
        "plt.plot(gru_pred_test, color='y', label='GRU')\n",
        "plt.plot(y_test_reg.flatten(),'b', label='Actual')\n",
        "plt.legend(loc=0)\n",
        "plt.title('Actual vs Predicted AlphaRNN_t')\n",
        "plt.show()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAGuCAYAAABrxzvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZFqE9GlcR5",
        "colab_type": "code",
        "outputId": "ca99b70b-a2dc-436f-d658-ab35f718bc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mse = mean_squared_error(y_test_reg[:,0],y_predicted_ar_t)\n",
        "print(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0004427545573362202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVXru1dJApZF",
        "colab_type": "code",
        "outputId": "703fa91d-8f25-406a-de0b-f5cc70077600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mse_std = np.math.sqrt(mse)\n",
        "print(mse_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022216155077072364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNbGTp4QgUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7UbO-ooTQoz3",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(model.output, feed_dict={model.input_layer: x_test_reg})\n",
        "y_predicted_ar=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_ar[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMdbVYYdQvEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(model.output, feed_dict={model.input_layer: x_train_reg})\n",
        "y_predicted_train=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_train[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVau9K7Q5sl",
        "colab_type": "code",
        "outputId": "0bf13883-5fc2-423b-bb7a-81cb7e2c8549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\n",
        "se = mean_squared_error(y_test_reg[:,0],y_predicted_ar)\n",
        "print(mse)\n",
        "mse_std = np.math.sqrt(mse)\n",
        "print(mse_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00042334075261903065\n",
            "0.02057524611320678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLXOjS8PVXe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEw4AWqxcjfF",
        "colab_type": "text"
      },
      "source": [
        "#Comparison with Keras models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Or6iq32awi0e",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7ooyKI-wlwo",
        "colab": {}
      },
      "source": [
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0Bjom9wpT2",
        "colab": {}
      },
      "source": [
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LeXlBJgwtdy",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FImnMFUowv1P",
        "colab": {}
      },
      "source": [
        "def RNN_model2(n_units=10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(SimpleRNN(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def GRU_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(GRU(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def LSTM_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(LSTM(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    #LPNorm.build_loss(p = float('inf'))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def AlphaRNN(hidden_units = 10, l1_reg=0):\n",
        "  reg_model = Sequential()\n",
        "  #reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  #reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "  reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "  #reg_model.add(Dropout(0.2))\n",
        "  reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=500,callbacks=[es])\n",
        "  return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNI_srXYw1yf",
        "colab": {}
      },
      "source": [
        "n_units = [1,2,5,10,20]\n",
        "l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for RNN...\")\n",
        "model = KerasRegressor(build_fn=RNN_model2, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_rnn = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']\n",
        "#n_units = [10, 20, 30, 40, 50, 60, 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhMb6Muy5pTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_omBRtqnMRS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = RNN_model2(nodes_rnn,0)\n",
        "rnn_fit = rnn_model.fit(x_train_reg,y_train_reg, epochs = 2000, batch_size=500, callbacks=[es])\n",
        "#rnn_fit = rnn_model.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=100, callbacks=[es])\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],rnn_pred_test[:,0])\n",
        "print(\"RNN test data mse = \" + str(mse))\n",
        "print(\"RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9nDKwwbyAR",
        "colab_type": "code",
        "outputId": "a9272408-b394-486e-af2c-66ba225ebc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained RNN\n",
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 30us/step\n",
            "374/374 [==============================] - 0s 35us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMPQfeVb4Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the plain RNN\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDbGyuU5YamG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasRegressor(build_fn=LSTM_model2, epochs=2000, batch_size=100, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_lstm = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F64LeCexTn7I",
        "colab": {}
      },
      "source": [
        "lstm_model = LSTM_model2(nodes_lstm,0)\n",
        "lstm_fit = lstm_model.fit(x_train_reg,y_train_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "#score = cross_val_score(y_test_reg[:,0],g\n",
        "lstm_model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR_ze7BcavuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "#score = cross_val_score(y_test_reg[:,0],g\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPjEQGIaac6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions with the trained LSTM\n",
        "lstm_pred_train = lstm_model.predict(x_train_reg, verbose=1)\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbYi-p7Objfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], lstm_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:],lstm_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IRswaScYqi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Hyper parameter tuning of GRU model\")\n",
        "model = KerasRegressor(build_fn=GRU_model2, epochs=2000, batch_size=500, verbose=2)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes_gru = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Od0Dpe-rPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_model = GRU_model2(nodes_gru,0)\n",
        "gru_fit = gru_model.fit(x_train_reg,y_train_reg, epochs=2000, batch_size=500, callbacks=[es])\n",
        "gru_pred_test = gru_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test[:,0])\n",
        "print(\"GRU test data mse = \" + str(mse))\n",
        "print(\"GRU test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VbtK2zhi63mp",
        "colab": {}
      },
      "source": [
        "# MFD: Also try GRU, SimpleRNN\n",
        "# Need to cross-validate for the best number of hidden units (i.e. hidden_size)\n",
        "#gru_model = Sequential()\n",
        "#gru_model.add(GRU(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "#gru_model.add(Dense(1))\n",
        "#gru_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "#gru_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-XAiOFZjNq",
        "colab_type": "code",
        "outputId": "4c42a5b3-2f1d-41c2-be50-5ee5120038dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#gru_fit = gru_model.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=100, callbacks=[es])\n",
        "gru_pred_test = gru_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test[:,0])\n",
        "print(\"GRU test data mse = \" + str(mse))\n",
        "print(\"GRU test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "374/374 [==============================] - 0s 99us/step\n",
            "GRU test data mse = 0.00038023759714453945\n",
            "GRU test std mse =  0.01949968197547179\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  (None, 5)                 105       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 111\n",
            "Trainable params: 111\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo6NSAo5aFwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_pred_train = gru_model.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjClxCUlZ0WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], gru_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], gru_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHA7NNMy-tag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "hidden_size = 5\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "rnn_model.add(Dense(1))\n",
        "rnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "rnn_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUCECDU8JEwv",
        "colab_type": "code",
        "outputId": "0d3d248a-d955-4bc2-f15e-c79407634a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 5)                 35        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 41\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h70UAC4v-y7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 5\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(hidden_size, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "lstm_model.fit(x_train_reg, y_train_reg, epochs=2000, batch_size=500, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvcs-lveLFPU",
        "colab_type": "code",
        "outputId": "5144b923-0caf-49a1-9f93-c9742209ceb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse = \" + str(np.sqrt(mse)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "374/374 [==============================] - 0s 87us/step\n",
            "LSTM test data mse = 0.0005170304470553611\n",
            "LSTM test std mse = 0.02273830352192883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lege7PPLInX2",
        "colab_type": "code",
        "outputId": "d22b31a9-3d89-45e4-834e-7d48f40a6d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 5)                 140       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 146\n",
            "Trainable params: 146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16ZWjzOxxKIJ",
        "outputId": "4752e538-de5f-4d52-9949-e2b3928d7b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained LSTM\n",
        "lstm_pred_train = lstm_model.predict(x_train_reg, verbose=1)\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 67us/step\n",
            "374/374 [==============================] - 0s 95us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Imz5JXTnxS1y",
        "outputId": "bcf1963c-724b-4697-e9f9-3a94d1f67b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# make predictions with the trained RNN\n",
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 45us/step\n",
            "374/374 [==============================] - 0s 37us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJgsZCf2xVGl",
        "outputId": "116773e6-fe69-4303-d0f7-7d3aa64ea1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "gru_pred_train = gru_model.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 49us/step\n",
            "374/374 [==============================] - 0s 63us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mbWnLlQG_0z",
        "colab_type": "code",
        "outputId": "b0e4cfab-942e-4dbb-c92a-b5bcb87a2fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "alpharnnt_pred_train = alpharnnt.predict(x_train_reg, verbose=1)\n",
        "alpharnnt_pred_test = alpharnnt.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 46us/step\n",
            "374/374 [==============================] - 0s 47us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2RrHtFrQ6zl",
        "colab_type": "code",
        "outputId": "7f9520c0-ba6b-4af3-ebfc-e4de234bddfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "alpharnn_pred_train = alpharnn.predict(x_train_reg, verbose=1)\n",
        "alpharnn_pred_test = alpharnn.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1556/1556 [==============================] - 0s 40us/step\n",
            "374/374 [==============================] - 0s 42us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRQEeO42DpVv",
        "outputId": "3a802909-62f1-4898-dfb1-a80252c505ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the plain RNN\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0004990908195479893\n",
            "0.0004548309840356207\n",
            "0.02234034063187017\n",
            "0.021326766844405196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7fHAc8zHDyFZ",
        "outputId": "e553f8dd-3c84-4f8b-8ad3-fbb6e4fe34b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], gru_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], gru_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0005000548808021149\n",
            "0.00045362849019611295\n",
            "0.02236190691336754\n",
            "0.021298556058947118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKcHJU79D_K7",
        "outputId": "77a31dbe-5ee6-4d3d-a08d-3fdc0df87409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], lstm_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:],lstm_pred_test[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_std = np.sqrt(MSE_train)\n",
        "print(MSE_train_std)\n",
        "MSE_test_std = np.sqrt(MSE_test)\n",
        "print(MSE_test_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0004928369560054375\n",
            "0.0004456200560310017\n",
            "0.022199931441458046\n",
            "0.02110971473116114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Igbsz8AD4vi",
        "outputId": "d311ae03-0803-4a5c-f354-eb31131fbb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], alpharnnt_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], alpharnnt_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0004976949721127741\n",
            "0.0004485624818102432\n",
            "0.02230907824435546\n",
            "0.02117929370423488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkE8rbfTVcMC",
        "colab_type": "code",
        "outputId": "87597ff9-e1f8-4330-983b-eb6cb5eda78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], alpharnn_pred_train)  #train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], alpharnn_pred_test)     #validation_losses[:, 0])\n",
        "print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000511170497246813\n",
            "0.00046380252857424967\n",
            "0.022609079973471124\n",
            "0.02153607505034865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "314f7e44-08e3-4b20-cf31-cfb0e38b2d5e",
        "id": "9NyCR4Z3W-__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "#MSE_train = mean_squared_error(df_train[use_feature][n_steps:], y_predicted_train_t)  #train_losses[:, 0])\n",
        "#print(MSE_train)\n",
        "#MSE_test = mean_squared_error(df_test[use_feature][n_steps:], y_predicted_ar_t)     #validation_losses[:, 0])\n",
        "#print(MSE_test)\n",
        "MSE_train_sd = np.sqrt(MSE_train)\n",
        "print(MSE_train_sd)\n",
        "MSE_test_sd = np.sqrt(MSE_test)\n",
        "print(MSE_test_sd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14375467892b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMSE_train_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_train_sd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMSE_test_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_test_sd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t6SkTDvIxXcV",
        "outputId": "117884d6-f2bf-4104-d266-d89d919c7333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "train_line_real = plt.plot(df_train.index[n_steps:], df_train[use_feature][n_steps:], color=\"black\", label=\"Observed (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], lstm_pred_train[:, 0], color=\"red\", label=\"LSTM Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], rnn_pred_train[:, 0], color=\"blue\", label=\"RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], gru_pred_train[:, 0], color=\"orange\", label=\"GRU Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], alpharnn_pred_train[:,0], color='green', label=\"alpha RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], alpharnnt_pred_train[:,0], color=\"yellow\", label=\"alpha_t RNN Predict (Training)\" )\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Training)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"black\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], lstm_pred_test[:, 0], color=\"red\", label=\"LSTM Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], rnn_pred_test[:, 0], color=\"blue\", label=\"RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], gru_pred_test[:, 0], color=\"orange\", label=\"GRU Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpharnn_pred_test,color=\"green\", label=\"alpha RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpharnnt_pred_test, color=\"yellow\", label= \"alpha_t RNN Predict (Testing)\")\n",
        "# train_line_pred = plt.plot(session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH4IAJWAgED12AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}