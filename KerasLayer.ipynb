{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "KerasLayer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/KerasLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmsnQfHWWlX-",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7e296c5-d277-4691-cefe-3a8f5b40b9f5"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 14 23:15:31 2020\n",
        "\n",
        "@author: macbookpro\n",
        "\"\"\"\n",
        " \n",
        "\n",
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import layers\n",
        "#from alphaRNN import *\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)\n",
        "    \n",
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)\n",
        "\n",
        "p = 30 # the number of lags (in both the data and the models)\n",
        "vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]\n",
        "\n",
        "df = pd.DataFrame(vols, columns=['vol'])\n",
        "\n",
        "use_features = ['vol'] \n",
        "target = 'vol'\n",
        "n_steps = 10 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(len(df)*train_weight)\n",
        "\n",
        "df_train = df[use_features].iloc[:split]\n",
        "print(df_train)\n",
        "df_test = df[use_features].iloc[split:]\n",
        "\n",
        "def get_lagged_features(value, n_steps):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps, 0, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)\n",
        "\n",
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))\n",
        "\n",
        "train_batch_size = y_train_reg.shape[0]\n",
        "test_batch_size = y_test_reg.shape[0]    \n",
        "\n",
        "class SimpleAlphaRNN(Layer):\n",
        "\n",
        "    def __init__(self, input_dimensions, hidden_size=5, output_dim=1, dtype=tf.float64, **kwargs):\n",
        "        self.input_dimensions = input_dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_dim = output_dim\n",
        "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
        "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
        "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
        "        # Biases for hidden vectors of shape (hidden_size,)\n",
        "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')        \n",
        "        # Define the input layer placeholder\n",
        "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
        "        self.alpha = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='alpha')        \n",
        "        #tf.placeholder(dtype=tf.float64, shape=(None, 1), name='alpha')\n",
        "        # Put the time-dimension upfront for the scan operator\n",
        "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')        \n",
        "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
        "        self.h_0 = tf.multiply(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)),name='h_0')        \n",
        "        # Perform the scan operator\n",
        "        self.h_t_transposed = tf.scan(self.call, self.x_t, initializer=self.h_0, name='h_t_transposed')        \n",
        "        # Transpose the result back\n",
        "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
        "        self.W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
        "        self.b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
        "        self._output = tf.multiply(self.h_t, self.W_output) + self.b_output \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.Uh = self.add_weight(name='Uh',\n",
        "                                      shape=(self.hidden_size, self.hidden_size),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "\n",
        "        self.Wh = self.add_weight(name='Wh',\n",
        "                                           shape=(self.input_dimensions,self.hidden_size),\n",
        "                                           initializer='uniform',\n",
        "                                           trainable=True)\n",
        "        \n",
        "        self.W_output = self.add_weight(name='W_output',\n",
        "                                           shape=(hidden_size,1),\n",
        "                                           initializer='uniform',\n",
        "                                           trainable=True)\n",
        "        super(SimpleAlphaRNN, self).build(input_shape)  # Be sure to call this at the end\n",
        "        \n",
        "    def call(self, h_tml, x_t):\n",
        "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(h_tml, self.Uh) + self.bh)\n",
        "        return h_proposal\n",
        "        \n",
        "    #def call2(self, h_tml, x_t):\n",
        "        # Update hidden state h_t\n",
        "    #    h_proposal = np.math.tanh(K.dot(x_t, self.Wh) + K.dot(h_tml, self.Uh) + self.bh)\n",
        "        # Update the (smoothed) hidden state with exponential smoothing\n",
        "    #    h_t = K.dot(1 - np.math.tanh(self.alpha), h_tm1) + K.dot(np.math.tanh(self.alpha), h_proposal)\n",
        "    #    return h_t \n",
        "\n",
        "    #def call(self, x):\n",
        "        #return K.dot(x, self.kernel)\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "\n",
        "input_dimension = 1\n",
        "hidden_units = 5\n",
        "#model = Sequential()\n",
        "model_in = Input(shape=(input_dimension, hidden_units))  \n",
        "#model_out = \n",
        "#model.\n",
        "#model.add(SimpleAlphaRNN(input_dimension,hidden_units,1))\n",
        "#model_layer = layers.Layer(SimpleAlphaRNN(input_dimension,hidden_units))(input)\n",
        "#model.add(model_layer)\n",
        "#model.add(Dense(1))\n",
        "model_out = SimpleAlphaRNN(input_dimension,hidden_units)  #(model_in)\n",
        "#layers.Dense(Input, activation='sigmoid')(Input)\n",
        "model =  Model(inputs=[model_in], outputs=[model_out])\n",
        "adamOpt = optimizers.Adam(lr=1e-4)\n",
        "model.compile(optimizer=adamOpt, loss='mean_squared_error')\n",
        "hist = model.fit(x_train_reg,y_train_reg,verbose=2, epochs=2000, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           vol\n",
            "0     0.231760\n",
            "1     0.234163\n",
            "2     0.236118\n",
            "3     0.238111\n",
            "4     0.240346\n",
            "5     0.242620\n",
            "6     0.244921\n",
            "7     0.246864\n",
            "8     0.249513\n",
            "9     0.251972\n",
            "10    0.254264\n",
            "11    0.256771\n",
            "12    0.259587\n",
            "13    0.261881\n",
            "14    0.264645\n",
            "15    0.267366\n",
            "16    0.270018\n",
            "17    0.272699\n",
            "18    0.275689\n",
            "19    0.278759\n",
            "20    0.281803\n",
            "21    0.284439\n",
            "22    0.287653\n",
            "23    0.290854\n",
            "24    0.294061\n",
            "25    0.297234\n",
            "26    0.300527\n",
            "27    0.304021\n",
            "28    0.307590\n",
            "29    0.311187\n",
            "...        ...\n",
            "1546 -0.011436\n",
            "1547 -0.011533\n",
            "1548 -0.011484\n",
            "1549 -0.011600\n",
            "1550 -0.011823\n",
            "1551 -0.011848\n",
            "1552 -0.011636\n",
            "1553 -0.011656\n",
            "1554 -0.011709\n",
            "1555 -0.011783\n",
            "1556 -0.011949\n",
            "1557 -0.011762\n",
            "1558 -0.011780\n",
            "1559 -0.011837\n",
            "1560 -0.011790\n",
            "1561 -0.011746\n",
            "1562 -0.012055\n",
            "1563 -0.012101\n",
            "1564 -0.012253\n",
            "1565 -0.011793\n",
            "1566 -0.011939\n",
            "1567 -0.012213\n",
            "1568 -0.012138\n",
            "1569 -0.012121\n",
            "1570 -0.012194\n",
            "1571  0.149624\n",
            "1572  0.153028\n",
            "1573  0.156368\n",
            "1574  0.159760\n",
            "1575  0.163050\n",
            "\n",
            "[1576 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <__main__.SimpleAlphaRNN object at 0x103272be0>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d7006a51b487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleAlphaRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(model_in)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m#layers.Dense(Input, activation='sigmoid')(Input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0madamOpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madamOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                  \u001b[0;34m'the output of a Keras `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                                  \u001b[0;34m'(thus holding past layer metadata). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                  'Found: ' + str(x))\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         self._compute_previous_mask = (\n",
            "\u001b[0;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <__main__.SimpleAlphaRNN object at 0x103272be0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrxZDZuiWlYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}