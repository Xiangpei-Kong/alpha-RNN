{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "KerasLayer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/KerasLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmsnQfHWWlX-",
        "colab_type": "code",
        "outputId": "40e253b1-1b04-470e-a4a3-e79c648e990a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 14 23:15:31 2020\n",
        "\n",
        "@author: macbookpro\n",
        "\"\"\"\n",
        " \n",
        "\n",
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import layers\n",
        "#from alphaRNN import *\n",
        "from keras import *\n",
        "from keras.legacy import interfaces\n",
        "# To make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)\n",
        "    \n",
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)\n",
        "\n",
        "p = 30 # the number of lags (in both the data and the models)\n",
        "vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]\n",
        "\n",
        "df = pd.DataFrame(vols, columns=['vol'])\n",
        "\n",
        "use_features = ['vol'] \n",
        "target = 'vol'\n",
        "n_steps = 10 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(len(df)*train_weight)\n",
        "\n",
        "df_train = df[use_features].iloc[:split]\n",
        "print(df_train)\n",
        "df_test = df[use_features].iloc[split:]\n",
        "\n",
        "def get_lagged_features(value, n_steps):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps, 0, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)\n",
        "\n",
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "#x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "#x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "#y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "#x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "#y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "#x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "#x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "#y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "#x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "#y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))\n",
        "\n",
        "#train_batch_size = y_train_reg.shape[0]\n",
        "#test_batch_size = y_test_reg.shape[0]    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           vol\n",
            "0     0.231685\n",
            "1     0.233905\n",
            "2     0.236207\n",
            "3     0.238376\n",
            "4     0.240477\n",
            "...        ...\n",
            "1571  0.149595\n",
            "1572  0.152918\n",
            "1573  0.156165\n",
            "1574  0.159597\n",
            "1575  0.163079\n",
            "\n",
            "[1576 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QInb-xC2MtAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY1C3s4TLSW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphaRNNCell(Layer):\n",
        "    \"\"\"Cell class for AlphaRNN.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        super(AlphaRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        \n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        prev_output = states[0]\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(prev_output),\n",
        "                self.recurrent_dropout,\n",
        "                training=training)\n",
        "\n",
        "        dp_mask = self._dropout_mask\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if dp_mask is not None:\n",
        "            h = K.dot(inputs * dp_mask, self.kernel)\n",
        "        else:\n",
        "            h = K.dot(inputs, self.kernel)\n",
        "        if self.bias is not None:\n",
        "            h = K.bias_add(h, self.bias)\n",
        "\n",
        "        if rec_dp_mask is not None:\n",
        "            prev_output *= rec_dp_mask\n",
        "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        output = K.tanh(self.alpha)* output + 1-K.tanh(self.alpha)* prev_output\n",
        "        # Properly set learning phase on output tensor.\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                output._uses_learning_phase = True\n",
        "        return output, [output]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphaRNN(keras.layers.RNN):\n",
        "    \"\"\"Fully-connected AlphaRNN where the output is to be fed back to input.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if 'implementation' in kwargs:\n",
        "            kwargs.pop('implementation')\n",
        "            warnings.warn('The `implementation` argument '\n",
        "                          'in `SimpleRNN` has been deprecated. '\n",
        "                          'Please remove it from your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphaRNNCell(units,\n",
        "                             activation=activation,\n",
        "                             use_bias=use_bias,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             recurrent_initializer=recurrent_initializer,\n",
        "                             bias_initializer=bias_initializer,\n",
        "                             kernel_regularizer=kernel_regularizer,\n",
        "                             recurrent_regularizer=recurrent_regularizer,\n",
        "                             bias_regularizer=bias_regularizer,\n",
        "                             kernel_constraint=kernel_constraint,\n",
        "                             recurrent_constraint=recurrent_constraint,\n",
        "                             bias_constraint=bias_constraint,\n",
        "                             dropout=dropout,\n",
        "                             recurrent_dropout=recurrent_dropout)\n",
        "        super(AlphaRNN, self).__init__(cell,\n",
        "                                        return_sequences=return_sequences,\n",
        "                                        return_state=return_state,\n",
        "                                        go_backwards=go_backwards,\n",
        "                                        stateful=stateful,\n",
        "                                        unroll=unroll,\n",
        "                                        **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphaRNN, self).call(inputs,\n",
        "                                           mask=mask,\n",
        "                                           training=training,\n",
        "                                           initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config:\n",
        "            config.pop('implementation')\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdobsJiulG0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphatRNNCell(Layer):\n",
        "    \"\"\"Cell class for the AlphatRNN layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        reset_after: GRU convention (whether to apply reset gate after or\n",
        "            before matrix multiplication). False = \"before\" (default),\n",
        "            True = \"after\" (CuDNN compatible).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 **kwargs):\n",
        "        super(AlphatCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 2),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias_shape = (2, 2 * self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            \n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # alpha\n",
        "        self.kernel_alpha = self.kernel[:, :self.units]\n",
        "        self.recurrent_kernel_alpha = self.recurrent_kernel[:, :self.units]\n",
        "        # recurrnce\n",
        "        self.kernel_h = self.kernel[:, self.units:]\n",
        "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_alpha = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units:]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "            \n",
        "            self.recurrent_bias_alpha = self.recurrent_bias[:self.units]    \n",
        "            self.recurrent_bias_h = self.recurrent_bias[self.units:]\n",
        "        else:\n",
        "            self.input_bias_alpha = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_alpha = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_alpha = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_alpha = input\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_alpha = K.dot(inputs_alpha, self.kernel_alpha)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_alpha = K.bias_add(x_alpha, self.input_bias_alpha)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_alpha = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_alpha = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_alpha = K.dot(h_tm1_alpha, self.recurrent_kernel_alpha)\n",
        "           \n",
        "            if self.use_bias:\n",
        "                recurrent_alpha = K.bias_add(recurrent_alpha, self.recurrent_bias_alpha)\n",
        "\n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "           \n",
        "            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n",
        "            if self.use_bias:\n",
        "                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
        "            x_alpha = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            \n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                  matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "            \n",
        "            recurrent_alpha = matrix_inner[:, :self.units] \n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "            recurrent_h = matrix_inner[:, self.units: 2 * self.units]  \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        h = alpha * h_tm1 + (1 - alpha) * hh\n",
        "\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphatRNN(keras.layers.RNN):\n",
        "    \"\"\"Alpha_t RNN\n",
        "    There are two variants. The default one is based on 1406.1078v3 and\n",
        "    has reset gate applied to hidden state before matrix multiplication. The\n",
        "    other one is based on original 1406.1078v1 and has the order reversed.\n",
        "    The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
        "    inference on CPU. Thus it has separate biases for `kernel` and\n",
        "    `recurrent_kernel`. Use `'reset_after'=True` and\n",
        "    `recurrent_activation='sigmoid'`.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: sigmoid (`sigmoid`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        \n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphatCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation)             \n",
        "        super(AlphatRNN, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphatRNN, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Ls0KyNONY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSSokmMsOa1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9YQuMUg0bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=1e-4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMbxTnZCUdJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6cd4437-ac2f-4fe0-b70f-94dccd816e06"
      },
      "source": [
        "hidden_units=5\n",
        "l1_reg=0\n",
        "reg_model = Sequential()\n",
        "reg_model.add(AlphaRNN(hidden_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1566/1566 [==============================] - 1s 759us/step - loss: 0.0833\n",
            "Epoch 2/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0472\n",
            "Epoch 3/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0300\n",
            "Epoch 4/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0242\n",
            "Epoch 5/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0231\n",
            "Epoch 6/2000\n",
            "1566/1566 [==============================] - 0s 19us/step - loss: 0.0231\n",
            "Epoch 7/2000\n",
            " 100/1566 [>.............................] - ETA: 0s - loss: 0.0236"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0230\n",
            "Epoch 8/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0230\n",
            "Epoch 9/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0230\n",
            "Epoch 10/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0230\n",
            "Epoch 11/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0230\n",
            "Epoch 12/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0230\n",
            "Epoch 13/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0230\n",
            "Epoch 14/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0230\n",
            "Epoch 15/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0230\n",
            "Epoch 16/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0230\n",
            "Epoch 17/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0230\n",
            "Epoch 18/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0229\n",
            "Epoch 19/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0229\n",
            "Epoch 20/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0229\n",
            "Epoch 21/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0229\n",
            "Epoch 22/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0229\n",
            "Epoch 23/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0229\n",
            "Epoch 24/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0228\n",
            "Epoch 25/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0228\n",
            "Epoch 26/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0228\n",
            "Epoch 27/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0227\n",
            "Epoch 28/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0227\n",
            "Epoch 29/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0227\n",
            "Epoch 30/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0226\n",
            "Epoch 31/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0226\n",
            "Epoch 32/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0225\n",
            "Epoch 33/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0225\n",
            "Epoch 34/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0224\n",
            "Epoch 35/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0224\n",
            "Epoch 36/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0223\n",
            "Epoch 37/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0222\n",
            "Epoch 38/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0221\n",
            "Epoch 39/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0221\n",
            "Epoch 40/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0219\n",
            "Epoch 41/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0218\n",
            "Epoch 42/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0216\n",
            "Epoch 43/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0215\n",
            "Epoch 44/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0213\n",
            "Epoch 45/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0211\n",
            "Epoch 46/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0209\n",
            "Epoch 47/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0207\n",
            "Epoch 48/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0204\n",
            "Epoch 49/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0200\n",
            "Epoch 50/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0197\n",
            "Epoch 51/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0192\n",
            "Epoch 52/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0187\n",
            "Epoch 53/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0181\n",
            "Epoch 54/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0174\n",
            "Epoch 55/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 0.0166\n",
            "Epoch 56/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0156\n",
            "Epoch 57/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 0.0143\n",
            "Epoch 58/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0128\n",
            "Epoch 59/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0109\n",
            "Epoch 60/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0087\n",
            "Epoch 61/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0062\n",
            "Epoch 62/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0036\n",
            "Epoch 63/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 0.0018\n",
            "Epoch 64/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0013\n",
            "Epoch 65/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0011\n",
            "Epoch 66/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 0.0011\n",
            "Epoch 67/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0011\n",
            "Epoch 68/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 0.0010\n",
            "Epoch 69/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 0.0010\n",
            "Epoch 70/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 9.8806e-04\n",
            "Epoch 71/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 9.8057e-04\n",
            "Epoch 72/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 9.5507e-04\n",
            "Epoch 73/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 9.3701e-04\n",
            "Epoch 74/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 9.1968e-04\n",
            "Epoch 75/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 9.0592e-04\n",
            "Epoch 76/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 8.8651e-04\n",
            "Epoch 77/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 8.7230e-04\n",
            "Epoch 78/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 8.5835e-04\n",
            "Epoch 79/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 8.5013e-04\n",
            "Epoch 80/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 8.3460e-04\n",
            "Epoch 81/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 8.2514e-04\n",
            "Epoch 82/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 8.1425e-04\n",
            "Epoch 83/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 8.0476e-04\n",
            "Epoch 84/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 7.9700e-04\n",
            "Epoch 85/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 7.8707e-04\n",
            "Epoch 86/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.7894e-04\n",
            "Epoch 87/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 7.7297e-04\n",
            "Epoch 88/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 7.6397e-04\n",
            "Epoch 89/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.5781e-04\n",
            "Epoch 90/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 7.4949e-04\n",
            "Epoch 91/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 7.4354e-04\n",
            "Epoch 92/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 7.3721e-04\n",
            "Epoch 93/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.3291e-04\n",
            "Epoch 94/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 7.2537e-04\n",
            "Epoch 95/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 7.2150e-04\n",
            "Epoch 96/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.1593e-04\n",
            "Epoch 97/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.1540e-04\n",
            "Epoch 98/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 7.0744e-04\n",
            "Epoch 99/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 7.0033e-04\n",
            "Epoch 100/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.9515e-04\n",
            "Epoch 101/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.9064e-04\n",
            "Epoch 102/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 6.8761e-04\n",
            "Epoch 103/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.8212e-04\n",
            "Epoch 104/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.7760e-04\n",
            "Epoch 105/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.7378e-04\n",
            "Epoch 106/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.6973e-04\n",
            "Epoch 107/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.6541e-04\n",
            "Epoch 108/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.6251e-04\n",
            "Epoch 109/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.5792e-04\n",
            "Epoch 110/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.5425e-04\n",
            "Epoch 111/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.5054e-04\n",
            "Epoch 112/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.4749e-04\n",
            "Epoch 113/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.4515e-04\n",
            "Epoch 114/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 6.3940e-04\n",
            "Epoch 115/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.3705e-04\n",
            "Epoch 116/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.3340e-04\n",
            "Epoch 117/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.3370e-04\n",
            "Epoch 118/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.2748e-04\n",
            "Epoch 119/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 6.2384e-04\n",
            "Epoch 120/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.2516e-04\n",
            "Epoch 121/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 6.2171e-04\n",
            "Epoch 122/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.1992e-04\n",
            "Epoch 123/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 6.1363e-04\n",
            "Epoch 124/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 6.1045e-04\n",
            "Epoch 125/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 6.1091e-04\n",
            "Epoch 126/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 6.1124e-04\n",
            "Epoch 127/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.0438e-04\n",
            "Epoch 128/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 6.0035e-04\n",
            "Epoch 129/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.9765e-04\n",
            "Epoch 130/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.9615e-04\n",
            "Epoch 131/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.9385e-04\n",
            "Epoch 132/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.9264e-04\n",
            "Epoch 133/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.9001e-04\n",
            "Epoch 134/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.8761e-04\n",
            "Epoch 135/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.8583e-04\n",
            "Epoch 136/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.8423e-04\n",
            "Epoch 137/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.8163e-04\n",
            "Epoch 138/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.8009e-04\n",
            "Epoch 139/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.7785e-04\n",
            "Epoch 140/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.7970e-04\n",
            "Epoch 141/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.7666e-04\n",
            "Epoch 142/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.7497e-04\n",
            "Epoch 143/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.7211e-04\n",
            "Epoch 144/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.7331e-04\n",
            "Epoch 145/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.6800e-04\n",
            "Epoch 146/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.6588e-04\n",
            "Epoch 147/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.6467e-04\n",
            "Epoch 148/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.6263e-04\n",
            "Epoch 149/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.6107e-04\n",
            "Epoch 150/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.5952e-04\n",
            "Epoch 151/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.5950e-04\n",
            "Epoch 152/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.5676e-04\n",
            "Epoch 153/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.5577e-04\n",
            "Epoch 154/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.5376e-04\n",
            "Epoch 155/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.5280e-04\n",
            "Epoch 156/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.5197e-04\n",
            "Epoch 157/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.5014e-04\n",
            "Epoch 158/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.5020e-04\n",
            "Epoch 159/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.4830e-04\n",
            "Epoch 160/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.4668e-04\n",
            "Epoch 161/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.4547e-04\n",
            "Epoch 162/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.4548e-04\n",
            "Epoch 163/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.4338e-04\n",
            "Epoch 164/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.4150e-04\n",
            "Epoch 165/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.4237e-04\n",
            "Epoch 166/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.3981e-04\n",
            "Epoch 167/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.4218e-04\n",
            "Epoch 168/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.4168e-04\n",
            "Epoch 169/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.3793e-04\n",
            "Epoch 170/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.3558e-04\n",
            "Epoch 171/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.3446e-04\n",
            "Epoch 172/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.3535e-04\n",
            "Epoch 173/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.3398e-04\n",
            "Epoch 174/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.3245e-04\n",
            "Epoch 175/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.3040e-04\n",
            "Epoch 176/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.3032e-04\n",
            "Epoch 177/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 5.2813e-04\n",
            "Epoch 178/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.2919e-04\n",
            "Epoch 179/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.2632e-04\n",
            "Epoch 180/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.2588e-04\n",
            "Epoch 181/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.2519e-04\n",
            "Epoch 182/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 5.2601e-04\n",
            "Epoch 183/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.2389e-04\n",
            "Epoch 184/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.2321e-04\n",
            "Epoch 185/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.2265e-04\n",
            "Epoch 186/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.2045e-04\n",
            "Epoch 187/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.2163e-04\n",
            "Epoch 188/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.1903e-04\n",
            "Epoch 189/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.1876e-04\n",
            "Epoch 190/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 5.1730e-04\n",
            "Epoch 191/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.1713e-04\n",
            "Epoch 192/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.1743e-04\n",
            "Epoch 193/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.1584e-04\n",
            "Epoch 194/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.1865e-04\n",
            "Epoch 195/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 5.1449e-04\n",
            "Epoch 196/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.1411e-04\n",
            "Epoch 197/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1459e-04\n",
            "Epoch 198/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.1173e-04\n",
            "Epoch 199/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.1311e-04\n",
            "Epoch 200/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.1367e-04\n",
            "Epoch 201/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.1150e-04\n",
            "Epoch 202/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.1143e-04\n",
            "Epoch 203/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 5.1141e-04\n",
            "Epoch 204/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.0845e-04\n",
            "Epoch 205/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.0852e-04\n",
            "Epoch 206/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0862e-04\n",
            "Epoch 207/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.0805e-04\n",
            "Epoch 208/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.0703e-04\n",
            "Epoch 209/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.0607e-04\n",
            "Epoch 210/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.0540e-04\n",
            "Epoch 211/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.0967e-04\n",
            "Epoch 212/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0862e-04\n",
            "Epoch 213/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 5.0538e-04\n",
            "Epoch 214/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0410e-04\n",
            "Epoch 215/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0631e-04\n",
            "Epoch 216/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0457e-04\n",
            "Epoch 217/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0274e-04\n",
            "Epoch 218/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.0326e-04\n",
            "Epoch 219/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0211e-04\n",
            "Epoch 220/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0198e-04\n",
            "Epoch 221/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 5.0193e-04\n",
            "Epoch 222/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0018e-04\n",
            "Epoch 223/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0102e-04\n",
            "Epoch 224/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9951e-04\n",
            "Epoch 225/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0179e-04\n",
            "Epoch 226/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9984e-04\n",
            "Epoch 227/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9977e-04\n",
            "Epoch 228/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 5.0052e-04\n",
            "Epoch 229/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9697e-04\n",
            "Epoch 230/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9695e-04\n",
            "Epoch 231/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9594e-04\n",
            "Epoch 232/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9816e-04\n",
            "Epoch 233/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9736e-04\n",
            "Epoch 234/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9528e-04\n",
            "Epoch 235/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9722e-04\n",
            "Epoch 236/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9546e-04\n",
            "Epoch 237/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9520e-04\n",
            "Epoch 238/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9445e-04\n",
            "Epoch 239/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9477e-04\n",
            "Epoch 240/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9523e-04\n",
            "Epoch 241/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9355e-04\n",
            "Epoch 242/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9632e-04\n",
            "Epoch 243/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9449e-04\n",
            "Epoch 244/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9277e-04\n",
            "Epoch 245/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9330e-04\n",
            "Epoch 246/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9138e-04\n",
            "Epoch 247/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9264e-04\n",
            "Epoch 248/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9493e-04\n",
            "Epoch 249/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9213e-04\n",
            "Epoch 250/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9141e-04\n",
            "Epoch 251/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9216e-04\n",
            "Epoch 252/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9245e-04\n",
            "Epoch 253/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9160e-04\n",
            "Epoch 254/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9165e-04\n",
            "Epoch 255/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9535e-04\n",
            "Epoch 256/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.9794e-04\n",
            "Epoch 257/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9215e-04\n",
            "Epoch 258/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9323e-04\n",
            "Epoch 259/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.9122e-04\n",
            "Epoch 260/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8929e-04\n",
            "Epoch 261/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8912e-04\n",
            "Epoch 262/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8975e-04\n",
            "Epoch 263/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9162e-04\n",
            "Epoch 264/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9151e-04\n",
            "Epoch 265/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8869e-04\n",
            "Epoch 266/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8747e-04\n",
            "Epoch 267/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8834e-04\n",
            "Epoch 268/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9017e-04\n",
            "Epoch 269/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8979e-04\n",
            "Epoch 270/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8817e-04\n",
            "Epoch 271/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9101e-04\n",
            "Epoch 272/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8824e-04\n",
            "Epoch 273/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8853e-04\n",
            "Epoch 274/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8907e-04\n",
            "Epoch 275/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8697e-04\n",
            "Epoch 276/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8817e-04\n",
            "Epoch 277/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8602e-04\n",
            "Epoch 278/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8587e-04\n",
            "Epoch 279/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8635e-04\n",
            "Epoch 280/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8762e-04\n",
            "Epoch 281/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8610e-04\n",
            "Epoch 282/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8655e-04\n",
            "Epoch 283/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8579e-04\n",
            "Epoch 284/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8701e-04\n",
            "Epoch 285/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8998e-04\n",
            "Epoch 286/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8707e-04\n",
            "Epoch 287/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8695e-04\n",
            "Epoch 288/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8690e-04\n",
            "Epoch 289/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8597e-04\n",
            "Epoch 290/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8996e-04\n",
            "Epoch 291/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8987e-04\n",
            "Epoch 292/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8519e-04\n",
            "Epoch 293/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8595e-04\n",
            "Epoch 294/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8461e-04\n",
            "Epoch 295/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8662e-04\n",
            "Epoch 296/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9082e-04\n",
            "Epoch 297/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8498e-04\n",
            "Epoch 298/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8616e-04\n",
            "Epoch 299/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8547e-04\n",
            "Epoch 300/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9036e-04\n",
            "Epoch 301/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8565e-04\n",
            "Epoch 302/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8919e-04\n",
            "Epoch 303/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8645e-04\n",
            "Epoch 304/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8563e-04\n",
            "Epoch 305/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8662e-04\n",
            "Epoch 306/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8536e-04\n",
            "Epoch 307/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8445e-04\n",
            "Epoch 308/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8517e-04\n",
            "Epoch 309/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8622e-04\n",
            "Epoch 310/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8643e-04\n",
            "Epoch 311/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9059e-04\n",
            "Epoch 312/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8736e-04\n",
            "Epoch 313/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8532e-04\n",
            "Epoch 314/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8346e-04\n",
            "Epoch 315/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8377e-04\n",
            "Epoch 316/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8436e-04\n",
            "Epoch 317/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8937e-04\n",
            "Epoch 318/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8986e-04\n",
            "Epoch 319/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8342e-04\n",
            "Epoch 320/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8318e-04\n",
            "Epoch 321/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8521e-04\n",
            "Epoch 322/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9227e-04\n",
            "Epoch 323/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8742e-04\n",
            "Epoch 324/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8585e-04\n",
            "Epoch 325/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8543e-04\n",
            "Epoch 326/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8497e-04\n",
            "Epoch 327/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8484e-04\n",
            "Epoch 328/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8330e-04\n",
            "Epoch 329/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8590e-04\n",
            "Epoch 330/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8411e-04\n",
            "Epoch 331/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8362e-04\n",
            "Epoch 332/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8497e-04\n",
            "Epoch 333/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8477e-04\n",
            "Epoch 334/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8427e-04\n",
            "Epoch 335/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8432e-04\n",
            "Epoch 336/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8604e-04\n",
            "Epoch 337/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9059e-04\n",
            "Epoch 338/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8490e-04\n",
            "Epoch 339/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8280e-04\n",
            "Epoch 340/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8795e-04\n",
            "Epoch 341/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8802e-04\n",
            "Epoch 342/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8679e-04\n",
            "Epoch 343/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8445e-04\n",
            "Epoch 344/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8250e-04\n",
            "Epoch 345/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8463e-04\n",
            "Epoch 346/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8347e-04\n",
            "Epoch 347/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8650e-04\n",
            "Epoch 348/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8611e-04\n",
            "Epoch 349/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8571e-04\n",
            "Epoch 350/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8365e-04\n",
            "Epoch 351/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8548e-04\n",
            "Epoch 352/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8830e-04\n",
            "Epoch 353/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8418e-04\n",
            "Epoch 354/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8567e-04\n",
            "Epoch 355/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8972e-04\n",
            "Epoch 356/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8869e-04\n",
            "Epoch 357/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8644e-04\n",
            "Epoch 358/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8856e-04\n",
            "Epoch 359/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8354e-04\n",
            "Epoch 360/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8236e-04\n",
            "Epoch 361/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8479e-04\n",
            "Epoch 362/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8711e-04\n",
            "Epoch 363/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8313e-04\n",
            "Epoch 364/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8239e-04\n",
            "Epoch 365/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8945e-04\n",
            "Epoch 366/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8402e-04\n",
            "Epoch 367/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8416e-04\n",
            "Epoch 368/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8451e-04\n",
            "Epoch 369/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8431e-04\n",
            "Epoch 370/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8246e-04\n",
            "Epoch 371/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8214e-04\n",
            "Epoch 372/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8319e-04\n",
            "Epoch 373/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8456e-04\n",
            "Epoch 374/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8501e-04\n",
            "Epoch 375/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8473e-04\n",
            "Epoch 376/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8492e-04\n",
            "Epoch 377/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8850e-04\n",
            "Epoch 378/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8697e-04\n",
            "Epoch 379/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8234e-04\n",
            "Epoch 380/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8204e-04\n",
            "Epoch 381/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8478e-04\n",
            "Epoch 382/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8256e-04\n",
            "Epoch 383/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8550e-04\n",
            "Epoch 384/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8267e-04\n",
            "Epoch 385/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8753e-04\n",
            "Epoch 386/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8521e-04\n",
            "Epoch 387/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8661e-04\n",
            "Epoch 388/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8466e-04\n",
            "Epoch 389/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8647e-04\n",
            "Epoch 390/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8393e-04\n",
            "Epoch 391/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8505e-04\n",
            "Epoch 392/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8298e-04\n",
            "Epoch 393/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8310e-04\n",
            "Epoch 394/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8484e-04\n",
            "Epoch 395/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8372e-04\n",
            "Epoch 396/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8404e-04\n",
            "Epoch 397/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8735e-04\n",
            "Epoch 398/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8462e-04\n",
            "Epoch 399/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8409e-04\n",
            "Epoch 400/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8430e-04\n",
            "Epoch 401/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8530e-04\n",
            "Epoch 402/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8326e-04\n",
            "Epoch 403/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8366e-04\n",
            "Epoch 404/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8304e-04\n",
            "Epoch 405/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8308e-04\n",
            "Epoch 406/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9075e-04\n",
            "Epoch 407/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8956e-04\n",
            "Epoch 408/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8485e-04\n",
            "Epoch 409/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8338e-04\n",
            "Epoch 410/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8299e-04\n",
            "Epoch 411/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8433e-04\n",
            "Epoch 412/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8186e-04\n",
            "Epoch 413/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8321e-04\n",
            "Epoch 414/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8218e-04\n",
            "Epoch 415/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8321e-04\n",
            "Epoch 416/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8764e-04\n",
            "Epoch 417/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8315e-04\n",
            "Epoch 418/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8291e-04\n",
            "Epoch 419/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8314e-04\n",
            "Epoch 420/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8374e-04\n",
            "Epoch 421/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8233e-04\n",
            "Epoch 422/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8605e-04\n",
            "Epoch 423/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8466e-04\n",
            "Epoch 424/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9168e-04\n",
            "Epoch 425/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8365e-04\n",
            "Epoch 426/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8652e-04\n",
            "Epoch 427/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8360e-04\n",
            "Epoch 428/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8166e-04\n",
            "Epoch 429/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8360e-04\n",
            "Epoch 430/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8253e-04\n",
            "Epoch 431/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8352e-04\n",
            "Epoch 432/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8489e-04\n",
            "Epoch 433/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8509e-04\n",
            "Epoch 434/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8307e-04\n",
            "Epoch 435/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8685e-04\n",
            "Epoch 436/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8368e-04\n",
            "Epoch 437/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8316e-04\n",
            "Epoch 438/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8770e-04\n",
            "Epoch 439/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8770e-04\n",
            "Epoch 440/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8565e-04\n",
            "Epoch 441/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8248e-04\n",
            "Epoch 442/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8367e-04\n",
            "Epoch 443/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8805e-04\n",
            "Epoch 444/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8362e-04\n",
            "Epoch 445/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8257e-04\n",
            "Epoch 446/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8600e-04\n",
            "Epoch 447/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8354e-04\n",
            "Epoch 448/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8314e-04\n",
            "Epoch 449/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8255e-04\n",
            "Epoch 450/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8878e-04\n",
            "Epoch 451/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8543e-04\n",
            "Epoch 452/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8781e-04\n",
            "Epoch 453/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8305e-04\n",
            "Epoch 454/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8871e-04\n",
            "Epoch 455/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8754e-04\n",
            "Epoch 456/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8353e-04\n",
            "Epoch 457/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8225e-04\n",
            "Epoch 458/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8385e-04\n",
            "Epoch 459/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8225e-04\n",
            "Epoch 460/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8315e-04\n",
            "Epoch 461/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8424e-04\n",
            "Epoch 462/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8637e-04\n",
            "Epoch 463/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 5.0050e-04\n",
            "Epoch 464/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9243e-04\n",
            "Epoch 465/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8607e-04\n",
            "Epoch 466/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8093e-04\n",
            "Epoch 467/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8369e-04\n",
            "Epoch 468/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8398e-04\n",
            "Epoch 469/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8793e-04\n",
            "Epoch 470/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8574e-04\n",
            "Epoch 471/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8295e-04\n",
            "Epoch 472/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8286e-04\n",
            "Epoch 473/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8189e-04\n",
            "Epoch 474/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8153e-04\n",
            "Epoch 475/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8213e-04\n",
            "Epoch 476/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8730e-04\n",
            "Epoch 477/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8903e-04\n",
            "Epoch 478/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8526e-04\n",
            "Epoch 479/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8276e-04\n",
            "Epoch 480/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8367e-04\n",
            "Epoch 481/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8235e-04\n",
            "Epoch 482/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8674e-04\n",
            "Epoch 483/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8616e-04\n",
            "Epoch 484/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8572e-04\n",
            "Epoch 485/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8692e-04\n",
            "Epoch 486/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9216e-04\n",
            "Epoch 487/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9272e-04\n",
            "Epoch 488/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8705e-04\n",
            "Epoch 489/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8234e-04\n",
            "Epoch 490/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8161e-04\n",
            "Epoch 491/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8474e-04\n",
            "Epoch 492/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8309e-04\n",
            "Epoch 493/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8192e-04\n",
            "Epoch 494/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8238e-04\n",
            "Epoch 495/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8425e-04\n",
            "Epoch 496/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9008e-04\n",
            "Epoch 497/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8333e-04\n",
            "Epoch 498/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8697e-04\n",
            "Epoch 499/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8686e-04\n",
            "Epoch 500/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8505e-04\n",
            "Epoch 501/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8262e-04\n",
            "Epoch 502/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8422e-04\n",
            "Epoch 503/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8097e-04\n",
            "Epoch 504/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8535e-04\n",
            "Epoch 505/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8489e-04\n",
            "Epoch 506/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8509e-04\n",
            "Epoch 507/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.9060e-04\n",
            "Epoch 508/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8691e-04\n",
            "Epoch 509/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8226e-04\n",
            "Epoch 510/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8189e-04\n",
            "Epoch 511/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8187e-04\n",
            "Epoch 512/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8437e-04\n",
            "Epoch 513/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8308e-04\n",
            "Epoch 514/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8140e-04\n",
            "Epoch 515/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8247e-04\n",
            "Epoch 516/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8679e-04\n",
            "Epoch 517/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8329e-04\n",
            "Epoch 518/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8853e-04\n",
            "Epoch 519/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8427e-04\n",
            "Epoch 520/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8323e-04\n",
            "Epoch 521/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8458e-04\n",
            "Epoch 522/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8270e-04\n",
            "Epoch 523/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8161e-04\n",
            "Epoch 524/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8377e-04\n",
            "Epoch 525/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8509e-04\n",
            "Epoch 526/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8554e-04\n",
            "Epoch 527/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8810e-04\n",
            "Epoch 528/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8673e-04\n",
            "Epoch 529/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8157e-04\n",
            "Epoch 530/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8190e-04\n",
            "Epoch 531/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8791e-04\n",
            "Epoch 532/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8340e-04\n",
            "Epoch 533/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8141e-04\n",
            "Epoch 534/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8380e-04\n",
            "Epoch 535/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8474e-04\n",
            "Epoch 536/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8441e-04\n",
            "Epoch 537/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8424e-04\n",
            "Epoch 538/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8651e-04\n",
            "Epoch 539/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8241e-04\n",
            "Epoch 540/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8346e-04\n",
            "Epoch 541/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8637e-04\n",
            "Epoch 542/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8256e-04\n",
            "Epoch 543/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8932e-04\n",
            "Epoch 544/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8283e-04\n",
            "Epoch 545/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8174e-04\n",
            "Epoch 546/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8228e-04\n",
            "Epoch 547/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8464e-04\n",
            "Epoch 548/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8357e-04\n",
            "Epoch 549/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8386e-04\n",
            "Epoch 550/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8534e-04\n",
            "Epoch 551/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8382e-04\n",
            "Epoch 552/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8312e-04\n",
            "Epoch 553/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8232e-04\n",
            "Epoch 554/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8775e-04\n",
            "Epoch 555/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8245e-04\n",
            "Epoch 556/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8980e-04\n",
            "Epoch 557/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9039e-04\n",
            "Epoch 558/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9067e-04\n",
            "Epoch 559/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8400e-04\n",
            "Epoch 560/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8392e-04\n",
            "Epoch 561/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8465e-04\n",
            "Epoch 562/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8440e-04\n",
            "Epoch 563/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8190e-04\n",
            "Epoch 564/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9063e-04\n",
            "Epoch 565/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8473e-04\n",
            "Epoch 566/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8477e-04\n",
            "Epoch 567/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8455e-04\n",
            "Epoch 568/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8797e-04\n",
            "Epoch 569/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8470e-04\n",
            "Epoch 570/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8585e-04\n",
            "Epoch 571/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8660e-04\n",
            "Epoch 572/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9078e-04\n",
            "Epoch 573/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8447e-04\n",
            "Epoch 574/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8591e-04\n",
            "Epoch 575/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9323e-04\n",
            "Epoch 576/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9328e-04\n",
            "Epoch 577/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8486e-04\n",
            "Epoch 578/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8382e-04\n",
            "Epoch 579/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8458e-04\n",
            "Epoch 580/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8502e-04\n",
            "Epoch 581/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8285e-04\n",
            "Epoch 582/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8251e-04\n",
            "Epoch 583/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9024e-04\n",
            "Epoch 584/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8478e-04\n",
            "Epoch 585/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8398e-04\n",
            "Epoch 586/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9004e-04\n",
            "Epoch 587/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9065e-04\n",
            "Epoch 588/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8512e-04\n",
            "Epoch 589/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8447e-04\n",
            "Epoch 590/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8318e-04\n",
            "Epoch 591/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8516e-04\n",
            "Epoch 592/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8418e-04\n",
            "Epoch 593/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8424e-04\n",
            "Epoch 594/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8571e-04\n",
            "Epoch 595/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8583e-04\n",
            "Epoch 596/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8114e-04\n",
            "Epoch 597/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8193e-04\n",
            "Epoch 598/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8539e-04\n",
            "Epoch 599/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8298e-04\n",
            "Epoch 600/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8365e-04\n",
            "Epoch 601/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8177e-04\n",
            "Epoch 602/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8486e-04\n",
            "Epoch 603/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8099e-04\n",
            "Epoch 604/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8368e-04\n",
            "Epoch 605/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8557e-04\n",
            "Epoch 606/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8214e-04\n",
            "Epoch 607/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8149e-04\n",
            "Epoch 608/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8183e-04\n",
            "Epoch 609/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8600e-04\n",
            "Epoch 610/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8808e-04\n",
            "Epoch 611/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8210e-04\n",
            "Epoch 612/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8158e-04\n",
            "Epoch 613/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8224e-04\n",
            "Epoch 614/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8312e-04\n",
            "Epoch 615/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8169e-04\n",
            "Epoch 616/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8408e-04\n",
            "Epoch 617/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8352e-04\n",
            "Epoch 618/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8095e-04\n",
            "Epoch 619/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8646e-04\n",
            "Epoch 620/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8481e-04\n",
            "Epoch 621/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9250e-04\n",
            "Epoch 622/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8268e-04\n",
            "Epoch 623/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8258e-04\n",
            "Epoch 624/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8630e-04\n",
            "Epoch 625/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8328e-04\n",
            "Epoch 626/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8439e-04\n",
            "Epoch 627/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8682e-04\n",
            "Epoch 628/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8963e-04\n",
            "Epoch 629/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8582e-04\n",
            "Epoch 630/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8313e-04\n",
            "Epoch 631/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8756e-04\n",
            "Epoch 632/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9261e-04\n",
            "Epoch 633/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8998e-04\n",
            "Epoch 634/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8378e-04\n",
            "Epoch 635/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8216e-04\n",
            "Epoch 636/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8875e-04\n",
            "Epoch 637/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8486e-04\n",
            "Epoch 638/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8459e-04\n",
            "Epoch 639/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8143e-04\n",
            "Epoch 640/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8183e-04\n",
            "Epoch 641/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8454e-04\n",
            "Epoch 642/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8267e-04\n",
            "Epoch 643/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8593e-04\n",
            "Epoch 644/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8629e-04\n",
            "Epoch 645/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9028e-04\n",
            "Epoch 646/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8804e-04\n",
            "Epoch 647/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9064e-04\n",
            "Epoch 648/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8385e-04\n",
            "Epoch 649/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8460e-04\n",
            "Epoch 650/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8213e-04\n",
            "Epoch 651/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8633e-04\n",
            "Epoch 652/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9066e-04\n",
            "Epoch 653/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8700e-04\n",
            "Epoch 654/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8735e-04\n",
            "Epoch 655/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8177e-04\n",
            "Epoch 656/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8890e-04\n",
            "Epoch 657/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8643e-04\n",
            "Epoch 658/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9080e-04\n",
            "Epoch 659/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8099e-04\n",
            "Epoch 660/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8118e-04\n",
            "Epoch 661/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8363e-04\n",
            "Epoch 662/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8567e-04\n",
            "Epoch 663/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.9102e-04\n",
            "Epoch 664/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8498e-04\n",
            "Epoch 665/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8299e-04\n",
            "Epoch 666/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8486e-04\n",
            "Epoch 667/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8170e-04\n",
            "Epoch 668/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8225e-04\n",
            "Epoch 669/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8109e-04\n",
            "Epoch 670/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8358e-04\n",
            "Epoch 671/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8145e-04\n",
            "Epoch 672/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8391e-04\n",
            "Epoch 673/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8314e-04\n",
            "Epoch 674/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8106e-04\n",
            "Epoch 675/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8846e-04\n",
            "Epoch 676/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9167e-04\n",
            "Epoch 677/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8197e-04\n",
            "Epoch 678/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8888e-04\n",
            "Epoch 679/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8303e-04\n",
            "Epoch 680/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8336e-04\n",
            "Epoch 681/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8121e-04\n",
            "Epoch 682/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8474e-04\n",
            "Epoch 683/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8347e-04\n",
            "Epoch 684/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8171e-04\n",
            "Epoch 685/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8081e-04\n",
            "Epoch 686/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8566e-04\n",
            "Epoch 687/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8879e-04\n",
            "Epoch 688/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8526e-04\n",
            "Epoch 689/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8328e-04\n",
            "Epoch 690/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8213e-04\n",
            "Epoch 691/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8414e-04\n",
            "Epoch 692/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8320e-04\n",
            "Epoch 693/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8195e-04\n",
            "Epoch 694/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8294e-04\n",
            "Epoch 695/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8530e-04\n",
            "Epoch 696/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8380e-04\n",
            "Epoch 697/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8235e-04\n",
            "Epoch 698/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8207e-04\n",
            "Epoch 699/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8498e-04\n",
            "Epoch 700/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8967e-04\n",
            "Epoch 701/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8708e-04\n",
            "Epoch 702/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8128e-04\n",
            "Epoch 703/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8363e-04\n",
            "Epoch 704/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8506e-04\n",
            "Epoch 705/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9108e-04\n",
            "Epoch 706/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8622e-04\n",
            "Epoch 707/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8571e-04\n",
            "Epoch 708/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8565e-04\n",
            "Epoch 709/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8135e-04\n",
            "Epoch 710/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8683e-04\n",
            "Epoch 711/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8180e-04\n",
            "Epoch 712/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8202e-04\n",
            "Epoch 713/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9308e-04\n",
            "Epoch 714/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8448e-04\n",
            "Epoch 715/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8182e-04\n",
            "Epoch 716/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8197e-04\n",
            "Epoch 717/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8261e-04\n",
            "Epoch 718/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8815e-04\n",
            "Epoch 719/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8531e-04\n",
            "Epoch 720/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8258e-04\n",
            "Epoch 721/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8253e-04\n",
            "Epoch 722/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8291e-04\n",
            "Epoch 723/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8440e-04\n",
            "Epoch 724/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8826e-04\n",
            "Epoch 725/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8722e-04\n",
            "Epoch 726/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9037e-04\n",
            "Epoch 727/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9106e-04\n",
            "Epoch 728/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8705e-04\n",
            "Epoch 729/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8223e-04\n",
            "Epoch 730/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8197e-04\n",
            "Epoch 731/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8152e-04\n",
            "Epoch 732/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8536e-04\n",
            "Epoch 733/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8305e-04\n",
            "Epoch 734/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8556e-04\n",
            "Epoch 735/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8506e-04\n",
            "Epoch 736/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8274e-04\n",
            "Epoch 737/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8416e-04\n",
            "Epoch 738/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8542e-04\n",
            "Epoch 739/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8206e-04\n",
            "Epoch 740/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8181e-04\n",
            "Epoch 741/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8279e-04\n",
            "Epoch 742/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8116e-04\n",
            "Epoch 743/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8310e-04\n",
            "Epoch 744/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8423e-04\n",
            "Epoch 745/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8821e-04\n",
            "Epoch 746/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8250e-04\n",
            "Epoch 747/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8104e-04\n",
            "Epoch 748/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8188e-04\n",
            "Epoch 749/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8633e-04\n",
            "Epoch 750/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8102e-04\n",
            "Epoch 751/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8416e-04\n",
            "Epoch 752/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8238e-04\n",
            "Epoch 753/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8371e-04\n",
            "Epoch 754/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8287e-04\n",
            "Epoch 755/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8337e-04\n",
            "Epoch 756/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8546e-04\n",
            "Epoch 757/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8438e-04\n",
            "Epoch 758/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8146e-04\n",
            "Epoch 759/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8454e-04\n",
            "Epoch 760/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8318e-04\n",
            "Epoch 761/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8158e-04\n",
            "Epoch 762/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8112e-04\n",
            "Epoch 763/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8226e-04\n",
            "Epoch 764/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8693e-04\n",
            "Epoch 765/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8739e-04\n",
            "Epoch 766/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8336e-04\n",
            "Epoch 767/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8478e-04\n",
            "Epoch 768/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8413e-04\n",
            "Epoch 769/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8549e-04\n",
            "Epoch 770/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8296e-04\n",
            "Epoch 771/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8100e-04\n",
            "Epoch 772/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8660e-04\n",
            "Epoch 773/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8321e-04\n",
            "Epoch 774/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8626e-04\n",
            "Epoch 775/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8587e-04\n",
            "Epoch 776/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8767e-04\n",
            "Epoch 777/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8156e-04\n",
            "Epoch 778/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8892e-04\n",
            "Epoch 779/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8322e-04\n",
            "Epoch 780/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8217e-04\n",
            "Epoch 781/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8182e-04\n",
            "Epoch 782/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8175e-04\n",
            "Epoch 783/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8252e-04\n",
            "Epoch 784/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8475e-04\n",
            "Epoch 785/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8822e-04\n",
            "Epoch 786/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9014e-04\n",
            "Epoch 787/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8350e-04\n",
            "Epoch 788/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8110e-04\n",
            "Epoch 789/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8700e-04\n",
            "Epoch 790/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8857e-04\n",
            "Epoch 791/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8549e-04\n",
            "Epoch 792/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8285e-04\n",
            "Epoch 793/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8251e-04\n",
            "Epoch 794/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8772e-04\n",
            "Epoch 795/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8496e-04\n",
            "Epoch 796/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8172e-04\n",
            "Epoch 797/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8333e-04\n",
            "Epoch 798/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8238e-04\n",
            "Epoch 799/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8175e-04\n",
            "Epoch 800/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8340e-04\n",
            "Epoch 801/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8049e-04\n",
            "Epoch 802/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8279e-04\n",
            "Epoch 803/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8720e-04\n",
            "Epoch 804/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8327e-04\n",
            "Epoch 805/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8895e-04\n",
            "Epoch 806/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.9342e-04\n",
            "Epoch 807/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8970e-04\n",
            "Epoch 808/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8348e-04\n",
            "Epoch 809/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9087e-04\n",
            "Epoch 810/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8769e-04\n",
            "Epoch 811/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8549e-04\n",
            "Epoch 812/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8324e-04\n",
            "Epoch 813/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8405e-04\n",
            "Epoch 814/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8371e-04\n",
            "Epoch 815/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8458e-04\n",
            "Epoch 816/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8206e-04\n",
            "Epoch 817/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8020e-04\n",
            "Epoch 818/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8139e-04\n",
            "Epoch 819/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8214e-04\n",
            "Epoch 820/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8100e-04\n",
            "Epoch 821/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8081e-04\n",
            "Epoch 822/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8198e-04\n",
            "Epoch 823/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8393e-04\n",
            "Epoch 824/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8501e-04\n",
            "Epoch 825/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8773e-04\n",
            "Epoch 826/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8537e-04\n",
            "Epoch 827/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8547e-04\n",
            "Epoch 828/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8281e-04\n",
            "Epoch 829/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8095e-04\n",
            "Epoch 830/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8254e-04\n",
            "Epoch 831/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8129e-04\n",
            "Epoch 832/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8305e-04\n",
            "Epoch 833/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8411e-04\n",
            "Epoch 834/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8442e-04\n",
            "Epoch 835/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8433e-04\n",
            "Epoch 836/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8190e-04\n",
            "Epoch 837/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8061e-04\n",
            "Epoch 838/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8161e-04\n",
            "Epoch 839/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8459e-04\n",
            "Epoch 840/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8357e-04\n",
            "Epoch 841/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8119e-04\n",
            "Epoch 842/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8621e-04\n",
            "Epoch 843/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9043e-04\n",
            "Epoch 844/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8405e-04\n",
            "Epoch 845/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8488e-04\n",
            "Epoch 846/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8525e-04\n",
            "Epoch 847/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8106e-04\n",
            "Epoch 848/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8151e-04\n",
            "Epoch 849/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8564e-04\n",
            "Epoch 850/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8558e-04\n",
            "Epoch 851/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8256e-04\n",
            "Epoch 852/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8322e-04\n",
            "Epoch 853/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8238e-04\n",
            "Epoch 854/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8120e-04\n",
            "Epoch 855/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8053e-04\n",
            "Epoch 856/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8959e-04\n",
            "Epoch 857/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8745e-04\n",
            "Epoch 858/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8290e-04\n",
            "Epoch 859/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8519e-04\n",
            "Epoch 860/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8532e-04\n",
            "Epoch 861/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9853e-04\n",
            "Epoch 862/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8629e-04\n",
            "Epoch 863/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8629e-04\n",
            "Epoch 864/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.7979e-04\n",
            "Epoch 865/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8166e-04\n",
            "Epoch 866/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8495e-04\n",
            "Epoch 867/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8757e-04\n",
            "Epoch 868/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8968e-04\n",
            "Epoch 869/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8218e-04\n",
            "Epoch 870/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8154e-04\n",
            "Epoch 871/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8118e-04\n",
            "Epoch 872/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8153e-04\n",
            "Epoch 873/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8480e-04\n",
            "Epoch 874/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8127e-04\n",
            "Epoch 875/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8329e-04\n",
            "Epoch 876/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8184e-04\n",
            "Epoch 877/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8041e-04\n",
            "Epoch 878/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8548e-04\n",
            "Epoch 879/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8296e-04\n",
            "Epoch 880/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8360e-04\n",
            "Epoch 881/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8099e-04\n",
            "Epoch 882/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8823e-04\n",
            "Epoch 883/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8383e-04\n",
            "Epoch 884/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8589e-04\n",
            "Epoch 885/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9088e-04\n",
            "Epoch 886/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8232e-04\n",
            "Epoch 887/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8540e-04\n",
            "Epoch 888/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8427e-04\n",
            "Epoch 889/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8735e-04\n",
            "Epoch 890/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8598e-04\n",
            "Epoch 891/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8207e-04\n",
            "Epoch 892/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8152e-04\n",
            "Epoch 893/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9064e-04\n",
            "Epoch 894/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8691e-04\n",
            "Epoch 895/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8400e-04\n",
            "Epoch 896/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8530e-04\n",
            "Epoch 897/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8394e-04\n",
            "Epoch 898/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8935e-04\n",
            "Epoch 899/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8318e-04\n",
            "Epoch 900/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8083e-04\n",
            "Epoch 901/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8524e-04\n",
            "Epoch 902/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8376e-04\n",
            "Epoch 903/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8568e-04\n",
            "Epoch 904/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8098e-04\n",
            "Epoch 905/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8363e-04\n",
            "Epoch 906/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8322e-04\n",
            "Epoch 907/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8580e-04\n",
            "Epoch 908/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.9684e-04\n",
            "Epoch 909/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8484e-04\n",
            "Epoch 910/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8418e-04\n",
            "Epoch 911/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8533e-04\n",
            "Epoch 912/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8841e-04\n",
            "Epoch 913/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 4.8311e-04\n",
            "Epoch 914/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8252e-04\n",
            "Epoch 915/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8241e-04\n",
            "Epoch 916/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8604e-04\n",
            "Epoch 917/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8899e-04\n",
            "Epoch 918/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8031e-04\n",
            "Epoch 919/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8961e-04\n",
            "Epoch 920/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8481e-04\n",
            "Epoch 921/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8494e-04\n",
            "Epoch 922/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8464e-04\n",
            "Epoch 923/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8245e-04\n",
            "Epoch 924/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8266e-04\n",
            "Epoch 925/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9235e-04\n",
            "Epoch 926/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8548e-04\n",
            "Epoch 927/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8790e-04\n",
            "Epoch 928/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8373e-04\n",
            "Epoch 929/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8456e-04\n",
            "Epoch 930/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8129e-04\n",
            "Epoch 931/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8985e-04\n",
            "Epoch 932/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8318e-04\n",
            "Epoch 933/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8173e-04\n",
            "Epoch 934/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8311e-04\n",
            "Epoch 935/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8629e-04\n",
            "Epoch 936/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8140e-04\n",
            "Epoch 937/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8177e-04\n",
            "Epoch 938/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8192e-04\n",
            "Epoch 939/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8252e-04\n",
            "Epoch 940/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8224e-04\n",
            "Epoch 941/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8471e-04\n",
            "Epoch 942/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8337e-04\n",
            "Epoch 943/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8113e-04\n",
            "Epoch 944/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8316e-04\n",
            "Epoch 945/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8234e-04\n",
            "Epoch 946/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8114e-04\n",
            "Epoch 947/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8297e-04\n",
            "Epoch 948/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8681e-04\n",
            "Epoch 949/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8371e-04\n",
            "Epoch 950/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9117e-04\n",
            "Epoch 951/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8274e-04\n",
            "Epoch 952/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8326e-04\n",
            "Epoch 953/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8636e-04\n",
            "Epoch 954/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8414e-04\n",
            "Epoch 955/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8233e-04\n",
            "Epoch 956/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8089e-04\n",
            "Epoch 957/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8306e-04\n",
            "Epoch 958/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8436e-04\n",
            "Epoch 959/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8056e-04\n",
            "Epoch 960/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8561e-04\n",
            "Epoch 961/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8312e-04\n",
            "Epoch 962/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8455e-04\n",
            "Epoch 963/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8063e-04\n",
            "Epoch 964/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8497e-04\n",
            "Epoch 965/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8147e-04\n",
            "Epoch 966/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8201e-04\n",
            "Epoch 967/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8169e-04\n",
            "Epoch 968/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8297e-04\n",
            "Epoch 969/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8933e-04\n",
            "Epoch 970/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8650e-04\n",
            "Epoch 971/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8461e-04\n",
            "Epoch 972/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8355e-04\n",
            "Epoch 973/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8072e-04\n",
            "Epoch 974/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8194e-04\n",
            "Epoch 975/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8145e-04\n",
            "Epoch 976/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8138e-04\n",
            "Epoch 977/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8152e-04\n",
            "Epoch 978/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8254e-04\n",
            "Epoch 979/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8606e-04\n",
            "Epoch 980/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8333e-04\n",
            "Epoch 981/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8205e-04\n",
            "Epoch 982/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8702e-04\n",
            "Epoch 983/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8143e-04\n",
            "Epoch 984/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8724e-04\n",
            "Epoch 985/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8815e-04\n",
            "Epoch 986/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8359e-04\n",
            "Epoch 987/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8075e-04\n",
            "Epoch 988/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8353e-04\n",
            "Epoch 989/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8411e-04\n",
            "Epoch 990/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8073e-04\n",
            "Epoch 991/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8317e-04\n",
            "Epoch 992/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8095e-04\n",
            "Epoch 993/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8240e-04\n",
            "Epoch 994/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8157e-04\n",
            "Epoch 995/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8879e-04\n",
            "Epoch 996/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9545e-04\n",
            "Epoch 997/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8766e-04\n",
            "Epoch 998/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9085e-04\n",
            "Epoch 999/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8210e-04\n",
            "Epoch 1000/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8731e-04\n",
            "Epoch 1001/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9257e-04\n",
            "Epoch 1002/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8341e-04\n",
            "Epoch 1003/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8049e-04\n",
            "Epoch 1004/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8042e-04\n",
            "Epoch 1005/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8604e-04\n",
            "Epoch 1006/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8716e-04\n",
            "Epoch 1007/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8283e-04\n",
            "Epoch 1008/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8259e-04\n",
            "Epoch 1009/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8122e-04\n",
            "Epoch 1010/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8111e-04\n",
            "Epoch 1011/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8541e-04\n",
            "Epoch 1012/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8452e-04\n",
            "Epoch 1013/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.7982e-04\n",
            "Epoch 1014/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8263e-04\n",
            "Epoch 1015/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8617e-04\n",
            "Epoch 1016/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8426e-04\n",
            "Epoch 1017/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8600e-04\n",
            "Epoch 1018/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8217e-04\n",
            "Epoch 1019/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8297e-04\n",
            "Epoch 1020/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8408e-04\n",
            "Epoch 1021/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8235e-04\n",
            "Epoch 1022/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8220e-04\n",
            "Epoch 1023/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8356e-04\n",
            "Epoch 1024/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8132e-04\n",
            "Epoch 1025/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8379e-04\n",
            "Epoch 1026/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8210e-04\n",
            "Epoch 1027/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8397e-04\n",
            "Epoch 1028/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8485e-04\n",
            "Epoch 1029/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9003e-04\n",
            "Epoch 1030/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8243e-04\n",
            "Epoch 1031/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8027e-04\n",
            "Epoch 1032/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8372e-04\n",
            "Epoch 1033/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8189e-04\n",
            "Epoch 1034/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8446e-04\n",
            "Epoch 1035/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8690e-04\n",
            "Epoch 1036/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8608e-04\n",
            "Epoch 1037/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9014e-04\n",
            "Epoch 1038/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8374e-04\n",
            "Epoch 1039/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8708e-04\n",
            "Epoch 1040/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8127e-04\n",
            "Epoch 1041/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8206e-04\n",
            "Epoch 1042/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 5.0321e-04\n",
            "Epoch 1043/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8999e-04\n",
            "Epoch 1044/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8670e-04\n",
            "Epoch 1045/2000\n",
            "1566/1566 [==============================] - 0s 19us/step - loss: 4.8948e-04\n",
            "Epoch 1046/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8290e-04\n",
            "Epoch 1047/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8279e-04\n",
            "Epoch 1048/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8362e-04\n",
            "Epoch 1049/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8391e-04\n",
            "Epoch 1050/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8299e-04\n",
            "Epoch 1051/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8136e-04\n",
            "Epoch 1052/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8435e-04\n",
            "Epoch 1053/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8169e-04\n",
            "Epoch 1054/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8504e-04\n",
            "Epoch 1055/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8342e-04\n",
            "Epoch 1056/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8382e-04\n",
            "Epoch 1057/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8791e-04\n",
            "Epoch 1058/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8826e-04\n",
            "Epoch 1059/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8833e-04\n",
            "Epoch 1060/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8474e-04\n",
            "Epoch 1061/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.7957e-04\n",
            "Epoch 1062/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8039e-04\n",
            "Epoch 1063/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.7995e-04\n",
            "Epoch 1064/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8032e-04\n",
            "Epoch 1065/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8162e-04\n",
            "Epoch 1066/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8390e-04\n",
            "Epoch 1067/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8103e-04\n",
            "Epoch 1068/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8137e-04\n",
            "Epoch 1069/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8840e-04\n",
            "Epoch 1070/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8377e-04\n",
            "Epoch 1071/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8160e-04\n",
            "Epoch 1072/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8156e-04\n",
            "Epoch 1073/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8159e-04\n",
            "Epoch 1074/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8419e-04\n",
            "Epoch 1075/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8717e-04\n",
            "Epoch 1076/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8330e-04\n",
            "Epoch 1077/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8041e-04\n",
            "Epoch 1078/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8361e-04\n",
            "Epoch 1079/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8112e-04\n",
            "Epoch 1080/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8313e-04\n",
            "Epoch 1081/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8504e-04\n",
            "Epoch 1082/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8230e-04\n",
            "Epoch 1083/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8165e-04\n",
            "Epoch 1084/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8426e-04\n",
            "Epoch 1085/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8515e-04\n",
            "Epoch 1086/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8808e-04\n",
            "Epoch 1087/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8323e-04\n",
            "Epoch 1088/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8025e-04\n",
            "Epoch 1089/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8595e-04\n",
            "Epoch 1090/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8575e-04\n",
            "Epoch 1091/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9362e-04\n",
            "Epoch 1092/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8727e-04\n",
            "Epoch 1093/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8581e-04\n",
            "Epoch 1094/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8503e-04\n",
            "Epoch 1095/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8185e-04\n",
            "Epoch 1096/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8112e-04\n",
            "Epoch 1097/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8085e-04\n",
            "Epoch 1098/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8016e-04\n",
            "Epoch 1099/2000\n",
            "1566/1566 [==============================] - 0s 19us/step - loss: 4.8375e-04\n",
            "Epoch 1100/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8408e-04\n",
            "Epoch 1101/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8228e-04\n",
            "Epoch 1102/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8115e-04\n",
            "Epoch 1103/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8150e-04\n",
            "Epoch 1104/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8044e-04\n",
            "Epoch 1105/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8143e-04\n",
            "Epoch 1106/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8138e-04\n",
            "Epoch 1107/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8181e-04\n",
            "Epoch 1108/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8492e-04\n",
            "Epoch 1109/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8358e-04\n",
            "Epoch 1110/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8528e-04\n",
            "Epoch 1111/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8067e-04\n",
            "Epoch 1112/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8204e-04\n",
            "Epoch 1113/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8063e-04\n",
            "Epoch 1114/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8935e-04\n",
            "Epoch 1115/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8195e-04\n",
            "Epoch 1116/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8110e-04\n",
            "Epoch 1117/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8279e-04\n",
            "Epoch 1118/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8113e-04\n",
            "Epoch 1119/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8194e-04\n",
            "Epoch 1120/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8133e-04\n",
            "Epoch 1121/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8233e-04\n",
            "Epoch 1122/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8345e-04\n",
            "Epoch 1123/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8411e-04\n",
            "Epoch 1124/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8058e-04\n",
            "Epoch 1125/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8171e-04\n",
            "Epoch 1126/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8314e-04\n",
            "Epoch 1127/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8888e-04\n",
            "Epoch 1128/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8585e-04\n",
            "Epoch 1129/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8609e-04\n",
            "Epoch 1130/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8038e-04\n",
            "Epoch 1131/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8205e-04\n",
            "Epoch 1132/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8055e-04\n",
            "Epoch 1133/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8132e-04\n",
            "Epoch 1134/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8583e-04\n",
            "Epoch 1135/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8896e-04\n",
            "Epoch 1136/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8222e-04\n",
            "Epoch 1137/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8131e-04\n",
            "Epoch 1138/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8320e-04\n",
            "Epoch 1139/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8120e-04\n",
            "Epoch 1140/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8197e-04\n",
            "Epoch 1141/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8179e-04\n",
            "Epoch 1142/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8125e-04\n",
            "Epoch 1143/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8128e-04\n",
            "Epoch 1144/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8344e-04\n",
            "Epoch 1145/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8081e-04\n",
            "Epoch 1146/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8166e-04\n",
            "Epoch 1147/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8037e-04\n",
            "Epoch 1148/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8483e-04\n",
            "Epoch 1149/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8237e-04\n",
            "Epoch 1150/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8230e-04\n",
            "Epoch 1151/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8324e-04\n",
            "Epoch 1152/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8147e-04\n",
            "Epoch 1153/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8263e-04\n",
            "Epoch 1154/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8097e-04\n",
            "Epoch 1155/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8526e-04\n",
            "Epoch 1156/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8065e-04\n",
            "Epoch 1157/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8020e-04\n",
            "Epoch 1158/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8503e-04\n",
            "Epoch 1159/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8300e-04\n",
            "Epoch 1160/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8494e-04\n",
            "Epoch 1161/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8273e-04\n",
            "Epoch 1162/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8362e-04\n",
            "Epoch 1163/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8173e-04\n",
            "Epoch 1164/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.7964e-04\n",
            "Epoch 1165/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8199e-04\n",
            "Epoch 1166/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8053e-04\n",
            "Epoch 1167/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8113e-04\n",
            "Epoch 1168/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8263e-04\n",
            "Epoch 1169/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9778e-04\n",
            "Epoch 1170/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8078e-04\n",
            "Epoch 1171/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8196e-04\n",
            "Epoch 1172/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8044e-04\n",
            "Epoch 1173/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8570e-04\n",
            "Epoch 1174/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8267e-04\n",
            "Epoch 1175/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8149e-04\n",
            "Epoch 1176/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8624e-04\n",
            "Epoch 1177/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8961e-04\n",
            "Epoch 1178/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8092e-04\n",
            "Epoch 1179/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8200e-04\n",
            "Epoch 1180/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8121e-04\n",
            "Epoch 1181/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.9486e-04\n",
            "Epoch 1182/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9219e-04\n",
            "Epoch 1183/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8629e-04\n",
            "Epoch 1184/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8789e-04\n",
            "Epoch 1185/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8130e-04\n",
            "Epoch 1186/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8086e-04\n",
            "Epoch 1187/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8560e-04\n",
            "Epoch 1188/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8612e-04\n",
            "Epoch 1189/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8555e-04\n",
            "Epoch 1190/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8583e-04\n",
            "Epoch 1191/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8339e-04\n",
            "Epoch 1192/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8195e-04\n",
            "Epoch 1193/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8183e-04\n",
            "Epoch 1194/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8126e-04\n",
            "Epoch 1195/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8345e-04\n",
            "Epoch 1196/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8016e-04\n",
            "Epoch 1197/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8205e-04\n",
            "Epoch 1198/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8080e-04\n",
            "Epoch 1199/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.9042e-04\n",
            "Epoch 1200/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8981e-04\n",
            "Epoch 1201/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8151e-04\n",
            "Epoch 1202/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8318e-04\n",
            "Epoch 1203/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8835e-04\n",
            "Epoch 1204/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8467e-04\n",
            "Epoch 1205/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8468e-04\n",
            "Epoch 1206/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8286e-04\n",
            "Epoch 1207/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8424e-04\n",
            "Epoch 1208/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8830e-04\n",
            "Epoch 1209/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8408e-04\n",
            "Epoch 1210/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8440e-04\n",
            "Epoch 1211/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8076e-04\n",
            "Epoch 1212/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8325e-04\n",
            "Epoch 1213/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8161e-04\n",
            "Epoch 1214/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8288e-04\n",
            "Epoch 1215/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8353e-04\n",
            "Epoch 1216/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8802e-04\n",
            "Epoch 1217/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8628e-04\n",
            "Epoch 1218/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8272e-04\n",
            "Epoch 1219/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8129e-04\n",
            "Epoch 1220/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8112e-04\n",
            "Epoch 1221/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8177e-04\n",
            "Epoch 1222/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8129e-04\n",
            "Epoch 1223/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8257e-04\n",
            "Epoch 1224/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8083e-04\n",
            "Epoch 1225/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8334e-04\n",
            "Epoch 1226/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8167e-04\n",
            "Epoch 1227/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8391e-04\n",
            "Epoch 1228/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8021e-04\n",
            "Epoch 1229/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8688e-04\n",
            "Epoch 1230/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8658e-04\n",
            "Epoch 1231/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8376e-04\n",
            "Epoch 1232/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8242e-04\n",
            "Epoch 1233/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8297e-04\n",
            "Epoch 1234/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8110e-04\n",
            "Epoch 1235/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8418e-04\n",
            "Epoch 1236/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8848e-04\n",
            "Epoch 1237/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8474e-04\n",
            "Epoch 1238/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8260e-04\n",
            "Epoch 1239/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8258e-04\n",
            "Epoch 1240/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.7993e-04\n",
            "Epoch 1241/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8028e-04\n",
            "Epoch 1242/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8073e-04\n",
            "Epoch 1243/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.7955e-04\n",
            "Epoch 1244/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8396e-04\n",
            "Epoch 1245/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8205e-04\n",
            "Epoch 1246/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8047e-04\n",
            "Epoch 1247/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8512e-04\n",
            "Epoch 1248/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8304e-04\n",
            "Epoch 1249/2000\n",
            "1566/1566 [==============================] - 0s 24us/step - loss: 4.8173e-04\n",
            "Epoch 1250/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8430e-04\n",
            "Epoch 1251/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8010e-04\n",
            "Epoch 1252/2000\n",
            "1566/1566 [==============================] - 0s 25us/step - loss: 4.8563e-04\n",
            "Epoch 1253/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8075e-04\n",
            "Epoch 1254/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8274e-04\n",
            "Epoch 1255/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8416e-04\n",
            "Epoch 1256/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8504e-04\n",
            "Epoch 1257/2000\n",
            "1566/1566 [==============================] - 0s 23us/step - loss: 4.8230e-04\n",
            "Epoch 1258/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8104e-04\n",
            "Epoch 1259/2000\n",
            "1566/1566 [==============================] - 0s 20us/step - loss: 4.8407e-04\n",
            "Epoch 1260/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8319e-04\n",
            "Epoch 1261/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8290e-04\n",
            "Epoch 1262/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8183e-04\n",
            "Epoch 1263/2000\n",
            "1566/1566 [==============================] - 0s 22us/step - loss: 4.8126e-04\n",
            "Epoch 1264/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8318e-04\n",
            "Epoch 1265/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8159e-04\n",
            "Epoch 1266/2000\n",
            "1566/1566 [==============================] - 0s 21us/step - loss: 4.8113e-04\n",
            "Epoch 1267/2000\n",
            " 100/1566 [>.............................] - ETA: 0s - loss: 3.0387e-05"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-W-Lh2jhGOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7290c754-1c0d-47a7-879c-2adc81523841"
      },
      "source": [
        "print(np.tanh(reg_model.layers[0].get_weights()[3]))\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6268091]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE90OgzpcWB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be683ff1-5feb-43e1-b2e5-adcf313fefa7"
      },
      "source": [
        "alpha_rnn_pred_train = reg_model.predict(x_train_reg, verbose=1)\n",
        "alpha_rnn_pred_test = reg_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 285us/step\n",
            "384/384 [==============================] - 0s 30us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaT2m6A7erEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "469fdd62-e09e-4207-87d7-d329229e000b"
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"orange\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpha_rnn_pred_test[:, 0], color=\"red\", label=\"RNN Predict (Testing)\")\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5YtRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxS4AYIZf57S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN_model(n_units=10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(SimpleRNN(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6MLiMOyf-tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model=RNN_model(hidden_units, l1_reg)\n",
        "rnn_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIXAHIj2i_Qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT3ucd9Fid57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d305437-b4aa-422c-9dee-f00a3c61038c"
      },
      "source": [
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 33us/step\n",
            "384/384 [==============================] - 0s 38us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxcTtqC7iixP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "0c132166-ccdc-4d79-a3b8-eba46e51b9ec"
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"black\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpha_rnn_pred_test[:, 0], color=\"red\", label=\"alpha RNN Predict (Testing)\")\n",
        "plt.plot(df_test.index[n_steps:], rnn_pred_test[:, 0], color=\"blue\", label=\"RNN Predict (Testing)\")\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSPlJCLiIiIiOSREnIRERERkTz6/62Xq3t+RszOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}