{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "KerasLayer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/KerasLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmsnQfHWWlX-",
        "colab_type": "code",
        "outputId": "40e253b1-1b04-470e-a4a3-e79c648e990a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan 14 23:15:31 2020\n",
        "\n",
        "@author: macbookpro\n",
        "\"\"\"\n",
        " \n",
        "\n",
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import layers\n",
        "#from alphaRNN import *\n",
        "from keras import *\n",
        "from keras.legacy import interfaces\n",
        "# To make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)\n",
        "    \n",
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)\n",
        "\n",
        "p = 30 # the number of lags (in both the data and the models)\n",
        "vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]\n",
        "\n",
        "df = pd.DataFrame(vols, columns=['vol'])\n",
        "\n",
        "use_features = ['vol'] \n",
        "target = 'vol'\n",
        "n_steps = 10 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(len(df)*train_weight)\n",
        "\n",
        "df_train = df[use_features].iloc[:split]\n",
        "print(df_train)\n",
        "df_test = df[use_features].iloc[split:]\n",
        "\n",
        "def get_lagged_features(value, n_steps):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps, 0, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)\n",
        "\n",
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "#x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "#x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "#y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "#x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "#y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "#x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "#x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "#y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "#x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "#y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))\n",
        "\n",
        "#train_batch_size = y_train_reg.shape[0]\n",
        "#test_batch_size = y_test_reg.shape[0]    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           vol\n",
            "0     0.231685\n",
            "1     0.233905\n",
            "2     0.236207\n",
            "3     0.238376\n",
            "4     0.240477\n",
            "...        ...\n",
            "1571  0.149595\n",
            "1572  0.152918\n",
            "1573  0.156165\n",
            "1574  0.159597\n",
            "1575  0.163079\n",
            "\n",
            "[1576 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QInb-xC2MtAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY1C3s4TLSW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphaRNNCell(Layer):\n",
        "    \"\"\"Cell class for AlphaRNN.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        super(AlphaRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        \n",
        "        self.alpha = self.add_weight(shape=(1,),\n",
        "                                        name='alpha',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        prev_output = states[0]\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(prev_output),\n",
        "                self.recurrent_dropout,\n",
        "                training=training)\n",
        "\n",
        "        dp_mask = self._dropout_mask\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if dp_mask is not None:\n",
        "            h = K.dot(inputs * dp_mask, self.kernel)\n",
        "        else:\n",
        "            h = K.dot(inputs, self.kernel)\n",
        "        if self.bias is not None:\n",
        "            h = K.bias_add(h, self.bias)\n",
        "\n",
        "        if rec_dp_mask is not None:\n",
        "            prev_output *= rec_dp_mask\n",
        "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        output = K.tanh(self.alpha)* output + 1-K.tanh(self.alpha)* prev_output\n",
        "        # Properly set learning phase on output tensor.\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                output._uses_learning_phase = True\n",
        "        return output, [output]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphaRNN(keras.layers.RNN):\n",
        "    \"\"\"Fully-connected AlphaRNN where the output is to be fed back to input.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if 'implementation' in kwargs:\n",
        "            kwargs.pop('implementation')\n",
        "            warnings.warn('The `implementation` argument '\n",
        "                          'in `SimpleRNN` has been deprecated. '\n",
        "                          'Please remove it from your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphaRNNCell(units,\n",
        "                             activation=activation,\n",
        "                             use_bias=use_bias,\n",
        "                             kernel_initializer=kernel_initializer,\n",
        "                             recurrent_initializer=recurrent_initializer,\n",
        "                             bias_initializer=bias_initializer,\n",
        "                             kernel_regularizer=kernel_regularizer,\n",
        "                             recurrent_regularizer=recurrent_regularizer,\n",
        "                             bias_regularizer=bias_regularizer,\n",
        "                             kernel_constraint=kernel_constraint,\n",
        "                             recurrent_constraint=recurrent_constraint,\n",
        "                             bias_constraint=bias_constraint,\n",
        "                             dropout=dropout,\n",
        "                             recurrent_dropout=recurrent_dropout)\n",
        "        super(AlphaRNN, self).__init__(cell,\n",
        "                                        return_sequences=return_sequences,\n",
        "                                        return_state=return_state,\n",
        "                                        go_backwards=go_backwards,\n",
        "                                        stateful=stateful,\n",
        "                                        unroll=unroll,\n",
        "                                        **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphaRNN, self).call(inputs,\n",
        "                                           mask=mask,\n",
        "                                           training=training,\n",
        "                                           initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(AlphaRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config:\n",
        "            config.pop('implementation')\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdobsJiulG0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphatRNNCell(Layer):\n",
        "    \"\"\"Cell class for the AlphatRNN layer.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 **kwargs):\n",
        "        super(AlphatRNNCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.implementation = implementation\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if isinstance(self.recurrent_initializer, initializers.Identity):\n",
        "            def recurrent_identity(shape, gain=1., dtype=None):\n",
        "                del dtype\n",
        "                return gain * np.concatenate(\n",
        "                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
        "\n",
        "            self.recurrent_initializer = recurrent_identity\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units * 2),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 2),\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias_shape = (2, 2 * self.units)\n",
        "            self.bias = self.add_weight(shape=bias_shape,\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            \n",
        "            self.input_bias = K.flatten(self.bias[0])\n",
        "            self.recurrent_bias = K.flatten(self.bias[1])\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # alpha\n",
        "        self.kernel_alpha = self.kernel[:, :self.units]\n",
        "        self.recurrent_kernel_alpha = self.recurrent_kernel[:, :self.units]\n",
        "        # recurrnce\n",
        "        self.kernel_h = self.kernel[:, self.units:]\n",
        "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias for inputs\n",
        "            self.input_bias_alpha = self.input_bias[:self.units]\n",
        "            self.input_bias_h = self.input_bias[self.units:]\n",
        "            # bias for hidden state - just for compatibility with CuDNN\n",
        "            \n",
        "            self.recurrent_bias_alpha = self.recurrent_bias[:self.units]    \n",
        "            self.recurrent_bias_h = self.recurrent_bias[self.units:]\n",
        "        else:\n",
        "            self.input_bias_alpha = None\n",
        "            self.input_bias_h = None\n",
        "            \n",
        "            self.recurrent_bias_alpha = None\n",
        "            self.recurrent_bias_h = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "\n",
        "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
        "            self._dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(inputs),\n",
        "                self.dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "        if (0 < self.recurrent_dropout < 1 and\n",
        "                self._recurrent_dropout_mask is None):\n",
        "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
        "                K.ones_like(h_tm1),\n",
        "                self.recurrent_dropout,\n",
        "                training=training,\n",
        "                count=2)\n",
        "\n",
        "        # dropout matrices for input units\n",
        "        dp_mask = self._dropout_mask\n",
        "        # dropout matrices for recurrent units\n",
        "        rec_dp_mask = self._recurrent_dropout_mask\n",
        "\n",
        "        if self.implementation == 1:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs_alpha = inputs * dp_mask[0]\n",
        "                inputs_h = inputs * dp_mask[1]\n",
        "            else:\n",
        "                inputs_alpha = input\n",
        "                inputs_h = inputs\n",
        "\n",
        "            x_alpha = K.dot(inputs_alpha, self.kernel_alpha)\n",
        "            x_h = K.dot(inputs_h, self.kernel_h)\n",
        "            if self.use_bias:\n",
        "                x_alpha = K.bias_add(x_alpha, self.input_bias_alpha)\n",
        "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
        "\n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1_alpha = h_tm1 * rec_dp_mask[0]\n",
        "                h_tm1_h = h_tm1 * rec_dp_mask[1]\n",
        "            else:\n",
        "                h_tm1_alpha = h_tm1\n",
        "                h_tm1_h = h_tm1\n",
        "\n",
        "            recurrent_alpha = K.dot(h_tm1_alpha, self.recurrent_kernel_alpha)\n",
        "           \n",
        "            if self.use_bias:\n",
        "                recurrent_alpha = K.bias_add(recurrent_alpha, self.recurrent_bias_alpha)\n",
        "\n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "           \n",
        "            recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n",
        "            if self.use_bias:\n",
        "                recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
        "            \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "        else:\n",
        "            if 0. < self.dropout < 1.:\n",
        "                inputs *= dp_mask[0]\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(inputs, self.kernel)\n",
        "            if self.use_bias:\n",
        "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
        "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
        "            x_alpha = matrix_x[:, :self.units]\n",
        "            x_h = matrix_x[:, self.units: 2 * self.units]\n",
        "            \n",
        "            if 0. < self.recurrent_dropout < 1.:\n",
        "                h_tm1 *= rec_dp_mask[0]\n",
        "\n",
        "            \n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            if self.use_bias:\n",
        "                  matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "            \n",
        "            recurrent_alpha = matrix_inner[:, :self.units] \n",
        "            alpha = self.recurrent_activation(x_alpha + recurrent_alpha)\n",
        "            \n",
        "            recurrent_h = matrix_inner[:, self.units: 2 * self.units]  \n",
        "            hh = self.activation(x_h + recurrent_h)\n",
        "\n",
        "        # previous and candidate state mixed by update gate\n",
        "        h = alpha * h_tm1 + (1 - alpha) * hh\n",
        "\n",
        "        if 0 < self.dropout + self.recurrent_dropout:\n",
        "            if training is None:\n",
        "                h._uses_learning_phase = True\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNNCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class AlphatRNN(keras.layers.RNN):\n",
        "    \"\"\"Alpha_t RNN\n",
        "    There are two variants. The default one is based on 1406.1078v3 and\n",
        "    has reset gate applied to hidden state before matrix multiplication. The\n",
        "    other one is based on original 1406.1078v1 and has the order reversed.\n",
        "    The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
        "    inference on CPU. Thus it has separate biases for `kernel` and\n",
        "    `recurrent_kernel`. Use `'reset_after'=True` and\n",
        "    `recurrent_activation='sigmoid'`.\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        recurrent_activation: Activation function to use\n",
        "            for the recurrent step\n",
        "            (see [activations](../activations.md)).\n",
        "            Default: hyperbolic tangent (`tanh`).\n",
        "            If you pass `None`, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs\n",
        "            (see [initializers](../initializers.md)).\n",
        "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "            weights matrix,\n",
        "            used for the linear transformation of the recurrent state\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        recurrent_regularizer: Regularizer function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\").\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        recurrent_constraint: Constraint function applied to\n",
        "            the `recurrent_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the inputs.\n",
        "        recurrent_dropout: Float between 0 and 1.\n",
        "            Fraction of the units to drop for\n",
        "            the linear transformation of the recurrent state.\n",
        "        implementation: Implementation mode, either 1 or 2.\n",
        "            Mode 1 will structure its operations as a larger number of\n",
        "            smaller dot products and additions, whereas mode 2 will\n",
        "            batch them into fewer, larger operations. These modes will\n",
        "            have different performance profiles on different hardware and\n",
        "            for different applications.\n",
        "        return_sequences: Boolean. Whether to return the last output\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        unroll: Boolean (default False).\n",
        "            If True, the network will be unrolled,\n",
        "            else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN,\n",
        "            although it tends to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "        \n",
        "    # References\n",
        "        - [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "        - [On the Properties of Neural Machine Translation:\n",
        "           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
        "        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n",
        "           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n",
        "        - [A Theoretically Grounded Application of Dropout in\n",
        "           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "    \"\"\"\n",
        "\n",
        "    @interfaces.legacy_recurrent_support\n",
        "    def __init__(self, units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='tanh',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 implementation=2,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if implementation == 0:\n",
        "            warnings.warn('`implementation=0` has been deprecated, '\n",
        "                          'and now defaults to `implementation=1`.'\n",
        "                          'Please update your layer call.')\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "            warnings.warn(\n",
        "                'RNN dropout is no longer supported with the Theano backend '\n",
        "                'due to technical limitations. '\n",
        "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                'or use the TensorFlow backend.')\n",
        "            dropout = 0.\n",
        "            recurrent_dropout = 0.\n",
        "\n",
        "        cell = AlphatRNNCell(units,\n",
        "                       activation=activation,\n",
        "                       recurrent_activation=recurrent_activation,\n",
        "                       use_bias=use_bias,\n",
        "                       kernel_initializer=kernel_initializer,\n",
        "                       recurrent_initializer=recurrent_initializer,\n",
        "                       bias_initializer=bias_initializer,\n",
        "                       kernel_regularizer=kernel_regularizer,\n",
        "                       recurrent_regularizer=recurrent_regularizer,\n",
        "                       bias_regularizer=bias_regularizer,\n",
        "                       kernel_constraint=kernel_constraint,\n",
        "                       recurrent_constraint=recurrent_constraint,\n",
        "                       bias_constraint=bias_constraint,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=recurrent_dropout,\n",
        "                       implementation=implementation)             \n",
        "        super(AlphatRNN, self).__init__(cell,\n",
        "                                  return_sequences=return_sequences,\n",
        "                                  return_state=return_state,\n",
        "                                  go_backwards=go_backwards,\n",
        "                                  stateful=stateful,\n",
        "                                  unroll=unroll,\n",
        "                                  **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "        self.cell._dropout_mask = None\n",
        "        self.cell._recurrent_dropout_mask = None\n",
        "        return super(AlphatRNN, self).call(inputs,\n",
        "                                     mask=mask,\n",
        "                                     training=training,\n",
        "                                     initial_state=initial_state)\n",
        "\n",
        "    @property\n",
        "    def units(self):\n",
        "        return self.cell.units\n",
        "\n",
        "    @property\n",
        "    def activation(self):\n",
        "        return self.cell.activation\n",
        "\n",
        "    @property\n",
        "    def recurrent_activation(self):\n",
        "        return self.cell.recurrent_activation\n",
        "\n",
        "    @property\n",
        "    def use_bias(self):\n",
        "        return self.cell.use_bias\n",
        "\n",
        "    @property\n",
        "    def kernel_initializer(self):\n",
        "        return self.cell.kernel_initializer\n",
        "\n",
        "    @property\n",
        "    def recurrent_initializer(self):\n",
        "        return self.cell.recurrent_initializer\n",
        "\n",
        "    @property\n",
        "    def bias_initializer(self):\n",
        "        return self.cell.bias_initializer\n",
        "\n",
        "    @property\n",
        "    def kernel_regularizer(self):\n",
        "        return self.cell.kernel_regularizer\n",
        "\n",
        "    @property\n",
        "    def recurrent_regularizer(self):\n",
        "        return self.cell.recurrent_regularizer\n",
        "\n",
        "    @property\n",
        "    def bias_regularizer(self):\n",
        "        return self.cell.bias_regularizer\n",
        "\n",
        "    @property\n",
        "    def kernel_constraint(self):\n",
        "        return self.cell.kernel_constraint\n",
        "\n",
        "    @property\n",
        "    def recurrent_constraint(self):\n",
        "        return self.cell.recurrent_constraint\n",
        "\n",
        "    @property\n",
        "    def bias_constraint(self):\n",
        "        return self.cell.bias_constraint\n",
        "\n",
        "    @property\n",
        "    def dropout(self):\n",
        "        return self.cell.dropout\n",
        "\n",
        "    @property\n",
        "    def recurrent_dropout(self):\n",
        "        return self.cell.recurrent_dropout\n",
        "\n",
        "    @property\n",
        "    def implementation(self):\n",
        "        return self.cell.implementation\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer':\n",
        "                      regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout,\n",
        "                  'implementation': self.implementation}\n",
        "        base_config = super(AlphatRNN, self).get_config()\n",
        "        del base_config['cell']\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if 'implementation' in config and config['implementation'] == 0:\n",
        "            config['implementation'] = 1\n",
        "        return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Ls0KyNONY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSSokmMsOa1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9YQuMUg0bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=1e-2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMbxTnZCUdJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02357204-ac09-42b0-80dd-c1fda723bfb5"
      },
      "source": [
        "hidden_units=5\n",
        "l1_reg=0\n",
        "reg_model = Sequential()\n",
        "#reg_model.add(AlphaRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "#reg_model.add(AlphatRNN(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(GRU(hidden_units, activation='tanh', recurrent_activation='sigmoid', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "#reg_model.add(Dropout(0.2))\n",
        "reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "reg_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1566/1566 [==============================] - 2s 1ms/step - loss: 0.0447\n",
            "Epoch 2/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 0.0373\n",
            "Epoch 3/2000\n",
            "1566/1566 [==============================] - 0s 26us/step - loss: 0.0309\n",
            "Epoch 4/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0253\n",
            "Epoch 5/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0202\n",
            "Epoch 6/2000\n",
            " 100/1566 [>.............................] - ETA: 0s - loss: 0.0192"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 31us/step - loss: 0.0160\n",
            "Epoch 7/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 0.0132\n",
            "Epoch 8/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0111\n",
            "Epoch 9/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0089\n",
            "Epoch 10/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 0.0064\n",
            "Epoch 11/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0039\n",
            "Epoch 12/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0022\n",
            "Epoch 13/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0015\n",
            "Epoch 14/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0014\n",
            "Epoch 15/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 0.0014\n",
            "Epoch 16/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0013\n",
            "Epoch 17/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 0.0013\n",
            "Epoch 18/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 0.0013\n",
            "Epoch 19/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 0.0013\n",
            "Epoch 20/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0012\n",
            "Epoch 21/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 0.0012\n",
            "Epoch 22/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 0.0012\n",
            "Epoch 23/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 0.0012\n",
            "Epoch 24/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0012\n",
            "Epoch 25/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 0.0011\n",
            "Epoch 26/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0011\n",
            "Epoch 27/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0011\n",
            "Epoch 28/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0011\n",
            "Epoch 29/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 0.0011\n",
            "Epoch 30/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0011\n",
            "Epoch 31/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0010\n",
            "Epoch 32/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0010\n",
            "Epoch 33/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 0.0010\n",
            "Epoch 34/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.9764e-04\n",
            "Epoch 35/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 9.8150e-04\n",
            "Epoch 36/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.6663e-04\n",
            "Epoch 37/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 9.5561e-04\n",
            "Epoch 38/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.3917e-04\n",
            "Epoch 39/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.2714e-04\n",
            "Epoch 40/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.1536e-04\n",
            "Epoch 41/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 9.0554e-04\n",
            "Epoch 42/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.9631e-04\n",
            "Epoch 43/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 8.8485e-04\n",
            "Epoch 44/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 8.7396e-04\n",
            "Epoch 45/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.6363e-04\n",
            "Epoch 46/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 8.5401e-04\n",
            "Epoch 47/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.4665e-04\n",
            "Epoch 48/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.3603e-04\n",
            "Epoch 49/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 8.2807e-04\n",
            "Epoch 50/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.2002e-04\n",
            "Epoch 51/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.1556e-04\n",
            "Epoch 52/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.0878e-04\n",
            "Epoch 53/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.9757e-04\n",
            "Epoch 54/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 7.9409e-04\n",
            "Epoch 55/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.8407e-04\n",
            "Epoch 56/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 7.7535e-04\n",
            "Epoch 57/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 7.7145e-04\n",
            "Epoch 58/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.6691e-04\n",
            "Epoch 59/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.5995e-04\n",
            "Epoch 60/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 7.5543e-04\n",
            "Epoch 61/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 7.5014e-04\n",
            "Epoch 62/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 7.4227e-04\n",
            "Epoch 63/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.3931e-04\n",
            "Epoch 64/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 7.3324e-04\n",
            "Epoch 65/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 7.3240e-04\n",
            "Epoch 66/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.2396e-04\n",
            "Epoch 67/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 7.1770e-04\n",
            "Epoch 68/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 7.1397e-04\n",
            "Epoch 69/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 7.0975e-04\n",
            "Epoch 70/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 7.0589e-04\n",
            "Epoch 71/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 7.0293e-04\n",
            "Epoch 72/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.9861e-04\n",
            "Epoch 73/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.9510e-04\n",
            "Epoch 74/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.9225e-04\n",
            "Epoch 75/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 6.9115e-04\n",
            "Epoch 76/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.8504e-04\n",
            "Epoch 77/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 6.8166e-04\n",
            "Epoch 78/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.8073e-04\n",
            "Epoch 79/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 6.7480e-04\n",
            "Epoch 80/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.7479e-04\n",
            "Epoch 81/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.7005e-04\n",
            "Epoch 82/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 6.6561e-04\n",
            "Epoch 83/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.6548e-04\n",
            "Epoch 84/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.7186e-04\n",
            "Epoch 85/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.5720e-04\n",
            "Epoch 86/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.5449e-04\n",
            "Epoch 87/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.5499e-04\n",
            "Epoch 88/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 6.4950e-04\n",
            "Epoch 89/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.4824e-04\n",
            "Epoch 90/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.4510e-04\n",
            "Epoch 91/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.4237e-04\n",
            "Epoch 92/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 6.4214e-04\n",
            "Epoch 93/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 6.3802e-04\n",
            "Epoch 94/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 6.3534e-04\n",
            "Epoch 95/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.3226e-04\n",
            "Epoch 96/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.2949e-04\n",
            "Epoch 97/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.2865e-04\n",
            "Epoch 98/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.2575e-04\n",
            "Epoch 99/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.2425e-04\n",
            "Epoch 100/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.2459e-04\n",
            "Epoch 101/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.2119e-04\n",
            "Epoch 102/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.1878e-04\n",
            "Epoch 103/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.1513e-04\n",
            "Epoch 104/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.1341e-04\n",
            "Epoch 105/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.1104e-04\n",
            "Epoch 106/2000\n",
            "1566/1566 [==============================] - 0s 43us/step - loss: 6.1325e-04\n",
            "Epoch 107/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.0894e-04\n",
            "Epoch 108/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.0483e-04\n",
            "Epoch 109/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.1022e-04\n",
            "Epoch 110/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 6.0242e-04\n",
            "Epoch 111/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 6.0035e-04\n",
            "Epoch 112/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.9860e-04\n",
            "Epoch 113/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.9812e-04\n",
            "Epoch 114/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.9508e-04\n",
            "Epoch 115/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.9719e-04\n",
            "Epoch 116/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.9531e-04\n",
            "Epoch 117/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.9120e-04\n",
            "Epoch 118/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.8931e-04\n",
            "Epoch 119/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.8951e-04\n",
            "Epoch 120/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.8623e-04\n",
            "Epoch 121/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.9083e-04\n",
            "Epoch 122/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.8139e-04\n",
            "Epoch 123/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.8236e-04\n",
            "Epoch 124/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.8111e-04\n",
            "Epoch 125/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.7984e-04\n",
            "Epoch 126/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.7856e-04\n",
            "Epoch 127/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.7732e-04\n",
            "Epoch 128/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.7419e-04\n",
            "Epoch 129/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.7255e-04\n",
            "Epoch 130/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.7249e-04\n",
            "Epoch 131/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.7043e-04\n",
            "Epoch 132/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.7410e-04\n",
            "Epoch 133/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.7836e-04\n",
            "Epoch 134/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.6975e-04\n",
            "Epoch 135/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.6562e-04\n",
            "Epoch 136/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.6431e-04\n",
            "Epoch 137/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.6346e-04\n",
            "Epoch 138/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.6278e-04\n",
            "Epoch 139/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.6195e-04\n",
            "Epoch 140/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.5990e-04\n",
            "Epoch 141/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.5956e-04\n",
            "Epoch 142/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.5831e-04\n",
            "Epoch 143/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.5755e-04\n",
            "Epoch 144/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.5661e-04\n",
            "Epoch 145/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.5582e-04\n",
            "Epoch 146/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.5324e-04\n",
            "Epoch 147/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.5204e-04\n",
            "Epoch 148/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.5074e-04\n",
            "Epoch 149/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.4988e-04\n",
            "Epoch 150/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.4897e-04\n",
            "Epoch 151/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.4833e-04\n",
            "Epoch 152/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.5022e-04\n",
            "Epoch 153/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.4568e-04\n",
            "Epoch 154/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.4477e-04\n",
            "Epoch 155/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.4761e-04\n",
            "Epoch 156/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.4492e-04\n",
            "Epoch 157/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.4198e-04\n",
            "Epoch 158/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.4081e-04\n",
            "Epoch 159/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.4276e-04\n",
            "Epoch 160/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.4293e-04\n",
            "Epoch 161/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.4056e-04\n",
            "Epoch 162/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.3941e-04\n",
            "Epoch 163/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.3756e-04\n",
            "Epoch 164/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.3832e-04\n",
            "Epoch 165/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.3753e-04\n",
            "Epoch 166/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.4338e-04\n",
            "Epoch 167/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.3845e-04\n",
            "Epoch 168/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.3814e-04\n",
            "Epoch 169/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.3329e-04\n",
            "Epoch 170/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.3574e-04\n",
            "Epoch 171/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.3822e-04\n",
            "Epoch 172/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.3180e-04\n",
            "Epoch 173/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.3148e-04\n",
            "Epoch 174/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.3100e-04\n",
            "Epoch 175/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2827e-04\n",
            "Epoch 176/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2840e-04\n",
            "Epoch 177/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.3051e-04\n",
            "Epoch 178/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.3233e-04\n",
            "Epoch 179/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.3101e-04\n",
            "Epoch 180/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2650e-04\n",
            "Epoch 181/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2622e-04\n",
            "Epoch 182/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2703e-04\n",
            "Epoch 183/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.2374e-04\n",
            "Epoch 184/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2388e-04\n",
            "Epoch 185/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2290e-04\n",
            "Epoch 186/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2481e-04\n",
            "Epoch 187/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2516e-04\n",
            "Epoch 188/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.2168e-04\n",
            "Epoch 189/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2071e-04\n",
            "Epoch 190/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.1939e-04\n",
            "Epoch 191/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.2028e-04\n",
            "Epoch 192/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2365e-04\n",
            "Epoch 193/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.2060e-04\n",
            "Epoch 194/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.2067e-04\n",
            "Epoch 195/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1759e-04\n",
            "Epoch 196/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1988e-04\n",
            "Epoch 197/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1989e-04\n",
            "Epoch 198/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1698e-04\n",
            "Epoch 199/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1810e-04\n",
            "Epoch 200/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1840e-04\n",
            "Epoch 201/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1835e-04\n",
            "Epoch 202/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2177e-04\n",
            "Epoch 203/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1610e-04\n",
            "Epoch 204/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1577e-04\n",
            "Epoch 205/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1388e-04\n",
            "Epoch 206/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1456e-04\n",
            "Epoch 207/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1647e-04\n",
            "Epoch 208/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1236e-04\n",
            "Epoch 209/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1311e-04\n",
            "Epoch 210/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1253e-04\n",
            "Epoch 211/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1468e-04\n",
            "Epoch 212/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1244e-04\n",
            "Epoch 213/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1448e-04\n",
            "Epoch 214/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2313e-04\n",
            "Epoch 215/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2874e-04\n",
            "Epoch 216/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2082e-04\n",
            "Epoch 217/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1087e-04\n",
            "Epoch 218/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1342e-04\n",
            "Epoch 219/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1138e-04\n",
            "Epoch 220/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0848e-04\n",
            "Epoch 221/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0859e-04\n",
            "Epoch 222/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0799e-04\n",
            "Epoch 223/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0645e-04\n",
            "Epoch 224/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1573e-04\n",
            "Epoch 225/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0961e-04\n",
            "Epoch 226/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0768e-04\n",
            "Epoch 227/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0959e-04\n",
            "Epoch 228/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0982e-04\n",
            "Epoch 229/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0852e-04\n",
            "Epoch 230/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0741e-04\n",
            "Epoch 231/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0890e-04\n",
            "Epoch 232/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0725e-04\n",
            "Epoch 233/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1277e-04\n",
            "Epoch 234/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1347e-04\n",
            "Epoch 235/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0766e-04\n",
            "Epoch 236/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0714e-04\n",
            "Epoch 237/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0579e-04\n",
            "Epoch 238/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0713e-04\n",
            "Epoch 239/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0326e-04\n",
            "Epoch 240/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0355e-04\n",
            "Epoch 241/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0679e-04\n",
            "Epoch 242/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0686e-04\n",
            "Epoch 243/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0402e-04\n",
            "Epoch 244/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0743e-04\n",
            "Epoch 245/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0778e-04\n",
            "Epoch 246/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0479e-04\n",
            "Epoch 247/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0335e-04\n",
            "Epoch 248/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1126e-04\n",
            "Epoch 249/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0389e-04\n",
            "Epoch 250/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0750e-04\n",
            "Epoch 251/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0372e-04\n",
            "Epoch 252/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0163e-04\n",
            "Epoch 253/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0356e-04\n",
            "Epoch 254/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0361e-04\n",
            "Epoch 255/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0598e-04\n",
            "Epoch 256/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0506e-04\n",
            "Epoch 257/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0541e-04\n",
            "Epoch 258/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0531e-04\n",
            "Epoch 259/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0139e-04\n",
            "Epoch 260/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0196e-04\n",
            "Epoch 261/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0172e-04\n",
            "Epoch 262/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0317e-04\n",
            "Epoch 263/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0533e-04\n",
            "Epoch 264/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0368e-04\n",
            "Epoch 265/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0189e-04\n",
            "Epoch 266/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0252e-04\n",
            "Epoch 267/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0077e-04\n",
            "Epoch 268/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0518e-04\n",
            "Epoch 269/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0143e-04\n",
            "Epoch 270/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0196e-04\n",
            "Epoch 271/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0502e-04\n",
            "Epoch 272/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0129e-04\n",
            "Epoch 273/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0255e-04\n",
            "Epoch 274/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0108e-04\n",
            "Epoch 275/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0152e-04\n",
            "Epoch 276/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0138e-04\n",
            "Epoch 277/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0017e-04\n",
            "Epoch 278/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0259e-04\n",
            "Epoch 279/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0523e-04\n",
            "Epoch 280/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9995e-04\n",
            "Epoch 281/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0244e-04\n",
            "Epoch 282/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0177e-04\n",
            "Epoch 283/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9978e-04\n",
            "Epoch 284/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0300e-04\n",
            "Epoch 285/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0037e-04\n",
            "Epoch 286/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.0303e-04\n",
            "Epoch 287/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0763e-04\n",
            "Epoch 288/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0067e-04\n",
            "Epoch 289/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0371e-04\n",
            "Epoch 290/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9978e-04\n",
            "Epoch 291/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9960e-04\n",
            "Epoch 292/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0395e-04\n",
            "Epoch 293/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0075e-04\n",
            "Epoch 294/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0119e-04\n",
            "Epoch 295/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0046e-04\n",
            "Epoch 296/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0135e-04\n",
            "Epoch 297/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0250e-04\n",
            "Epoch 298/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0285e-04\n",
            "Epoch 299/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0522e-04\n",
            "Epoch 300/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0313e-04\n",
            "Epoch 301/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0454e-04\n",
            "Epoch 302/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0305e-04\n",
            "Epoch 303/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0103e-04\n",
            "Epoch 304/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0868e-04\n",
            "Epoch 305/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1073e-04\n",
            "Epoch 306/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0577e-04\n",
            "Epoch 307/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0351e-04\n",
            "Epoch 308/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0370e-04\n",
            "Epoch 309/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9981e-04\n",
            "Epoch 310/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0267e-04\n",
            "Epoch 311/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0041e-04\n",
            "Epoch 312/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0208e-04\n",
            "Epoch 313/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0217e-04\n",
            "Epoch 314/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0357e-04\n",
            "Epoch 315/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0707e-04\n",
            "Epoch 316/2000\n",
            "1566/1566 [==============================] - 0s 45us/step - loss: 5.0782e-04\n",
            "Epoch 317/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0139e-04\n",
            "Epoch 318/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0263e-04\n",
            "Epoch 319/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0432e-04\n",
            "Epoch 320/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0785e-04\n",
            "Epoch 321/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9949e-04\n",
            "Epoch 322/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9834e-04\n",
            "Epoch 323/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9830e-04\n",
            "Epoch 324/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0255e-04\n",
            "Epoch 325/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9833e-04\n",
            "Epoch 326/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9972e-04\n",
            "Epoch 327/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0454e-04\n",
            "Epoch 328/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0711e-04\n",
            "Epoch 329/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9787e-04\n",
            "Epoch 330/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0184e-04\n",
            "Epoch 331/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9990e-04\n",
            "Epoch 332/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0139e-04\n",
            "Epoch 333/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9874e-04\n",
            "Epoch 334/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9995e-04\n",
            "Epoch 335/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9915e-04\n",
            "Epoch 336/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 5.0053e-04\n",
            "Epoch 337/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0041e-04\n",
            "Epoch 338/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9793e-04\n",
            "Epoch 339/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0036e-04\n",
            "Epoch 340/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0089e-04\n",
            "Epoch 341/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0173e-04\n",
            "Epoch 342/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9840e-04\n",
            "Epoch 343/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0022e-04\n",
            "Epoch 344/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0661e-04\n",
            "Epoch 345/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9927e-04\n",
            "Epoch 346/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0536e-04\n",
            "Epoch 347/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0325e-04\n",
            "Epoch 348/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0349e-04\n",
            "Epoch 349/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0286e-04\n",
            "Epoch 350/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0242e-04\n",
            "Epoch 351/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0394e-04\n",
            "Epoch 352/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9800e-04\n",
            "Epoch 353/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0452e-04\n",
            "Epoch 354/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0913e-04\n",
            "Epoch 355/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0418e-04\n",
            "Epoch 356/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9827e-04\n",
            "Epoch 357/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0666e-04\n",
            "Epoch 358/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1522e-04\n",
            "Epoch 359/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9850e-04\n",
            "Epoch 360/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0072e-04\n",
            "Epoch 361/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9928e-04\n",
            "Epoch 362/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9928e-04\n",
            "Epoch 363/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9862e-04\n",
            "Epoch 364/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9941e-04\n",
            "Epoch 365/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0374e-04\n",
            "Epoch 366/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0176e-04\n",
            "Epoch 367/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0140e-04\n",
            "Epoch 368/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0222e-04\n",
            "Epoch 369/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9895e-04\n",
            "Epoch 370/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0043e-04\n",
            "Epoch 371/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0193e-04\n",
            "Epoch 372/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1113e-04\n",
            "Epoch 373/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0850e-04\n",
            "Epoch 374/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0159e-04\n",
            "Epoch 375/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0080e-04\n",
            "Epoch 376/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9899e-04\n",
            "Epoch 377/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0181e-04\n",
            "Epoch 378/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9864e-04\n",
            "Epoch 379/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9771e-04\n",
            "Epoch 380/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0817e-04\n",
            "Epoch 381/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0075e-04\n",
            "Epoch 382/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0206e-04\n",
            "Epoch 383/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9960e-04\n",
            "Epoch 384/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9871e-04\n",
            "Epoch 385/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9870e-04\n",
            "Epoch 386/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0112e-04\n",
            "Epoch 387/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9858e-04\n",
            "Epoch 388/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0392e-04\n",
            "Epoch 389/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0147e-04\n",
            "Epoch 390/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9987e-04\n",
            "Epoch 391/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9833e-04\n",
            "Epoch 392/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0122e-04\n",
            "Epoch 393/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0193e-04\n",
            "Epoch 394/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0103e-04\n",
            "Epoch 395/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9964e-04\n",
            "Epoch 396/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9824e-04\n",
            "Epoch 397/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0544e-04\n",
            "Epoch 398/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.3007e-04\n",
            "Epoch 399/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2669e-04\n",
            "Epoch 400/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0916e-04\n",
            "Epoch 401/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0285e-04\n",
            "Epoch 402/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9963e-04\n",
            "Epoch 403/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0254e-04\n",
            "Epoch 404/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0142e-04\n",
            "Epoch 405/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0151e-04\n",
            "Epoch 406/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9962e-04\n",
            "Epoch 407/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9755e-04\n",
            "Epoch 408/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9847e-04\n",
            "Epoch 409/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0183e-04\n",
            "Epoch 410/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0253e-04\n",
            "Epoch 411/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0233e-04\n",
            "Epoch 412/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0256e-04\n",
            "Epoch 413/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0294e-04\n",
            "Epoch 414/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0726e-04\n",
            "Epoch 415/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0931e-04\n",
            "Epoch 416/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0096e-04\n",
            "Epoch 417/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0002e-04\n",
            "Epoch 418/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9846e-04\n",
            "Epoch 419/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0441e-04\n",
            "Epoch 420/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0240e-04\n",
            "Epoch 421/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0914e-04\n",
            "Epoch 422/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1152e-04\n",
            "Epoch 423/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0339e-04\n",
            "Epoch 424/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0321e-04\n",
            "Epoch 425/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0837e-04\n",
            "Epoch 426/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9937e-04\n",
            "Epoch 427/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0029e-04\n",
            "Epoch 428/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0187e-04\n",
            "Epoch 429/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9933e-04\n",
            "Epoch 430/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.0437e-04\n",
            "Epoch 431/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9957e-04\n",
            "Epoch 432/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9944e-04\n",
            "Epoch 433/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0566e-04\n",
            "Epoch 434/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0106e-04\n",
            "Epoch 435/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9745e-04\n",
            "Epoch 436/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9878e-04\n",
            "Epoch 437/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0728e-04\n",
            "Epoch 438/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9938e-04\n",
            "Epoch 439/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0367e-04\n",
            "Epoch 440/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9803e-04\n",
            "Epoch 441/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0053e-04\n",
            "Epoch 442/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0171e-04\n",
            "Epoch 443/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9812e-04\n",
            "Epoch 444/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0172e-04\n",
            "Epoch 445/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9943e-04\n",
            "Epoch 446/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0433e-04\n",
            "Epoch 447/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0547e-04\n",
            "Epoch 448/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0260e-04\n",
            "Epoch 449/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9848e-04\n",
            "Epoch 450/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9906e-04\n",
            "Epoch 451/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9935e-04\n",
            "Epoch 452/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9904e-04\n",
            "Epoch 453/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9972e-04\n",
            "Epoch 454/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0006e-04\n",
            "Epoch 455/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9740e-04\n",
            "Epoch 456/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0011e-04\n",
            "Epoch 457/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0196e-04\n",
            "Epoch 458/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0680e-04\n",
            "Epoch 459/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0765e-04\n",
            "Epoch 460/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0237e-04\n",
            "Epoch 461/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0430e-04\n",
            "Epoch 462/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0116e-04\n",
            "Epoch 463/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9996e-04\n",
            "Epoch 464/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9978e-04\n",
            "Epoch 465/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9794e-04\n",
            "Epoch 466/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0351e-04\n",
            "Epoch 467/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 5.0530e-04\n",
            "Epoch 468/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0754e-04\n",
            "Epoch 469/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9926e-04\n",
            "Epoch 470/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9901e-04\n",
            "Epoch 471/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0305e-04\n",
            "Epoch 472/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0081e-04\n",
            "Epoch 473/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0039e-04\n",
            "Epoch 474/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0122e-04\n",
            "Epoch 475/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0103e-04\n",
            "Epoch 476/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0234e-04\n",
            "Epoch 477/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9875e-04\n",
            "Epoch 478/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9885e-04\n",
            "Epoch 479/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0411e-04\n",
            "Epoch 480/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9945e-04\n",
            "Epoch 481/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9770e-04\n",
            "Epoch 482/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0124e-04\n",
            "Epoch 483/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9933e-04\n",
            "Epoch 484/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0003e-04\n",
            "Epoch 485/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9801e-04\n",
            "Epoch 486/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9780e-04\n",
            "Epoch 487/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0286e-04\n",
            "Epoch 488/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0127e-04\n",
            "Epoch 489/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0105e-04\n",
            "Epoch 490/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0590e-04\n",
            "Epoch 491/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0110e-04\n",
            "Epoch 492/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9936e-04\n",
            "Epoch 493/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9862e-04\n",
            "Epoch 494/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0012e-04\n",
            "Epoch 495/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9798e-04\n",
            "Epoch 496/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9963e-04\n",
            "Epoch 497/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0461e-04\n",
            "Epoch 498/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0243e-04\n",
            "Epoch 499/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9770e-04\n",
            "Epoch 500/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9982e-04\n",
            "Epoch 501/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0173e-04\n",
            "Epoch 502/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1154e-04\n",
            "Epoch 503/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0472e-04\n",
            "Epoch 504/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0200e-04\n",
            "Epoch 505/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0254e-04\n",
            "Epoch 506/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9866e-04\n",
            "Epoch 507/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0286e-04\n",
            "Epoch 508/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9957e-04\n",
            "Epoch 509/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9722e-04\n",
            "Epoch 510/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0176e-04\n",
            "Epoch 511/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0062e-04\n",
            "Epoch 512/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9793e-04\n",
            "Epoch 513/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9745e-04\n",
            "Epoch 514/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9725e-04\n",
            "Epoch 515/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0104e-04\n",
            "Epoch 516/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0335e-04\n",
            "Epoch 517/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9606e-04\n",
            "Epoch 518/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0188e-04\n",
            "Epoch 519/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0020e-04\n",
            "Epoch 520/2000\n",
            "1566/1566 [==============================] - 0s 39us/step - loss: 4.9837e-04\n",
            "Epoch 521/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0039e-04\n",
            "Epoch 522/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9860e-04\n",
            "Epoch 523/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9756e-04\n",
            "Epoch 524/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0539e-04\n",
            "Epoch 525/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0692e-04\n",
            "Epoch 526/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9748e-04\n",
            "Epoch 527/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9735e-04\n",
            "Epoch 528/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0051e-04\n",
            "Epoch 529/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0021e-04\n",
            "Epoch 530/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0019e-04\n",
            "Epoch 531/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0472e-04\n",
            "Epoch 532/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0095e-04\n",
            "Epoch 533/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0597e-04\n",
            "Epoch 534/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0134e-04\n",
            "Epoch 535/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9974e-04\n",
            "Epoch 536/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9738e-04\n",
            "Epoch 537/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9841e-04\n",
            "Epoch 538/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9875e-04\n",
            "Epoch 539/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0135e-04\n",
            "Epoch 540/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0097e-04\n",
            "Epoch 541/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9767e-04\n",
            "Epoch 542/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9919e-04\n",
            "Epoch 543/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9874e-04\n",
            "Epoch 544/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9741e-04\n",
            "Epoch 545/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9626e-04\n",
            "Epoch 546/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0159e-04\n",
            "Epoch 547/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9895e-04\n",
            "Epoch 548/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0065e-04\n",
            "Epoch 549/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0813e-04\n",
            "Epoch 550/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9954e-04\n",
            "Epoch 551/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9745e-04\n",
            "Epoch 552/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9882e-04\n",
            "Epoch 553/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9990e-04\n",
            "Epoch 554/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0521e-04\n",
            "Epoch 555/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0923e-04\n",
            "Epoch 556/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0135e-04\n",
            "Epoch 557/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0359e-04\n",
            "Epoch 558/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0160e-04\n",
            "Epoch 559/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0180e-04\n",
            "Epoch 560/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9967e-04\n",
            "Epoch 561/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9878e-04\n",
            "Epoch 562/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9952e-04\n",
            "Epoch 563/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0158e-04\n",
            "Epoch 564/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9950e-04\n",
            "Epoch 565/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0754e-04\n",
            "Epoch 566/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0247e-04\n",
            "Epoch 567/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9931e-04\n",
            "Epoch 568/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9637e-04\n",
            "Epoch 569/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0087e-04\n",
            "Epoch 570/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0141e-04\n",
            "Epoch 571/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9731e-04\n",
            "Epoch 572/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9823e-04\n",
            "Epoch 573/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9787e-04\n",
            "Epoch 574/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9908e-04\n",
            "Epoch 575/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9708e-04\n",
            "Epoch 576/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9885e-04\n",
            "Epoch 577/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0297e-04\n",
            "Epoch 578/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9993e-04\n",
            "Epoch 579/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9861e-04\n",
            "Epoch 580/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9825e-04\n",
            "Epoch 581/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9990e-04\n",
            "Epoch 582/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0315e-04\n",
            "Epoch 583/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9917e-04\n",
            "Epoch 584/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0023e-04\n",
            "Epoch 585/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0216e-04\n",
            "Epoch 586/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0235e-04\n",
            "Epoch 587/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9979e-04\n",
            "Epoch 588/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9965e-04\n",
            "Epoch 589/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0238e-04\n",
            "Epoch 590/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1418e-04\n",
            "Epoch 591/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0354e-04\n",
            "Epoch 592/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0147e-04\n",
            "Epoch 593/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9822e-04\n",
            "Epoch 594/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0321e-04\n",
            "Epoch 595/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0020e-04\n",
            "Epoch 596/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 5.0292e-04\n",
            "Epoch 597/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0115e-04\n",
            "Epoch 598/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9873e-04\n",
            "Epoch 599/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9806e-04\n",
            "Epoch 600/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9915e-04\n",
            "Epoch 601/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9893e-04\n",
            "Epoch 602/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0027e-04\n",
            "Epoch 603/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0139e-04\n",
            "Epoch 604/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0066e-04\n",
            "Epoch 605/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9762e-04\n",
            "Epoch 606/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0236e-04\n",
            "Epoch 607/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9778e-04\n",
            "Epoch 608/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0176e-04\n",
            "Epoch 609/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9876e-04\n",
            "Epoch 610/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9767e-04\n",
            "Epoch 611/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9766e-04\n",
            "Epoch 612/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0128e-04\n",
            "Epoch 613/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0823e-04\n",
            "Epoch 614/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0093e-04\n",
            "Epoch 615/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9867e-04\n",
            "Epoch 616/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9798e-04\n",
            "Epoch 617/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0139e-04\n",
            "Epoch 618/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0165e-04\n",
            "Epoch 619/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9942e-04\n",
            "Epoch 620/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9719e-04\n",
            "Epoch 621/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9973e-04\n",
            "Epoch 622/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9937e-04\n",
            "Epoch 623/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0230e-04\n",
            "Epoch 624/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9777e-04\n",
            "Epoch 625/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0507e-04\n",
            "Epoch 626/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0318e-04\n",
            "Epoch 627/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9727e-04\n",
            "Epoch 628/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9971e-04\n",
            "Epoch 629/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0527e-04\n",
            "Epoch 630/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0167e-04\n",
            "Epoch 631/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0530e-04\n",
            "Epoch 632/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0073e-04\n",
            "Epoch 633/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9782e-04\n",
            "Epoch 634/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9916e-04\n",
            "Epoch 635/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9659e-04\n",
            "Epoch 636/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9666e-04\n",
            "Epoch 637/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0278e-04\n",
            "Epoch 638/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0256e-04\n",
            "Epoch 639/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0136e-04\n",
            "Epoch 640/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9827e-04\n",
            "Epoch 641/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9816e-04\n",
            "Epoch 642/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0681e-04\n",
            "Epoch 643/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0894e-04\n",
            "Epoch 644/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9909e-04\n",
            "Epoch 645/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0139e-04\n",
            "Epoch 646/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9994e-04\n",
            "Epoch 647/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0212e-04\n",
            "Epoch 648/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9975e-04\n",
            "Epoch 649/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9935e-04\n",
            "Epoch 650/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9685e-04\n",
            "Epoch 651/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9767e-04\n",
            "Epoch 652/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9883e-04\n",
            "Epoch 653/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9774e-04\n",
            "Epoch 654/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0106e-04\n",
            "Epoch 655/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9712e-04\n",
            "Epoch 656/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9787e-04\n",
            "Epoch 657/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9759e-04\n",
            "Epoch 658/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0215e-04\n",
            "Epoch 659/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9902e-04\n",
            "Epoch 660/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0000e-04\n",
            "Epoch 661/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9938e-04\n",
            "Epoch 662/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9782e-04\n",
            "Epoch 663/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0265e-04\n",
            "Epoch 664/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0412e-04\n",
            "Epoch 665/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0069e-04\n",
            "Epoch 666/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9678e-04\n",
            "Epoch 667/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0260e-04\n",
            "Epoch 668/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9686e-04\n",
            "Epoch 669/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0202e-04\n",
            "Epoch 670/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9936e-04\n",
            "Epoch 671/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9716e-04\n",
            "Epoch 672/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0910e-04\n",
            "Epoch 673/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0604e-04\n",
            "Epoch 674/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9940e-04\n",
            "Epoch 675/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9878e-04\n",
            "Epoch 676/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9767e-04\n",
            "Epoch 677/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0155e-04\n",
            "Epoch 678/2000\n",
            "1566/1566 [==============================] - 0s 41us/step - loss: 5.0032e-04\n",
            "Epoch 679/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9692e-04\n",
            "Epoch 680/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0136e-04\n",
            "Epoch 681/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0159e-04\n",
            "Epoch 682/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0266e-04\n",
            "Epoch 683/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9755e-04\n",
            "Epoch 684/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0392e-04\n",
            "Epoch 685/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0607e-04\n",
            "Epoch 686/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0190e-04\n",
            "Epoch 687/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9904e-04\n",
            "Epoch 688/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9740e-04\n",
            "Epoch 689/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9614e-04\n",
            "Epoch 690/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0077e-04\n",
            "Epoch 691/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9743e-04\n",
            "Epoch 692/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9963e-04\n",
            "Epoch 693/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9757e-04\n",
            "Epoch 694/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9642e-04\n",
            "Epoch 695/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9844e-04\n",
            "Epoch 696/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9879e-04\n",
            "Epoch 697/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0226e-04\n",
            "Epoch 698/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0094e-04\n",
            "Epoch 699/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9637e-04\n",
            "Epoch 700/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9893e-04\n",
            "Epoch 701/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0131e-04\n",
            "Epoch 702/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0070e-04\n",
            "Epoch 703/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0156e-04\n",
            "Epoch 704/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9982e-04\n",
            "Epoch 705/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9751e-04\n",
            "Epoch 706/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0220e-04\n",
            "Epoch 707/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9768e-04\n",
            "Epoch 708/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9798e-04\n",
            "Epoch 709/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0049e-04\n",
            "Epoch 710/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9879e-04\n",
            "Epoch 711/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9814e-04\n",
            "Epoch 712/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0054e-04\n",
            "Epoch 713/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9921e-04\n",
            "Epoch 714/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9659e-04\n",
            "Epoch 715/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0271e-04\n",
            "Epoch 716/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9963e-04\n",
            "Epoch 717/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0221e-04\n",
            "Epoch 718/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9715e-04\n",
            "Epoch 719/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9905e-04\n",
            "Epoch 720/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9585e-04\n",
            "Epoch 721/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9814e-04\n",
            "Epoch 722/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0064e-04\n",
            "Epoch 723/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0003e-04\n",
            "Epoch 724/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0370e-04\n",
            "Epoch 725/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1246e-04\n",
            "Epoch 726/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0681e-04\n",
            "Epoch 727/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9860e-04\n",
            "Epoch 728/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9993e-04\n",
            "Epoch 729/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0422e-04\n",
            "Epoch 730/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 5.0140e-04\n",
            "Epoch 731/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.0499e-04\n",
            "Epoch 732/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0200e-04\n",
            "Epoch 733/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0013e-04\n",
            "Epoch 734/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9973e-04\n",
            "Epoch 735/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9842e-04\n",
            "Epoch 736/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9768e-04\n",
            "Epoch 737/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9703e-04\n",
            "Epoch 738/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9672e-04\n",
            "Epoch 739/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0786e-04\n",
            "Epoch 740/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0366e-04\n",
            "Epoch 741/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0258e-04\n",
            "Epoch 742/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9698e-04\n",
            "Epoch 743/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9984e-04\n",
            "Epoch 744/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9871e-04\n",
            "Epoch 745/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0046e-04\n",
            "Epoch 746/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0790e-04\n",
            "Epoch 747/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0117e-04\n",
            "Epoch 748/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9657e-04\n",
            "Epoch 749/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9729e-04\n",
            "Epoch 750/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9697e-04\n",
            "Epoch 751/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9856e-04\n",
            "Epoch 752/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0032e-04\n",
            "Epoch 753/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0337e-04\n",
            "Epoch 754/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0119e-04\n",
            "Epoch 755/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0156e-04\n",
            "Epoch 756/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9766e-04\n",
            "Epoch 757/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9820e-04\n",
            "Epoch 758/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9984e-04\n",
            "Epoch 759/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9669e-04\n",
            "Epoch 760/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0921e-04\n",
            "Epoch 761/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9822e-04\n",
            "Epoch 762/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9711e-04\n",
            "Epoch 763/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9751e-04\n",
            "Epoch 764/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0672e-04\n",
            "Epoch 765/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9983e-04\n",
            "Epoch 766/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9787e-04\n",
            "Epoch 767/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9715e-04\n",
            "Epoch 768/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0752e-04\n",
            "Epoch 769/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0067e-04\n",
            "Epoch 770/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9944e-04\n",
            "Epoch 771/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9479e-04\n",
            "Epoch 772/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0584e-04\n",
            "Epoch 773/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0003e-04\n",
            "Epoch 774/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9891e-04\n",
            "Epoch 775/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0382e-04\n",
            "Epoch 776/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0130e-04\n",
            "Epoch 777/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9769e-04\n",
            "Epoch 778/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9741e-04\n",
            "Epoch 779/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9728e-04\n",
            "Epoch 780/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9785e-04\n",
            "Epoch 781/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9756e-04\n",
            "Epoch 782/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9876e-04\n",
            "Epoch 783/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9888e-04\n",
            "Epoch 784/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0103e-04\n",
            "Epoch 785/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0050e-04\n",
            "Epoch 786/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0259e-04\n",
            "Epoch 787/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0427e-04\n",
            "Epoch 788/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9823e-04\n",
            "Epoch 789/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 5.0412e-04\n",
            "Epoch 790/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0342e-04\n",
            "Epoch 791/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9828e-04\n",
            "Epoch 792/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9996e-04\n",
            "Epoch 793/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9916e-04\n",
            "Epoch 794/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0408e-04\n",
            "Epoch 795/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0071e-04\n",
            "Epoch 796/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0285e-04\n",
            "Epoch 797/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0186e-04\n",
            "Epoch 798/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0314e-04\n",
            "Epoch 799/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9761e-04\n",
            "Epoch 800/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9750e-04\n",
            "Epoch 801/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9814e-04\n",
            "Epoch 802/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9702e-04\n",
            "Epoch 803/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9747e-04\n",
            "Epoch 804/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9702e-04\n",
            "Epoch 805/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9619e-04\n",
            "Epoch 806/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9685e-04\n",
            "Epoch 807/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9867e-04\n",
            "Epoch 808/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0263e-04\n",
            "Epoch 809/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9668e-04\n",
            "Epoch 810/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9824e-04\n",
            "Epoch 811/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9760e-04\n",
            "Epoch 812/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9830e-04\n",
            "Epoch 813/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9589e-04\n",
            "Epoch 814/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9682e-04\n",
            "Epoch 815/2000\n",
            " 100/1566 [>.............................] - ETA: 0s - loss: 1.7929e-05"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-W-Lh2jhGOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7290c754-1c0d-47a7-879c-2adc81523841"
      },
      "source": [
        "print(np.tanh(reg_model.layers[0].get_weights()[3]))\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6268091]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE90OgzpcWB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a205faf2-411d-47de-df6b-2099652247b7"
      },
      "source": [
        "alpha_rnn_pred_train = reg_model.predict(x_train_reg, verbose=1)\n",
        "alpha_rnn_pred_test = reg_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 1s 553us/step\n",
            "384/384 [==============================] - 0s 57us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaT2m6A7erEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "709e5970-7a03-43e5-ed59-cb351f21fda9"
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"orange\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpha_rnn_pred_test[:, 0], color=\"red\", label=\"RNN Predict (Testing)\")\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxS4AYIZf57S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN_model(n_units=10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(SimpleRNN(n_units, activation='tanh', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6MLiMOyf-tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model=RNN_model(hidden_units, l1_reg)\n",
        "rnn_model.fit(x_train_reg,y_train_reg,epochs=2000, batch_size=100, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIXAHIj2i_Qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT3ucd9Fid57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d305437-b4aa-422c-9dee-f00a3c61038c"
      },
      "source": [
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 33us/step\n",
            "384/384 [==============================] - 0s 38us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxcTtqC7iixP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "0c132166-ccdc-4d79-a3b8-eba46e51b9ec"
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"black\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], alpha_rnn_pred_test[:, 0], color=\"red\", label=\"alpha RNN Predict (Testing)\")\n",
        "plt.plot(df_test.index[n_steps:], rnn_pred_test[:, 0], color=\"blue\", label=\"RNN Predict (Testing)\")\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHHCAYAAAD3dE1gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSPlJCLiIiIiOSREnIRERERkTz6/62Xq3t+RszOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}