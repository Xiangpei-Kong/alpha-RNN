{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alpha_RNNs_regime_switching_vols(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfrdixon/alpha-RNN/blob/master/Alpha_RNNs_regime_switching_vols(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zm20LoTnuImQ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9BywHfc63kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "a686b404-eec8-4a08-904a-ca3839d58401"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "import os# Generate switching data set\n",
        "import random\n",
        "\n",
        "\n",
        "# Imports for alpha_rnns \n",
        "from IPython import display\n",
        "import tensorflow.compat.v1 as tf   \n",
        "tf.disable_v2_behavior()\n",
        "# Imports for stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1,l2\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7WLRTI-WuTTU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYzhUo7CuZLa",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, eps=0.01):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    step_length=np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      mu[i*step_length:((i*step_length)+1)]=step_size*(-1)**i\n",
        "     \n",
        "    for i in range(1, length):\n",
        "        sigma[i]=sigma[i-1] + mu[i] + eps*np.random.normal(0,1)\n",
        "        \n",
        "    return sigma   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DuOojJHWukx-",
        "colab": {}
      },
      "source": [
        "def generate_vol_sample(length, sigma_0, n_steps, step_size, p, eps=0.01, shift=0):\n",
        "    sigma = np.array([0]*length, dtype='float64')\n",
        "    sigma[0]=sigma_0\n",
        "    mu = np.array([0]*length, dtype='float64')\n",
        "    phi = np.array([0]*length*p, dtype='float64').reshape(length,p)\n",
        "    #phi2 = np.array([0]*length, dtype='float64')\n",
        "    step_length=100 #np.int(np.floor(np.float(length)/(2.0*n_steps)))\n",
        "    \n",
        "    for i in range(2*n_steps):\n",
        "      #mu[i*step_length:((i*step_length)+1)]=step_size #*(-1)**i\n",
        "      mu[i*step_length:((i+1)*step_length)]= step_size*(-1)**i\n",
        "      if i%2==0:  \n",
        "        phi[i*step_length:((i+1)*step_length),:]= 0.02\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=1.0\n",
        "      else:\n",
        "        phi[i*step_length:((i+1)*step_length),:]=0.01\n",
        "        #phi2[i*step_length:((i+1)*step_length)]=0.5\n",
        "    for i in range(p, length):\n",
        "        sigma[i]= mu[i-1] + np.random.normal(0,eps)\n",
        "        for j in range(p):\n",
        "          sigma[i]+=phi[i-1,j]*sigma[i-j]  \n",
        "        \n",
        "    return (sigma+shift)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JztXCwy5upi7",
        "colab": {}
      },
      "source": [
        "p = 30 # the number of lags (in both the data and the models)\n",
        "vols=generate_vol_sample(2000, 0.25, 15, 0.1, p, 1e-4, 0.13)[p:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "untInWSMuxSb",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(vols, columns=['vol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7wGqBI6u0M6",
        "outputId": "e0936296-877c-42e7-e7f1-ae9dcc0061e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "use_features = ['vol'] \n",
        "target = 'vol'\n",
        "n_steps = 10 # number of lags to include in the model\n",
        "\n",
        "train_weight = 0.8\n",
        "split = int(len(df)*train_weight)\n",
        "\n",
        "df_train = df[use_features].iloc[:split]\n",
        "print(df_train)\n",
        "df_test = df[use_features].iloc[split:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           vol\n",
            "0     0.232024\n",
            "1     0.233982\n",
            "2     0.236243\n",
            "3     0.238309\n",
            "4     0.240423\n",
            "...        ...\n",
            "1571  0.149751\n",
            "1572  0.153012\n",
            "1573  0.156434\n",
            "1574  0.159678\n",
            "1575  0.163013\n",
            "\n",
            "[1576 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTCv6NRBvBa4",
        "colab": {}
      },
      "source": [
        "def get_lagged_features(value, n_steps):\n",
        "    lag_list = []\n",
        "    for lag in range(n_steps, 0, -1):\n",
        "        lag_list.append(value.shift(lag))\n",
        "    return pd.concat(lag_list, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE0sdbrNvM3R",
        "colab": {}
      },
      "source": [
        "x_train_list = []\n",
        "for use_feature in use_features:\n",
        "    x_train_reg = get_lagged_features(df_train, n_steps).dropna()\n",
        "    x_train_list.append(x_train_reg)\n",
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "\n",
        "col_ords = []\n",
        "for i in range(n_steps):\n",
        "    for j in range(len(use_features)):\n",
        "        col_ords.append(i + j * n_steps)\n",
        "\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
        "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
        "\n",
        "x_test_list = []\n",
        "for use_feature in use_features:\n",
        "    x_test_reg = get_lagged_features(df_test, n_steps).dropna()\n",
        "    x_test_list.append(x_test_reg)\n",
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
        "\n",
        "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBwt67S4vQmu",
        "outputId": "8aebb014-4ad2-4cc0-9802-773a33889d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1566, 10, 1) (1566, 1, 1) (384, 10, 1) (384, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AFhK-lgavTuB",
        "colab": {}
      },
      "source": [
        "train_batch_size = y_train_reg.shape[0]\n",
        "test_batch_size = y_test_reg.shape[0]\n",
        "\n",
        "time_size = y_train_reg.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COZs3KSovWsf",
        "outputId": "c36dbccb-c58d-4441-9146-95928cbcdcf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GxhSP3k1vcIF",
        "colab": {}
      },
      "source": [
        "class alphaRNN:\n",
        "    \"\"\"Adapted from the Implementation of a Gated Recurrent Unit (GRU) as described in [1]. This is a GRU without a reset gate. It uses a hidden layer for smoothing\n",
        "    \n",
        "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    input_dimensions: int\n",
        "        The size of the input vectors (x_t).\n",
        "    hidden_size: int\n",
        "        The size of the hidden layer vectors (h_t).\n",
        "    dtype: obj\n",
        "        The datatype used for the variables and constants (optional).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
        "        self.input_dimensions = input_dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
        "        self.Walpha = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Walpha')\n",
        "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
        "        \n",
        "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
        "        self.Ualpha = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ualpha')\n",
        "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
        "        \n",
        "        # Biases for hidden vectors of shape (hidden_size,)\n",
        "        self.balpha = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='balpha')\n",
        "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
        "        \n",
        "        # Define the input layer placeholder\n",
        "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
        "        self.alpha_t = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='alpha_t')\n",
        "        # Put the time-dimension upfront for the scan operator\n",
        "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
        "        \n",
        "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
        "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
        "        \n",
        "        # Perform the scan operator\n",
        "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
        "        \n",
        "        # Transpose the result back\n",
        "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
        "\n",
        "    def forward_pass(self, h_tm1, x_t):\n",
        "        \"\"\"Perform a forward pass.\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        h_tm1: np.matrix\n",
        "            The hidden state at the previous timestep (h_{t-1}).\n",
        "        x_t: np.matrix\n",
        "            The input vector.\n",
        "        \"\"\"\n",
        "        # Update alpha_t\n",
        "        self.alpha_t = tf.sigmoid(tf.matmul(x_t, self.Walpha) + tf.matmul(h_tm1, self.Ualpha) + self.balpha)\n",
        "        tf.add_to_collection('alpha_t', self.alpha_t)\n",
        "        # Update hidden state h_t\n",
        "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(h_tm1, self.Uh) + self.bh)\n",
        "        \n",
        "        \n",
        "        # Update the (smoothed) hidden state with exponential smoothing\n",
        "        #h_t = tf.multiply(1 - self.alpha_t, h_tm1) + tf.multiply(self.alpha_t, h_proposal)\n",
        "        h_t = tf.multiply(1 - tf.tanh(self.alpha_t), h_tm1) + tf.multiply(tf.tanh(self.alpha_t), h_proposal) \n",
        "        return h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GJxDq4vyvg-4",
        "outputId": "a7f14f04-fc69-4dbe-9463-43ed81920725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "tf.get_collection('alpha_t')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'h_t_transposed/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_2/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_3/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_4/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_5/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_6/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_7/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_8/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_9/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_10/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_11/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_12/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_13/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_14/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_15/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_16/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_17/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_18/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_19/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_20/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_21/while/Sigmoid:0' shape=(?, 10) dtype=float64>,\n",
              " <tf.Tensor 'h_t_transposed_22/while/Sigmoid:0' shape=(?, 10) dtype=float64>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4TI-54dIvkZM",
        "colab": {}
      },
      "source": [
        "class simpleAlphaRNN:\n",
        "    \"\"\"Adapted from the Implementation of a Gated Recurrent Unit (GRU) as described in [1]. This is a GRU without a reset gate. \n",
        "       It uses a scalar smoothing\n",
        "    \n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    \n",
        "    input_dimensions: int\n",
        "        The size of the input vectors (x_t).\n",
        "    hidden_size: int\n",
        "        The size of the hidden layer vectors (h_t).\n",
        "    dtype: obj\n",
        "        The datatype used for the variables and constants (optional).\n",
        "        \n",
        "    Todo\n",
        "    --------\n",
        "    1) how to constrain alpha to be in [0,1]?\n",
        "    2) when hidden size >1, how to couple with scalar alpha?\n",
        "    3) Should alpha be a vector of size hidden_size (is this better)?\n",
        "    4) Get value of fitted alpha (see Diagnostics (visualize fitted values)) below\n",
        "    4) Adapt the training to use early stopping (instead of fixed number of epoches)\n",
        "    5) Compare with GRU, LSTM, simpleRNN\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
        "        self.input_dimensions = input_dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
        "        \n",
        "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
        "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
        "        \n",
        "        # Biases for hidden vectors of shape (hidden_size,)\n",
        "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
        "        \n",
        "        # Define the input layer placeholder\n",
        "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
        "        self.alpha = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='alpha')\n",
        "        \n",
        "        #tf.placeholder(dtype=tf.float64, shape=(None, 1), name='alpha')\n",
        "        # Put the time-dimension upfront for the scan operator\n",
        "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
        "        \n",
        "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
        "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
        "        \n",
        "        # Perform the scan operator\n",
        "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
        "        \n",
        "        # Transpose the result back\n",
        "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
        "\n",
        "    def forward_pass(self, h_tm1, x_t):\n",
        "        \"\"\"Perform a forward pass.\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        h_tm1: np.matrix\n",
        "            The hidden state at the previous timestep (h_{t-1}).\n",
        "        x_t: np.matrix\n",
        "            The input vector.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Update hidden state h_t\n",
        "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(h_tm1, self.Uh) + self.bh)\n",
        "        \n",
        "        # Update the (smoothed) hidden state with exponential smoothing\n",
        "        h_t = tf.multiply(1 - tf.tanh(self.alpha), h_tm1) + tf.multiply(tf.tanh(self.alpha), h_proposal)\n",
        "        \n",
        "        return h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zhIvjFLhvqWD",
        "colab": {}
      },
      "source": [
        "    \n",
        "#%% (3) Initialize and train the model.\n",
        "\n",
        "# The input has 2 dimensions: dimension 0 is reserved for the first term and dimension 1 is reserved for the second term\n",
        "input_dimensions = 1\n",
        "\n",
        "# Arbitrary number for the size of the hidden state\n",
        "hidden_size = 10 #<= MFD: vary this between 1,2,5,10,20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I6gTEbjbvuIF",
        "colab": {}
      },
      "source": [
        "def train(alpharnn, train_x, val_x, train_y, val_y, max_epochs=2000, batch_size=100):\n",
        "    \n",
        "  # Create a placeholder for the expected output\n",
        "  expected_output_batch_train = tf.placeholder(dtype=tf.float64, shape=(batch_size, time_size,1), name='expected_output_batch_train')\n",
        "  expected_output_train = tf.placeholder(dtype=tf.float64, shape=(train_x.shape[0], time_size,1), name='expected_output_train')\n",
        "  #expected_output_test = tf.placeholder(dtype=tf.float64, shape(test_x.shape[0], time_size,1), name='expected_output_test')\n",
        "  expected_output_val = tf.placeholder(dtype=tf.float64, shape=(val_x.shape[0], time_size,1), name='expected_output_val')\n",
        "\n",
        "  #expected_output = tf.placeholder(dtype=tf.float64, shape=(batch_size, time_size), name='expected_output')\n",
        "\n",
        "  # Just use quadratic loss\n",
        "  train_batch_loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output_batch_train, 2)) / float(batch_size)\n",
        "  train_loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output_train, 2)) / float(train_x.shape[0])\n",
        "  validation_loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output_val, 2)) / float(val_x.shape[0])\n",
        "\n",
        "  #accuracy = tf.reduce_mean(tf.cast(validation_loss, tf.float64))\n",
        "\n",
        "  # Use the Adam optimizer for training\n",
        "  train_step = tf.train.AdamOptimizer().minimize(train_batch_loss)\n",
        "    \n",
        "  # Initialize the losses\n",
        "  train_losses = []\n",
        "  validation_losses = []\n",
        "      \n",
        "  # Initialize all the variables\n",
        "  init_variables = tf.global_variables_initializer()\n",
        "      \n",
        "  #myNumpyData = np.ones([10,20])\n",
        "  session.run(init_variables) # , {gru.Y: np.ones([20,20])})\n",
        " \n",
        "  # Perform all the iterations\n",
        "  patience_cnt = 0\n",
        "  for epoch in range(max_epochs): # Compute the losses\n",
        "        patience = 50\n",
        "        min_delta = 0.000001\n",
        "        \n",
        "\n",
        "        total_batch = int(train_x.shape[0] / batch_size)\n",
        "    \n",
        "        for i in range(total_batch):\n",
        "          batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
        "          batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
        "          session.run([train_step], feed_dict={alpharnn.input_layer: batch_x, expected_output_batch_train: batch_y})        \n",
        "        train_loss_ = session.run(train_loss, feed_dict={alpharnn.input_layer: train_x, expected_output_train: train_y})\n",
        "        validation_loss_ = session.run(validation_loss, feed_dict={alpharnn.input_layer: val_x, expected_output_val: val_y})\n",
        "\n",
        "        # Log the losses\n",
        "        train_losses += [train_loss_]\n",
        "        validation_losses += [validation_loss_]\n",
        "          #mse = mean_squared_error(train_losses,validation_losses) <= this is wrong! \n",
        "          # I know...it's original values and the predicted values\n",
        "        #keras uses: model.compile(loss='mean_squared_error', optimizer='sgd') \n",
        "        # and: keras.losses.mean_squared_error(y_true, y_pred)\n",
        "        #y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "        #MSE = np.square(np.subtract(y_test_reg,y_predicted)).mean()\n",
        "\n",
        "        if epoch % 50 == 0: \n",
        "            print('Epoch ', epoch, '/', max_epochs, ': ',\n",
        "                      \"\\tTraining Loss: {:.5f}\".format(train_loss_),\n",
        "                      \"\\tValidation Loss: {:.5f}\".format(validation_loss_),\n",
        "                      #\"\\tloss:\", MSE                      \n",
        "\n",
        "                 )\n",
        "        if epoch > 0 and (validation_losses[epoch-1] - validation_losses[epoch]) > min_delta:\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            #print(patience_cnt)\n",
        "        if patience_cnt > patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "  saver = tf.train.Saver()\n",
        "  saved_path = saver.save(session, './saved_variable')          \n",
        "  #print('\\nMSE: ',mse)\n",
        "  plt.plot(train_losses, '-b', label='Train loss')\n",
        "  plt.plot(validation_losses, '-r', label='Validation loss')\n",
        "  plt.legend(loc=0)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "  return alpharnn, validation_losses[-1]        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GP-8IPH9IFa",
        "colab": {}
      },
      "source": [
        "## MFD: ignore - not correct\n",
        "def cross_validate(session, split_size):\n",
        "\n",
        "  results = []\n",
        "\n",
        "  tscv = TimeSeriesSplit(split_size)\n",
        "    \n",
        "  \"\"\"\n",
        "  The training set has size i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1) \n",
        "  in the i``th split, with a test set of size ``n_samples//(n_splits + 1), \n",
        "  where n_samples is the number of samples.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  for train_idx, val_idx in tscv.split(x_train_reg):\n",
        "    print(\"TRAIN:\", train_idx.shape, \"TEST:\", val_idx.shape)\n",
        "    expected_output_val = tf.placeholder(dtype=tf.float64, shape=(len(val_idx), time_size,1), name='expected_output_val')\n",
        "\n",
        "    train_x = x_train_reg[train_idx]\n",
        "    train_y = y_train_reg[train_idx]\n",
        "\n",
        "    val_x = x_train_reg[val_idx]\n",
        "    val_y = y_train_reg[val_idx]\n",
        "    \n",
        "    train(alpharnn, train_x, val_x, train_y, val_y)\n",
        "    val_loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output_val, 2)) / float(len(val_idx))\n",
        "  \n",
        "    # MFD: suggest to change to val_loss and not use train_loss\n",
        "    results.append(session.run(val_loss, feed_dict={alpharnn.input_layer: val_x, expected_output_val: val_y}))\n",
        "\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LYpJCmZEGo-t",
        "colab": {}
      },
      "source": [
        "## MFD: ignore - not correct\n",
        "def cross_validate_t(session, split_size):\n",
        "\n",
        "  models = []\n",
        "\n",
        "  tscv = TimeSeriesSplit(split_size)\n",
        "    \n",
        "  \"\"\"\n",
        "  The training set has size i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1) \n",
        "  in the i``th split, with a test set of size ``n_samples//(n_splits + 1), \n",
        "  where n_samples is the number of samples.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  for train_idx, val_idx in tscv.split(x_train_reg):\n",
        "    print(\"TRAIN:\", train_idx.shape, \"TEST:\", val_idx.shape)\n",
        "    #expected_output_val = tf.placeholder(dtype=tf.float64, shape=(len(val_idx), time_size,1), name='expected_output_val')\n",
        "\n",
        "    train_x = x_train_reg[train_idx]\n",
        "    train_y = y_train_reg[train_idx]\n",
        "\n",
        "    val_x = x_train_reg[val_idx]\n",
        "    val_y = y_train_reg[val_idx]\n",
        "    \n",
        "    model=train(alpharnn_t, train_x, val_x, train_y, val_y)\n",
        "    #val_loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output_val, 2)) / float(len(val_idx))\n",
        "  \n",
        "    # MFD: suggest to change to val_loss and not use train_loss\n",
        "    #results.append(session.run(val_loss, feed_dict={alpharnn_t.input_layer: val_x, expected_output_val: val_y}))\n",
        "    models.append(model)\n",
        "  return models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vml889xGvzjN",
        "outputId": "cee704f5-5b93-4f5b-acda-d637788ede05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "\n",
        "\n",
        "#MSE_train = 0\n",
        "#MSE_test = 0 \n",
        "#models = []\n",
        "#n_splits = 10\n",
        "#tscv = TimeSeriesSplit(n_splits = 10)\n",
        "#for train_fold, test_fold in tscv.split(x_train_reg):\n",
        "  #x_train, x_test = x_train_reg[train_fold], x_train_reg[test_fold]\n",
        "  #y_train, y_test = y_train_reg[train_fold], y_test_reg[test_fold]\n",
        "session = tf.Session()\n",
        "  # Add an additional layer on top of each of the hidden state outputs\n",
        "alpharnn = simpleAlphaRNN(input_dimensions, hidden_size)\n",
        "W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
        "b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
        "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, alpharnn.h_t) \n",
        "train(alpharnn, x_train_reg, x_test_reg, y_train_reg, y_test_reg) \n",
        "\n",
        "#for model in models:\n",
        "#y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "#y_predicted_ar =np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "#for i in range(y_predicted_t.shape[0]):\n",
        "  #y_predicted_ar[i]=y_predicted[i][n_steps-1][0]\n",
        "  #MSE_test += mean_squared_error(y_test[:,0],y_predicted_ar[i])\n",
        "\n",
        "#y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_train_reg})\n",
        "#y_predicted_ar_train =np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "#for i in range(y_predicted.shape[0]):\n",
        "  #y_predicted_ar_train[i]=y_predicted[i][n_steps-1][0]\n",
        "  #MSE_train += mean_squared_error(y_train[:,0],y_predicted_ar_train[i])\n",
        "  \n",
        "  #MSE_train_alpha = MSE_train/n_splits\n",
        "  #print(\"MSE train alpha = \" + str(MSE_train_alpha))\n",
        "  #MSE_test_alpha = MSE_test/n_splits\n",
        "  #print(\"MSE test alpha = \" + str(MSE_test_alpha))\n",
        "  #MSE_train_alpha_std = np.math.sqrt(MSE_train_alpha/(n_splits-1))\n",
        "  #MSE_test_alpha_std = np.math.sqrt(MSE_test_alpha/(n_splits-1))\n",
        "  #print(\"MSE_train_alpha_std = \" + str(MSE_train_alpha_std))\n",
        "  #print(\"MSE_test_alpha_std = \" + str(MSE_test_alpha_std))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.19371 \tValidation Loss: 0.20065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ba114b44df7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpharnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpharnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#for model in models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-9912f78c8278>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(alpharnn, train_x, val_x, train_y, val_y, max_epochs, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m           \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0malpharnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_output_batch_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mtrain_loss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0malpharnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_output_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mvalidation_loss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0malpharnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_output_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0mfeed_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mflatten_dict_items\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input must be a dictionary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0mflat_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_dictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(d, **kw)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miterlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgxzSJApJFKo",
        "colab": {}
      },
      "source": [
        "#y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_train_reg})\n",
        "#y_predicted_ar_train=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "#for i in range(y_predicted.shape[0]):\n",
        "#     y_predicted_ar_train[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xH1TqhlbRfRw",
        "colab": {}
      },
      "source": [
        "#y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "#y_predicted_ar=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "#for i in range(y_predicted.shape[0]):\n",
        "#     y_predicted_ar[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTIKiBet463a"
      },
      "source": [
        "This is the main workflow=>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6t7cCuRs4kXJ",
        "outputId": "7fb14b61-0775-4eb5-9c3e-90b151e9c896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "models = []\n",
        "MSE_train = 0 \n",
        "MSE_test = 0\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits)\n",
        "hidden_sizes=[1,2,5,10,20]\n",
        "i=0\n",
        "val_losses=[]\n",
        "for train_fold, test_fold in tscv.split(x_train_reg):\n",
        "  x_train, x_test = x_train_reg[train_fold], x_train_reg[test_fold]\n",
        "  y_train, y_test = y_train_reg[train_fold], y_train_reg[test_fold]\n",
        "  session = tf.Session()\n",
        "  # Add an additional layer on top of each of the hidden state outputs\n",
        "  alpharnn_t = alphaRNN(input_dimensions, hidden_sizes[i])\n",
        "  W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_sizes[i], 1), mean=0, stddev=0.01))\n",
        "  b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
        "  output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, alpharnn_t.h_t) \n",
        "  #models.append(train(alpharnn_t, x_train, x_test, y_train, y_test)) \n",
        "  model, val_loss=train(alpharnn_t, x_train, x_test, y_train, y_test, max_epochs=2000, batch_size=int(100/n_splits))\n",
        "  val_losses.append(val_loss)\n",
        "  models.append(model)\n",
        "  i+=1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.25716 \tValidation Loss: 0.15458\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.08123 \tValidation Loss: 0.09600\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.02796 \tValidation Loss: 0.03344\n",
            "Epoch  150 / 2000 :  \tTraining Loss: 0.01862 \tValidation Loss: 0.02340\n",
            "Epoch  200 / 2000 :  \tTraining Loss: 0.01615 \tValidation Loss: 0.02204\n",
            "Epoch  250 / 2000 :  \tTraining Loss: 0.01465 \tValidation Loss: 0.02120\n",
            "Epoch  300 / 2000 :  \tTraining Loss: 0.01367 \tValidation Loss: 0.02065\n",
            "Epoch  350 / 2000 :  \tTraining Loss: 0.01300 \tValidation Loss: 0.02026\n",
            "Epoch  400 / 2000 :  \tTraining Loss: 0.01254 \tValidation Loss: 0.01998\n",
            "Epoch  450 / 2000 :  \tTraining Loss: 0.01221 \tValidation Loss: 0.01979\n",
            "Epoch  500 / 2000 :  \tTraining Loss: 0.01197 \tValidation Loss: 0.01965\n",
            "Epoch  550 / 2000 :  \tTraining Loss: 0.01180 \tValidation Loss: 0.01956\n",
            "Epoch  600 / 2000 :  \tTraining Loss: 0.01167 \tValidation Loss: 0.01950\n",
            "Epoch  650 / 2000 :  \tTraining Loss: 0.01159 \tValidation Loss: 0.01946\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQt/x+tFvH6y3CeiwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.20970 \tValidation Loss: 0.16906\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.02508 \tValidation Loss: 0.02236\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.01876 \tValidation Loss: 0.01731\n",
            "Epoch  150 / 2000 :  \tTraining Loss: 0.01671 \tValidation Loss: 0.01561\n",
            "Epoch  200 / 2000 :  \tTraining Loss: 0.01584 \tValidation Loss: 0.01489\n",
            "Epoch  250 / 2000 :  \tTraining Loss: 0.01547 \tValidation Loss: 0.01458\n",
            "Epoch  300 / 2000 :  \tTraining Loss: 0.01531 \tValidation Loss: 0.01445\n",
            "Epoch  350 / 2000 :  \tTraining Loss: 0.01525 \tValidation Loss: 0.01440\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEdCAYAAAAxRnE+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJIQSjIiIJIQSjIiIJIQSjIiIJMT/B+nQum5rWZoYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.19956 \tValidation Loss: 0.26478\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.01851 \tValidation Loss: 0.01719\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.01540 \tValidation Loss: 0.01281\n",
            "Epoch  150 / 2000 :  \tTraining Loss: 0.01501 \tValidation Loss: 0.01217\n",
            "Epoch  200 / 2000 :  \tTraining Loss: 0.01498 \tValidation Loss: 0.01209\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.15814 \tValidation Loss: 0.14621\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.01512 \tValidation Loss: 0.01980\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.01427 \tValidation Loss: 0.01903\n",
            "Epoch  150 / 2000 :  \tTraining Loss: 0.01422 \tValidation Loss: 0.01895\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIJ+f+Z7dF23zq9PwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.10724 \tValidation Loss: 0.10300\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.01527 \tValidation Loss: 0.01350\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.01515 \tValidation Loss: 0.01336\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dcj6MUl_qxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQZXa3l1ByQa",
        "colab_type": "text"
      },
      "source": [
        "Pick the model with the lowest val loss. you can *not* use the test set for model selection! This would be cheating\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Dl772wETlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "237db4fc-5f7b-448f-c5ff-c21a19855b3b"
      },
      "source": [
        "#Pick the model with the lowest val_loss. Retrain it on all data and then perform prediction\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_losses))\n",
        "print(hidden_sizes[idx])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWH0sGENEfAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "489af110-30e2-457a-c657-b94173691cc9"
      },
      "source": [
        "session = tf.Session()\n",
        "  # Add an additional layer on top of each of the hidden state outputs\n",
        "alpharnn = simpleAlphaRNN(input_dimensions, hidden_sizes[idx])\n",
        "W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_sizes[idx], 1), mean=0, stddev=0.01))\n",
        "b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
        "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, alpharnn.h_t) \n",
        "model,_=train(alpharnn, x_train_reg, x_test_reg, y_train_reg, y_test_reg) "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 / 2000 :  \tTraining Loss: 0.21901 \tValidation Loss: 0.22637\n",
            "Epoch  50 / 2000 :  \tTraining Loss: 0.02302 \tValidation Loss: 0.02236\n",
            "Epoch  100 / 2000 :  \tTraining Loss: 0.01544 \tValidation Loss: 0.01420\n",
            "Epoch  150 / 2000 :  \tTraining Loss: 0.01484 \tValidation Loss: 0.01350\n",
            "Epoch  200 / 2000 :  \tTraining Loss: 0.01480 \tValidation Loss: 0.01346\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5vGIeTBEyjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predicted_t = session.run(output, feed_dict={model.input_layer: x_test_reg})\n",
        "y_predicted_ar_t=np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "for i in range(y_predicted_t.shape[0]):\n",
        "     y_predicted_ar_t[i]=y_predicted_t[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxFz0a7E10q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "c06f177e-b25c-43fd-c89a-3c31c1cda0ae"
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "plt.plot(y_predicted_ar_t, color='r', label='Predicted')\n",
        "plt.plot(y_test_reg.flatten(),'b', label='Actual')\n",
        "plt.legend(loc=0)\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAGuCAYAAABfvvr0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiInFSUC0iIiIiEicF1SIiIiIicfp/u3MyVNBNk/YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHop0Rb9mdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Ignore everything below this cell\n",
        "\n",
        "\n",
        "# you don't need this...\n",
        "for model in models:\n",
        "  #print(\"model = \" + str(model))\n",
        "  y_predicted_t = session.run(output, feed_dict={model.input_layer: x_test_reg})\n",
        "  y_predicted_ar_t=np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "  for i in range(y_predicted_t.shape[0]):\n",
        "  y_predicted_ar_t[i]=y_predicted_t[i][n_steps-1][0]\n",
        "      #print(\"y_predicted_ar_t \" + str(y_predicted_ar_t[i]))\n",
        "      MSE_test += mean_squared_error(y_test[:,0],y_predicted_ar_t[i])\n",
        "      print(\"MSE test \" + str(MSE_test))\n",
        "\n",
        "  y_predicted_t = session.run(output, feed_dict={model.input_layer: x_train_reg})\n",
        "  y_predicted_ar_train_t=np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "  for i in range(y_predicted_t.shape[0]):\n",
        "     y_predicted_ar_train_t[i]=y_predicted_t[i][n_steps-1][0]\n",
        "     MSE_train += mean_squared_error(y_train[:,0],y_predicted_ar_train_t[i])\n",
        "  \n",
        "MSE_train_alpha_t = MSE_train/n_splits\n",
        "print(\"MSE train alpha_t = \" + str(MSE_train_alpha_t))\n",
        "MSE_test_alpha_t = MSE_test/n_splits\n",
        "print(\"MSE test alpha_t = \" + str(MSE_test_alpha_t))\n",
        "MSE_train_alpha_t_std = np.math.sqrt(MSE_train_alpha_t/(n_splits-1))\n",
        "MSE_test_alpha_t_std = np.math.sqrt(MSE_test_alpha_t/(n_splits-1))\n",
        "print(\"MSE_train_alpha_t std = \" + str(MSE_train_alpha_t_std))\n",
        "print(\"MSE_test_alpha_t std = \" + str(MSE_test_alpha_t_std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duMmeZhERWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Cs1rumbE5mN",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MjKBmJ-VEdj7",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBL7dOev_yKe",
        "colab": {}
      },
      "source": [
        "y_predicted_t = session.run(output, feed_dict={alpharnn_t.input_layer: x_test_reg})\n",
        "y_predicted_ar_t=np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "for i in range(y_predicted_t.shape[0]):\n",
        "     y_predicted_ar_t[i]=y_predicted_t[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1K_XexWAzEc",
        "colab": {}
      },
      "source": [
        "y_predicted_t = session.run(output, feed_dict={alpharnn_t.input_layer: x_train_reg})\n",
        "y_predicted_ar_train_t=np.array([0]*y_predicted_t.shape[0], dtype='float64')\n",
        "for i in range(y_predicted_t.shape[0]):\n",
        "     y_predicted_ar_train_t[i]=y_predicted_t[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4p6eEvOtzQiF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fQ-E8qWkzSy6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t8OEYLsOf9JY",
        "colab": {}
      },
      "source": [
        "#train_loss = 0\n",
        "#expected_output_test = 0\n",
        "with tf.Session() as session:\n",
        "  #saved_path = saver.save(session, './saved_variable')\n",
        "  #result = cross_validate(session, 5)\n",
        "  #print('Cross-validation result: %s' % result)\n",
        "  print('Test accuracy: %f' % session.run(train_loss, feed_dict={alpharnn_t.input_layer: x_test_reg, expected_output_test: y_test_reg}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yHKeHrveAb2C",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_OPZPN8AeED",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ISqgZQGsAcBV",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lK0tiZZBwBfq",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "y_predicted_ar=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_ar[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Le9qaQtCwC8S",
        "colab": {}
      },
      "source": [
        "y_predicted = session.run(output, feed_dict={alpharnn.input_layer: x_train_reg})\n",
        "y_predicted_ar_train=np.array([0]*y_predicted.shape[0], dtype='float64')\n",
        "for i in range(y_predicted.shape[0]):\n",
        "     y_predicted_ar_train[i]=y_predicted[i][n_steps-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P3JNITxswF2q",
        "outputId": "27521d87-0c50-469e-8474-4f201d7a1f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "plt.plot(y_predicted_ar, color='r', label='Predicted')\n",
        "plt.plot(y_test_reg.flatten(),'b', label='Actual')\n",
        "plt.legend(loc=0)\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAGuCAYAAABfvvr0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_Lcu-9vwSow",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.Session() as session:\n",
        "  #saved_path = saver.save(session, './saved_variable')\n",
        "  #result = cross_validate(session, 5)\n",
        "  #print('Cross-validation result: %s' % result)\n",
        "  print('Test accuracy: %f' % session.run(train_loss, feed_dict={alpharnn.input_layer: x_test_reg, expected_output_test: y_test_reg}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tgZbrudMwcNn",
        "outputId": "612333b5-8324-4cc3-bec0-0b3b3d525723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "print(train_losses[0][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-87b3611a396e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Or6iq32awi0e",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7ooyKI-wlwo",
        "colab": {}
      },
      "source": [
        "x_train_reg = pd.concat(x_train_list, axis=1)\n",
        "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
        "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
        "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0Bjom9wpT2",
        "colab": {}
      },
      "source": [
        "x_test_reg = pd.concat(x_test_list, axis=1)\n",
        "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
        "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
        "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LeXlBJgwtdy",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FImnMFUowv1P",
        "colab": {}
      },
      "source": [
        "def RNN_model2(n_units=10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(SimpleRNN(n_units, activation='relu', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def GRU_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(GRU(n_units, activation='relu', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model\n",
        "\n",
        "def LSTM_model2(n_units = 10, l1_reg=0):\n",
        "    reg_model = Sequential()\n",
        "    reg_model.add(LSTM(n_units, activation='relu', input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
        "    #LPNorm.build_loss(p = float('inf'))\n",
        "    reg_model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=l1(l1_reg)))\n",
        "    #reg_model.add(Dropout(0.2))\n",
        "    reg_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return reg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNI_srXYw1yf",
        "outputId": "00eb245a-0986-4297-b337-c677147adc4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_units = [10, 20, 30, 40, 50, 60, 100]\n",
        "l1_reg = [0]  #[0, 0.001]   #0.01, 0.1]\n",
        "#param_grid = dict(epochs=epochs,batch_size =batch_size)\n",
        "                  #n_neurons=n_neurons)\n",
        "                  #optimizers=optimizers,\n",
        "                  #n_neurons = n_neurons)\n",
        "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "#weight_constraint = [1, 2, 3, 4, 5]\n",
        "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "tscv = TimeSeriesSplit(n_splits = 10)\n",
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "#X_train, X_test, y_train, y_test = train_test_split(x_train_reg, y_train_reg, test_size=0.5, random_state=0) \n",
        "print(\"Hyper parameter tuning for RNN...\")\n",
        "model = KerasRegressor(build_fn=RNN_model2, epochs=2000, batch_size=100, verbose=1)\n",
        "grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=-1)\n",
        "grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n",
        "# Manual CV\n",
        "nodes = grid_result.best_params_['n_units']\n",
        "l1_reg = grid_result.best_params_['l1_reg']\n",
        "#n_units = [10, 20, 30, 40, 50, 60, 100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyper parameter tuning for RNN...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1566/1566 [==============================] - 13s 8ms/step - loss: 0.0413\n",
            "Epoch 2/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 0.0229\n",
            "Epoch 3/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 0.0027\n",
            "Epoch 4/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 0.0011\n",
            "Epoch 5/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 9.1861e-04\n",
            "Epoch 6/2000\n",
            " 100/1566 [>.............................] - ETA: 0s - loss: 1.6546e-04"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 31us/step - loss: 8.6794e-04\n",
            "Epoch 7/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 8.2146e-04\n",
            "Epoch 8/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 7.8715e-04\n",
            "Epoch 9/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 7.5906e-04\n",
            "Epoch 10/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 7.1665e-04\n",
            "Epoch 11/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 6.9844e-04\n",
            "Epoch 12/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.6641e-04\n",
            "Epoch 13/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 6.5739e-04\n",
            "Epoch 14/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 6.4485e-04\n",
            "Epoch 15/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 6.3995e-04\n",
            "Epoch 16/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 6.0124e-04\n",
            "Epoch 17/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.9907e-04\n",
            "Epoch 18/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.9814e-04\n",
            "Epoch 19/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.9449e-04\n",
            "Epoch 20/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.9793e-04\n",
            "Epoch 21/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 6.3605e-04\n",
            "Epoch 22/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.8488e-04\n",
            "Epoch 23/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.6028e-04\n",
            "Epoch 24/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.5587e-04\n",
            "Epoch 25/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.4239e-04\n",
            "Epoch 26/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.3703e-04\n",
            "Epoch 27/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.3422e-04\n",
            "Epoch 28/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.5301e-04\n",
            "Epoch 29/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.3649e-04\n",
            "Epoch 30/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2677e-04\n",
            "Epoch 31/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.6540e-04\n",
            "Epoch 32/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.7254e-04\n",
            "Epoch 33/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2696e-04\n",
            "Epoch 34/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2169e-04\n",
            "Epoch 35/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1085e-04\n",
            "Epoch 36/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2033e-04\n",
            "Epoch 37/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2177e-04\n",
            "Epoch 38/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1472e-04\n",
            "Epoch 39/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1673e-04\n",
            "Epoch 40/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0718e-04\n",
            "Epoch 41/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.1124e-04\n",
            "Epoch 42/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0877e-04\n",
            "Epoch 43/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0449e-04\n",
            "Epoch 44/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0971e-04\n",
            "Epoch 45/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0251e-04\n",
            "Epoch 46/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9895e-04\n",
            "Epoch 47/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9568e-04\n",
            "Epoch 48/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0654e-04\n",
            "Epoch 49/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9696e-04\n",
            "Epoch 50/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9912e-04\n",
            "Epoch 51/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0057e-04\n",
            "Epoch 52/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9717e-04\n",
            "Epoch 53/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0233e-04\n",
            "Epoch 54/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9601e-04\n",
            "Epoch 55/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9839e-04\n",
            "Epoch 56/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9344e-04\n",
            "Epoch 57/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9789e-04\n",
            "Epoch 58/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8877e-04\n",
            "Epoch 59/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9776e-04\n",
            "Epoch 60/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9852e-04\n",
            "Epoch 61/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0092e-04\n",
            "Epoch 62/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9723e-04\n",
            "Epoch 63/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0392e-04\n",
            "Epoch 64/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9220e-04\n",
            "Epoch 65/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0452e-04\n",
            "Epoch 66/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9301e-04\n",
            "Epoch 67/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9459e-04\n",
            "Epoch 68/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9505e-04\n",
            "Epoch 69/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9183e-04\n",
            "Epoch 70/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1313e-04\n",
            "Epoch 71/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9585e-04\n",
            "Epoch 72/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.9256e-04\n",
            "Epoch 73/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0435e-04\n",
            "Epoch 74/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1406e-04\n",
            "Epoch 75/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0000e-04\n",
            "Epoch 76/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9451e-04\n",
            "Epoch 77/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9971e-04\n",
            "Epoch 78/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9317e-04\n",
            "Epoch 79/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9383e-04\n",
            "Epoch 80/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0959e-04\n",
            "Epoch 81/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8753e-04\n",
            "Epoch 82/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0336e-04\n",
            "Epoch 83/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2096e-04\n",
            "Epoch 84/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9872e-04\n",
            "Epoch 85/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0240e-04\n",
            "Epoch 86/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9790e-04\n",
            "Epoch 87/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0083e-04\n",
            "Epoch 88/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9772e-04\n",
            "Epoch 89/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8984e-04\n",
            "Epoch 90/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0137e-04\n",
            "Epoch 91/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0095e-04\n",
            "Epoch 92/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0000e-04\n",
            "Epoch 93/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9217e-04\n",
            "Epoch 94/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9229e-04\n",
            "Epoch 95/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9796e-04\n",
            "Epoch 96/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9057e-04\n",
            "Epoch 97/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9068e-04\n",
            "Epoch 98/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1250e-04\n",
            "Epoch 99/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9179e-04\n",
            "Epoch 100/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9938e-04\n",
            "Epoch 101/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.2328e-04\n",
            "Epoch 102/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8948e-04\n",
            "Epoch 103/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9631e-04\n",
            "Epoch 104/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0369e-04\n",
            "Epoch 105/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9185e-04\n",
            "Epoch 106/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8801e-04\n",
            "Epoch 107/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0114e-04\n",
            "Epoch 108/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0980e-04\n",
            "Epoch 109/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9514e-04\n",
            "Epoch 110/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0115e-04\n",
            "Epoch 111/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9870e-04\n",
            "Epoch 112/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0001e-04\n",
            "Epoch 113/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1034e-04\n",
            "Epoch 114/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0652e-04\n",
            "Epoch 115/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9705e-04\n",
            "Epoch 116/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0357e-04\n",
            "Epoch 117/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0507e-04\n",
            "Epoch 118/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.3117e-04\n",
            "Epoch 119/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0492e-04\n",
            "Epoch 120/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9326e-04\n",
            "Epoch 121/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2282e-04\n",
            "Epoch 122/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0852e-04\n",
            "Epoch 123/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0815e-04\n",
            "Epoch 124/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9129e-04\n",
            "Epoch 125/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0632e-04\n",
            "Epoch 126/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8743e-04\n",
            "Epoch 127/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8694e-04\n",
            "Epoch 128/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0473e-04\n",
            "Epoch 129/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9213e-04\n",
            "Epoch 130/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9141e-04\n",
            "Epoch 131/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9064e-04\n",
            "Epoch 132/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8657e-04\n",
            "Epoch 133/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8489e-04\n",
            "Epoch 134/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0197e-04\n",
            "Epoch 135/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0040e-04\n",
            "Epoch 136/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8939e-04\n",
            "Epoch 137/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9057e-04\n",
            "Epoch 138/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8736e-04\n",
            "Epoch 139/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9998e-04\n",
            "Epoch 140/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9724e-04\n",
            "Epoch 141/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8932e-04\n",
            "Epoch 142/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.3656e-04\n",
            "Epoch 143/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9629e-04\n",
            "Epoch 144/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0438e-04\n",
            "Epoch 145/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8904e-04\n",
            "Epoch 146/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9298e-04\n",
            "Epoch 147/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9329e-04\n",
            "Epoch 148/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9206e-04\n",
            "Epoch 149/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0313e-04\n",
            "Epoch 150/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0157e-04\n",
            "Epoch 151/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8738e-04\n",
            "Epoch 152/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9133e-04\n",
            "Epoch 153/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8658e-04\n",
            "Epoch 154/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9701e-04\n",
            "Epoch 155/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9884e-04\n",
            "Epoch 156/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0172e-04\n",
            "Epoch 157/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9658e-04\n",
            "Epoch 158/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0374e-04\n",
            "Epoch 159/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.3198e-04\n",
            "Epoch 160/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.2794e-04\n",
            "Epoch 161/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0291e-04\n",
            "Epoch 162/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0080e-04\n",
            "Epoch 163/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0090e-04\n",
            "Epoch 164/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9016e-04\n",
            "Epoch 165/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1459e-04\n",
            "Epoch 166/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9816e-04\n",
            "Epoch 167/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9703e-04\n",
            "Epoch 168/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9885e-04\n",
            "Epoch 169/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9336e-04\n",
            "Epoch 170/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9397e-04\n",
            "Epoch 171/2000\n",
            "1566/1566 [==============================] - 0s 46us/step - loss: 4.9253e-04\n",
            "Epoch 172/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8569e-04\n",
            "Epoch 173/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9176e-04\n",
            "Epoch 174/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8464e-04\n",
            "Epoch 175/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0909e-04\n",
            "Epoch 176/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.2101e-04\n",
            "Epoch 177/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9782e-04\n",
            "Epoch 178/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9958e-04\n",
            "Epoch 179/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9393e-04\n",
            "Epoch 180/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9722e-04\n",
            "Epoch 181/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0476e-04\n",
            "Epoch 182/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9914e-04\n",
            "Epoch 183/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9532e-04\n",
            "Epoch 184/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9884e-04\n",
            "Epoch 185/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0720e-04\n",
            "Epoch 186/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0189e-04\n",
            "Epoch 187/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0036e-04\n",
            "Epoch 188/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9472e-04\n",
            "Epoch 189/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0593e-04\n",
            "Epoch 190/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.1075e-04\n",
            "Epoch 191/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8871e-04\n",
            "Epoch 192/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9316e-04\n",
            "Epoch 193/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9351e-04\n",
            "Epoch 194/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9233e-04\n",
            "Epoch 195/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8688e-04\n",
            "Epoch 196/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8831e-04\n",
            "Epoch 197/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9437e-04\n",
            "Epoch 198/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9027e-04\n",
            "Epoch 199/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9122e-04\n",
            "Epoch 200/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9142e-04\n",
            "Epoch 201/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9517e-04\n",
            "Epoch 202/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9045e-04\n",
            "Epoch 203/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8718e-04\n",
            "Epoch 204/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0243e-04\n",
            "Epoch 205/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1308e-04\n",
            "Epoch 206/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.3045e-04\n",
            "Epoch 207/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0129e-04\n",
            "Epoch 208/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0211e-04\n",
            "Epoch 209/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9722e-04\n",
            "Epoch 210/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9875e-04\n",
            "Epoch 211/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9534e-04\n",
            "Epoch 212/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8951e-04\n",
            "Epoch 213/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8944e-04\n",
            "Epoch 214/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8720e-04\n",
            "Epoch 215/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8914e-04\n",
            "Epoch 216/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8991e-04\n",
            "Epoch 217/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8982e-04\n",
            "Epoch 218/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8830e-04\n",
            "Epoch 219/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.9310e-04\n",
            "Epoch 220/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9765e-04\n",
            "Epoch 221/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9901e-04\n",
            "Epoch 222/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9278e-04\n",
            "Epoch 223/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0739e-04\n",
            "Epoch 224/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9491e-04\n",
            "Epoch 225/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9621e-04\n",
            "Epoch 226/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9334e-04\n",
            "Epoch 227/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9382e-04\n",
            "Epoch 228/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8835e-04\n",
            "Epoch 229/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8966e-04\n",
            "Epoch 230/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8725e-04\n",
            "Epoch 231/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8679e-04\n",
            "Epoch 232/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8908e-04\n",
            "Epoch 233/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8662e-04\n",
            "Epoch 234/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9796e-04\n",
            "Epoch 235/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9293e-04\n",
            "Epoch 236/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8735e-04\n",
            "Epoch 237/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8811e-04\n",
            "Epoch 238/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9038e-04\n",
            "Epoch 239/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9802e-04\n",
            "Epoch 240/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9488e-04\n",
            "Epoch 241/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9279e-04\n",
            "Epoch 242/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9822e-04\n",
            "Epoch 243/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0551e-04\n",
            "Epoch 244/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9951e-04\n",
            "Epoch 245/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9669e-04\n",
            "Epoch 246/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8411e-04\n",
            "Epoch 247/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9254e-04\n",
            "Epoch 248/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9582e-04\n",
            "Epoch 249/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9323e-04\n",
            "Epoch 250/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8940e-04\n",
            "Epoch 251/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8873e-04\n",
            "Epoch 252/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8913e-04\n",
            "Epoch 253/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9387e-04\n",
            "Epoch 254/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9060e-04\n",
            "Epoch 255/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8847e-04\n",
            "Epoch 256/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8710e-04\n",
            "Epoch 257/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8819e-04\n",
            "Epoch 258/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0055e-04\n",
            "Epoch 259/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9400e-04\n",
            "Epoch 260/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9574e-04\n",
            "Epoch 261/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0682e-04\n",
            "Epoch 262/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0701e-04\n",
            "Epoch 263/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9027e-04\n",
            "Epoch 264/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8304e-04\n",
            "Epoch 265/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8730e-04\n",
            "Epoch 266/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9093e-04\n",
            "Epoch 267/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8988e-04\n",
            "Epoch 268/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9997e-04\n",
            "Epoch 269/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9897e-04\n",
            "Epoch 270/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9200e-04\n",
            "Epoch 271/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8730e-04\n",
            "Epoch 272/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9231e-04\n",
            "Epoch 273/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9248e-04\n",
            "Epoch 274/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8575e-04\n",
            "Epoch 275/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9225e-04\n",
            "Epoch 276/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9408e-04\n",
            "Epoch 277/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.2344e-04\n",
            "Epoch 278/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9870e-04\n",
            "Epoch 279/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.9436e-04\n",
            "Epoch 280/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9641e-04\n",
            "Epoch 281/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9025e-04\n",
            "Epoch 282/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9290e-04\n",
            "Epoch 283/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9472e-04\n",
            "Epoch 284/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8881e-04\n",
            "Epoch 285/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8917e-04\n",
            "Epoch 286/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0560e-04\n",
            "Epoch 287/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.1534e-04\n",
            "Epoch 288/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9863e-04\n",
            "Epoch 289/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9088e-04\n",
            "Epoch 290/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9150e-04\n",
            "Epoch 291/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9439e-04\n",
            "Epoch 292/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9788e-04\n",
            "Epoch 293/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0404e-04\n",
            "Epoch 294/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9491e-04\n",
            "Epoch 295/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8959e-04\n",
            "Epoch 296/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8854e-04\n",
            "Epoch 297/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8712e-04\n",
            "Epoch 298/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8632e-04\n",
            "Epoch 299/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8836e-04\n",
            "Epoch 300/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9712e-04\n",
            "Epoch 301/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0090e-04\n",
            "Epoch 302/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8635e-04\n",
            "Epoch 303/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8774e-04\n",
            "Epoch 304/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8979e-04\n",
            "Epoch 305/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0262e-04\n",
            "Epoch 306/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8834e-04\n",
            "Epoch 307/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8438e-04\n",
            "Epoch 308/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8969e-04\n",
            "Epoch 309/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9636e-04\n",
            "Epoch 310/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9632e-04\n",
            "Epoch 311/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9682e-04\n",
            "Epoch 312/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9715e-04\n",
            "Epoch 313/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9970e-04\n",
            "Epoch 314/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9456e-04\n",
            "Epoch 315/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9004e-04\n",
            "Epoch 316/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9066e-04\n",
            "Epoch 317/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8623e-04\n",
            "Epoch 318/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9147e-04\n",
            "Epoch 319/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8540e-04\n",
            "Epoch 320/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8616e-04\n",
            "Epoch 321/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9395e-04\n",
            "Epoch 322/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8606e-04\n",
            "Epoch 323/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9617e-04\n",
            "Epoch 324/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8891e-04\n",
            "Epoch 325/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8703e-04\n",
            "Epoch 326/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8665e-04\n",
            "Epoch 327/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8357e-04\n",
            "Epoch 328/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8969e-04\n",
            "Epoch 329/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8615e-04\n",
            "Epoch 330/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8980e-04\n",
            "Epoch 331/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9259e-04\n",
            "Epoch 332/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9468e-04\n",
            "Epoch 333/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9261e-04\n",
            "Epoch 334/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9018e-04\n",
            "Epoch 335/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0224e-04\n",
            "Epoch 336/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9929e-04\n",
            "Epoch 337/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9068e-04\n",
            "Epoch 338/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9512e-04\n",
            "Epoch 339/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9945e-04\n",
            "Epoch 340/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9138e-04\n",
            "Epoch 341/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8752e-04\n",
            "Epoch 342/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8919e-04\n",
            "Epoch 343/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0987e-04\n",
            "Epoch 344/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9400e-04\n",
            "Epoch 345/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9375e-04\n",
            "Epoch 346/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8425e-04\n",
            "Epoch 347/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8814e-04\n",
            "Epoch 348/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9064e-04\n",
            "Epoch 349/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8566e-04\n",
            "Epoch 350/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0020e-04\n",
            "Epoch 351/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9732e-04\n",
            "Epoch 352/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9497e-04\n",
            "Epoch 353/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9403e-04\n",
            "Epoch 354/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1013e-04\n",
            "Epoch 355/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9892e-04\n",
            "Epoch 356/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0020e-04\n",
            "Epoch 357/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9185e-04\n",
            "Epoch 358/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8988e-04\n",
            "Epoch 359/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8287e-04\n",
            "Epoch 360/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8841e-04\n",
            "Epoch 361/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9525e-04\n",
            "Epoch 362/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9563e-04\n",
            "Epoch 363/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0137e-04\n",
            "Epoch 364/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9348e-04\n",
            "Epoch 365/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8768e-04\n",
            "Epoch 366/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9592e-04\n",
            "Epoch 367/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9151e-04\n",
            "Epoch 368/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8543e-04\n",
            "Epoch 369/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8515e-04\n",
            "Epoch 370/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9080e-04\n",
            "Epoch 371/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8344e-04\n",
            "Epoch 372/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8609e-04\n",
            "Epoch 373/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9481e-04\n",
            "Epoch 374/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1447e-04\n",
            "Epoch 375/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9375e-04\n",
            "Epoch 376/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9741e-04\n",
            "Epoch 377/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9268e-04\n",
            "Epoch 378/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8645e-04\n",
            "Epoch 379/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8839e-04\n",
            "Epoch 380/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9486e-04\n",
            "Epoch 381/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8811e-04\n",
            "Epoch 382/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9457e-04\n",
            "Epoch 383/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8390e-04\n",
            "Epoch 384/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7936e-04\n",
            "Epoch 385/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8816e-04\n",
            "Epoch 386/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8839e-04\n",
            "Epoch 387/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7990e-04\n",
            "Epoch 388/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8002e-04\n",
            "Epoch 389/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9167e-04\n",
            "Epoch 390/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8770e-04\n",
            "Epoch 391/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9212e-04\n",
            "Epoch 392/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9034e-04\n",
            "Epoch 393/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7956e-04\n",
            "Epoch 394/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9562e-04\n",
            "Epoch 395/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9653e-04\n",
            "Epoch 396/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8761e-04\n",
            "Epoch 397/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8974e-04\n",
            "Epoch 398/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8974e-04\n",
            "Epoch 399/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8734e-04\n",
            "Epoch 400/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0702e-04\n",
            "Epoch 401/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9852e-04\n",
            "Epoch 402/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0074e-04\n",
            "Epoch 403/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0509e-04\n",
            "Epoch 404/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9052e-04\n",
            "Epoch 405/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8971e-04\n",
            "Epoch 406/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9318e-04\n",
            "Epoch 407/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9111e-04\n",
            "Epoch 408/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9409e-04\n",
            "Epoch 409/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8904e-04\n",
            "Epoch 410/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8969e-04\n",
            "Epoch 411/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8663e-04\n",
            "Epoch 412/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9270e-04\n",
            "Epoch 413/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8849e-04\n",
            "Epoch 414/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8443e-04\n",
            "Epoch 415/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8280e-04\n",
            "Epoch 416/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8767e-04\n",
            "Epoch 417/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9117e-04\n",
            "Epoch 418/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8560e-04\n",
            "Epoch 419/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9480e-04\n",
            "Epoch 420/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0000e-04\n",
            "Epoch 421/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9333e-04\n",
            "Epoch 422/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9601e-04\n",
            "Epoch 423/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8741e-04\n",
            "Epoch 424/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0520e-04\n",
            "Epoch 425/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9545e-04\n",
            "Epoch 426/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9993e-04\n",
            "Epoch 427/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9949e-04\n",
            "Epoch 428/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9206e-04\n",
            "Epoch 429/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8728e-04\n",
            "Epoch 430/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8355e-04\n",
            "Epoch 431/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9685e-04\n",
            "Epoch 432/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9268e-04\n",
            "Epoch 433/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9245e-04\n",
            "Epoch 434/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8306e-04\n",
            "Epoch 435/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8704e-04\n",
            "Epoch 436/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8961e-04\n",
            "Epoch 437/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9505e-04\n",
            "Epoch 438/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8928e-04\n",
            "Epoch 439/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8882e-04\n",
            "Epoch 440/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8906e-04\n",
            "Epoch 441/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8704e-04\n",
            "Epoch 442/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8791e-04\n",
            "Epoch 443/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9099e-04\n",
            "Epoch 444/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8812e-04\n",
            "Epoch 445/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9000e-04\n",
            "Epoch 446/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8592e-04\n",
            "Epoch 447/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8858e-04\n",
            "Epoch 448/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8798e-04\n",
            "Epoch 449/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8661e-04\n",
            "Epoch 450/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8180e-04\n",
            "Epoch 451/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8088e-04\n",
            "Epoch 452/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0060e-04\n",
            "Epoch 453/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8372e-04\n",
            "Epoch 454/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8421e-04\n",
            "Epoch 455/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7988e-04\n",
            "Epoch 456/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9611e-04\n",
            "Epoch 457/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8476e-04\n",
            "Epoch 458/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8372e-04\n",
            "Epoch 459/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8356e-04\n",
            "Epoch 460/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8748e-04\n",
            "Epoch 461/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8484e-04\n",
            "Epoch 462/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0464e-04\n",
            "Epoch 463/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8968e-04\n",
            "Epoch 464/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9695e-04\n",
            "Epoch 465/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9789e-04\n",
            "Epoch 466/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8743e-04\n",
            "Epoch 467/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9106e-04\n",
            "Epoch 468/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9403e-04\n",
            "Epoch 469/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8457e-04\n",
            "Epoch 470/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8987e-04\n",
            "Epoch 471/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8635e-04\n",
            "Epoch 472/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8893e-04\n",
            "Epoch 473/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9356e-04\n",
            "Epoch 474/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0035e-04\n",
            "Epoch 475/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9172e-04\n",
            "Epoch 476/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8970e-04\n",
            "Epoch 477/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9519e-04\n",
            "Epoch 478/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8670e-04\n",
            "Epoch 479/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8016e-04\n",
            "Epoch 480/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8242e-04\n",
            "Epoch 481/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8204e-04\n",
            "Epoch 482/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8681e-04\n",
            "Epoch 483/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0604e-04\n",
            "Epoch 484/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9437e-04\n",
            "Epoch 485/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0552e-04\n",
            "Epoch 486/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9221e-04\n",
            "Epoch 487/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9629e-04\n",
            "Epoch 488/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8212e-04\n",
            "Epoch 489/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8443e-04\n",
            "Epoch 490/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9935e-04\n",
            "Epoch 491/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9709e-04\n",
            "Epoch 492/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9511e-04\n",
            "Epoch 493/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9174e-04\n",
            "Epoch 494/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8078e-04\n",
            "Epoch 495/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8617e-04\n",
            "Epoch 496/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8800e-04\n",
            "Epoch 497/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8439e-04\n",
            "Epoch 498/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8596e-04\n",
            "Epoch 499/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9827e-04\n",
            "Epoch 500/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9761e-04\n",
            "Epoch 501/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8984e-04\n",
            "Epoch 502/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9181e-04\n",
            "Epoch 503/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8883e-04\n",
            "Epoch 504/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8326e-04\n",
            "Epoch 505/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8572e-04\n",
            "Epoch 506/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9340e-04\n",
            "Epoch 507/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8910e-04\n",
            "Epoch 508/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8661e-04\n",
            "Epoch 509/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9922e-04\n",
            "Epoch 510/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8659e-04\n",
            "Epoch 511/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8101e-04\n",
            "Epoch 512/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8534e-04\n",
            "Epoch 513/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8491e-04\n",
            "Epoch 514/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9582e-04\n",
            "Epoch 515/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0112e-04\n",
            "Epoch 516/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8711e-04\n",
            "Epoch 517/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9259e-04\n",
            "Epoch 518/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0031e-04\n",
            "Epoch 519/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9195e-04\n",
            "Epoch 520/2000\n",
            "1566/1566 [==============================] - 0s 41us/step - loss: 4.9394e-04\n",
            "Epoch 521/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9430e-04\n",
            "Epoch 522/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9387e-04\n",
            "Epoch 523/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9955e-04\n",
            "Epoch 524/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9982e-04\n",
            "Epoch 525/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0341e-04\n",
            "Epoch 526/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0063e-04\n",
            "Epoch 527/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9319e-04\n",
            "Epoch 528/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9398e-04\n",
            "Epoch 529/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9313e-04\n",
            "Epoch 530/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8827e-04\n",
            "Epoch 531/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8627e-04\n",
            "Epoch 532/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8025e-04\n",
            "Epoch 533/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8204e-04\n",
            "Epoch 534/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8409e-04\n",
            "Epoch 535/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9714e-04\n",
            "Epoch 536/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8396e-04\n",
            "Epoch 537/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8794e-04\n",
            "Epoch 538/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8416e-04\n",
            "Epoch 539/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8488e-04\n",
            "Epoch 540/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8882e-04\n",
            "Epoch 541/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0329e-04\n",
            "Epoch 542/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9439e-04\n",
            "Epoch 543/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8845e-04\n",
            "Epoch 544/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9065e-04\n",
            "Epoch 545/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1098e-04\n",
            "Epoch 546/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9028e-04\n",
            "Epoch 547/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0315e-04\n",
            "Epoch 548/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8286e-04\n",
            "Epoch 549/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8983e-04\n",
            "Epoch 550/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8577e-04\n",
            "Epoch 551/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8716e-04\n",
            "Epoch 552/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8999e-04\n",
            "Epoch 553/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9567e-04\n",
            "Epoch 554/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8708e-04\n",
            "Epoch 555/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8409e-04\n",
            "Epoch 556/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8297e-04\n",
            "Epoch 557/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9725e-04\n",
            "Epoch 558/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9922e-04\n",
            "Epoch 559/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9287e-04\n",
            "Epoch 560/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8848e-04\n",
            "Epoch 561/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9113e-04\n",
            "Epoch 562/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8213e-04\n",
            "Epoch 563/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9454e-04\n",
            "Epoch 564/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0802e-04\n",
            "Epoch 565/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9530e-04\n",
            "Epoch 566/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9171e-04\n",
            "Epoch 567/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8960e-04\n",
            "Epoch 568/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8436e-04\n",
            "Epoch 569/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8584e-04\n",
            "Epoch 570/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8975e-04\n",
            "Epoch 571/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9210e-04\n",
            "Epoch 572/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9098e-04\n",
            "Epoch 573/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9696e-04\n",
            "Epoch 574/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8898e-04\n",
            "Epoch 575/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9033e-04\n",
            "Epoch 576/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9006e-04\n",
            "Epoch 577/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8277e-04\n",
            "Epoch 578/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8778e-04\n",
            "Epoch 579/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9017e-04\n",
            "Epoch 580/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0104e-04\n",
            "Epoch 581/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9513e-04\n",
            "Epoch 582/2000\n",
            "1566/1566 [==============================] - 0s 39us/step - loss: 4.8438e-04\n",
            "Epoch 583/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9350e-04\n",
            "Epoch 584/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1579e-04\n",
            "Epoch 585/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0382e-04\n",
            "Epoch 586/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9759e-04\n",
            "Epoch 587/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8904e-04\n",
            "Epoch 588/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8889e-04\n",
            "Epoch 589/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9514e-04\n",
            "Epoch 590/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8521e-04\n",
            "Epoch 591/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9540e-04\n",
            "Epoch 592/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0030e-04\n",
            "Epoch 593/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8418e-04\n",
            "Epoch 594/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9171e-04\n",
            "Epoch 595/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9256e-04\n",
            "Epoch 596/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9436e-04\n",
            "Epoch 597/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8284e-04\n",
            "Epoch 598/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8638e-04\n",
            "Epoch 599/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8293e-04\n",
            "Epoch 600/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8278e-04\n",
            "Epoch 601/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9200e-04\n",
            "Epoch 602/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9498e-04\n",
            "Epoch 603/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.1559e-04\n",
            "Epoch 604/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9720e-04\n",
            "Epoch 605/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8777e-04\n",
            "Epoch 606/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8767e-04\n",
            "Epoch 607/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0186e-04\n",
            "Epoch 608/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8581e-04\n",
            "Epoch 609/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8121e-04\n",
            "Epoch 610/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8676e-04\n",
            "Epoch 611/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9312e-04\n",
            "Epoch 612/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0135e-04\n",
            "Epoch 613/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9932e-04\n",
            "Epoch 614/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0010e-04\n",
            "Epoch 615/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9463e-04\n",
            "Epoch 616/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0036e-04\n",
            "Epoch 617/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0808e-04\n",
            "Epoch 618/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9311e-04\n",
            "Epoch 619/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8339e-04\n",
            "Epoch 620/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8249e-04\n",
            "Epoch 621/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8887e-04\n",
            "Epoch 622/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8161e-04\n",
            "Epoch 623/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8715e-04\n",
            "Epoch 624/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8363e-04\n",
            "Epoch 625/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8941e-04\n",
            "Epoch 626/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7740e-04\n",
            "Epoch 627/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8877e-04\n",
            "Epoch 628/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8540e-04\n",
            "Epoch 629/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9167e-04\n",
            "Epoch 630/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9897e-04\n",
            "Epoch 631/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0843e-04\n",
            "Epoch 632/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9112e-04\n",
            "Epoch 633/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9396e-04\n",
            "Epoch 634/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0365e-04\n",
            "Epoch 635/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0007e-04\n",
            "Epoch 636/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9624e-04\n",
            "Epoch 637/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1140e-04\n",
            "Epoch 638/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0189e-04\n",
            "Epoch 639/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9057e-04\n",
            "Epoch 640/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8941e-04\n",
            "Epoch 641/2000\n",
            "1566/1566 [==============================] - 0s 41us/step - loss: 4.8804e-04\n",
            "Epoch 642/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8788e-04\n",
            "Epoch 643/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0747e-04\n",
            "Epoch 644/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9407e-04\n",
            "Epoch 645/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9060e-04\n",
            "Epoch 646/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8826e-04\n",
            "Epoch 647/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9517e-04\n",
            "Epoch 648/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8841e-04\n",
            "Epoch 649/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8643e-04\n",
            "Epoch 650/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8446e-04\n",
            "Epoch 651/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9252e-04\n",
            "Epoch 652/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8761e-04\n",
            "Epoch 653/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8397e-04\n",
            "Epoch 654/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8521e-04\n",
            "Epoch 655/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8162e-04\n",
            "Epoch 656/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8129e-04\n",
            "Epoch 657/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0039e-04\n",
            "Epoch 658/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9671e-04\n",
            "Epoch 659/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8593e-04\n",
            "Epoch 660/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9920e-04\n",
            "Epoch 661/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9495e-04\n",
            "Epoch 662/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8959e-04\n",
            "Epoch 663/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9241e-04\n",
            "Epoch 664/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9377e-04\n",
            "Epoch 665/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0345e-04\n",
            "Epoch 666/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9194e-04\n",
            "Epoch 667/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9376e-04\n",
            "Epoch 668/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9420e-04\n",
            "Epoch 669/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9220e-04\n",
            "Epoch 670/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0247e-04\n",
            "Epoch 671/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9417e-04\n",
            "Epoch 672/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8052e-04\n",
            "Epoch 673/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8144e-04\n",
            "Epoch 674/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8798e-04\n",
            "Epoch 675/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9816e-04\n",
            "Epoch 676/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8485e-04\n",
            "Epoch 677/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8036e-04\n",
            "Epoch 678/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9337e-04\n",
            "Epoch 679/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9269e-04\n",
            "Epoch 680/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9247e-04\n",
            "Epoch 681/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8361e-04\n",
            "Epoch 682/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8464e-04\n",
            "Epoch 683/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9416e-04\n",
            "Epoch 684/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8072e-04\n",
            "Epoch 685/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8291e-04\n",
            "Epoch 686/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8345e-04\n",
            "Epoch 687/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9563e-04\n",
            "Epoch 688/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1197e-04\n",
            "Epoch 689/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0134e-04\n",
            "Epoch 690/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8936e-04\n",
            "Epoch 691/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8265e-04\n",
            "Epoch 692/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8263e-04\n",
            "Epoch 693/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9571e-04\n",
            "Epoch 694/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8099e-04\n",
            "Epoch 695/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8178e-04\n",
            "Epoch 696/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8440e-04\n",
            "Epoch 697/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8158e-04\n",
            "Epoch 698/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8389e-04\n",
            "Epoch 699/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8061e-04\n",
            "Epoch 700/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7800e-04\n",
            "Epoch 701/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9284e-04\n",
            "Epoch 702/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9712e-04\n",
            "Epoch 703/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7947e-04\n",
            "Epoch 704/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7624e-04\n",
            "Epoch 705/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7860e-04\n",
            "Epoch 706/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9547e-04\n",
            "Epoch 707/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8565e-04\n",
            "Epoch 708/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9578e-04\n",
            "Epoch 709/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8898e-04\n",
            "Epoch 710/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9060e-04\n",
            "Epoch 711/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9026e-04\n",
            "Epoch 712/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8641e-04\n",
            "Epoch 713/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9007e-04\n",
            "Epoch 714/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8746e-04\n",
            "Epoch 715/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9911e-04\n",
            "Epoch 716/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.1030e-04\n",
            "Epoch 717/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8633e-04\n",
            "Epoch 718/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9468e-04\n",
            "Epoch 719/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8939e-04\n",
            "Epoch 720/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9352e-04\n",
            "Epoch 721/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8828e-04\n",
            "Epoch 722/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9079e-04\n",
            "Epoch 723/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9149e-04\n",
            "Epoch 724/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9079e-04\n",
            "Epoch 725/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0468e-04\n",
            "Epoch 726/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9552e-04\n",
            "Epoch 727/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8456e-04\n",
            "Epoch 728/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8017e-04\n",
            "Epoch 729/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1308e-04\n",
            "Epoch 730/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8971e-04\n",
            "Epoch 731/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0011e-04\n",
            "Epoch 732/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8774e-04\n",
            "Epoch 733/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8454e-04\n",
            "Epoch 734/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8539e-04\n",
            "Epoch 735/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8047e-04\n",
            "Epoch 736/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9207e-04\n",
            "Epoch 737/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9107e-04\n",
            "Epoch 738/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8653e-04\n",
            "Epoch 739/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8465e-04\n",
            "Epoch 740/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 5.0509e-04\n",
            "Epoch 741/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9126e-04\n",
            "Epoch 742/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8512e-04\n",
            "Epoch 743/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8906e-04\n",
            "Epoch 744/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9709e-04\n",
            "Epoch 745/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8856e-04\n",
            "Epoch 746/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9000e-04\n",
            "Epoch 747/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8686e-04\n",
            "Epoch 748/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8399e-04\n",
            "Epoch 749/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8231e-04\n",
            "Epoch 750/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8285e-04\n",
            "Epoch 751/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9704e-04\n",
            "Epoch 752/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8990e-04\n",
            "Epoch 753/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8607e-04\n",
            "Epoch 754/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8177e-04\n",
            "Epoch 755/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7879e-04\n",
            "Epoch 756/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8147e-04\n",
            "Epoch 757/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8949e-04\n",
            "Epoch 758/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9188e-04\n",
            "Epoch 759/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8431e-04\n",
            "Epoch 760/2000\n",
            "1566/1566 [==============================] - 0s 39us/step - loss: 4.8817e-04\n",
            "Epoch 761/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9831e-04\n",
            "Epoch 762/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8477e-04\n",
            "Epoch 763/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8365e-04\n",
            "Epoch 764/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7898e-04\n",
            "Epoch 765/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8016e-04\n",
            "Epoch 766/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8889e-04\n",
            "Epoch 767/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8968e-04\n",
            "Epoch 768/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9108e-04\n",
            "Epoch 769/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8914e-04\n",
            "Epoch 770/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9287e-04\n",
            "Epoch 771/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8764e-04\n",
            "Epoch 772/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9798e-04\n",
            "Epoch 773/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9344e-04\n",
            "Epoch 774/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8264e-04\n",
            "Epoch 775/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9452e-04\n",
            "Epoch 776/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8735e-04\n",
            "Epoch 777/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0411e-04\n",
            "Epoch 778/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.1293e-04\n",
            "Epoch 779/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0237e-04\n",
            "Epoch 780/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0215e-04\n",
            "Epoch 781/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0164e-04\n",
            "Epoch 782/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0059e-04\n",
            "Epoch 783/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9460e-04\n",
            "Epoch 784/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8977e-04\n",
            "Epoch 785/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8892e-04\n",
            "Epoch 786/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0413e-04\n",
            "Epoch 787/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9771e-04\n",
            "Epoch 788/2000\n",
            "1566/1566 [==============================] - 0s 47us/step - loss: 4.8656e-04\n",
            "Epoch 789/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9763e-04\n",
            "Epoch 790/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9035e-04\n",
            "Epoch 791/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8937e-04\n",
            "Epoch 792/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8570e-04\n",
            "Epoch 793/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8765e-04\n",
            "Epoch 794/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9053e-04\n",
            "Epoch 795/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9259e-04\n",
            "Epoch 796/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9096e-04\n",
            "Epoch 797/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8709e-04\n",
            "Epoch 798/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9117e-04\n",
            "Epoch 799/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9269e-04\n",
            "Epoch 800/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8891e-04\n",
            "Epoch 801/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8504e-04\n",
            "Epoch 802/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8592e-04\n",
            "Epoch 803/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9007e-04\n",
            "Epoch 804/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9007e-04\n",
            "Epoch 805/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9098e-04\n",
            "Epoch 806/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9302e-04\n",
            "Epoch 807/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8516e-04\n",
            "Epoch 808/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8518e-04\n",
            "Epoch 809/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8200e-04\n",
            "Epoch 810/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7991e-04\n",
            "Epoch 811/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8253e-04\n",
            "Epoch 812/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8199e-04\n",
            "Epoch 813/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9326e-04\n",
            "Epoch 814/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8239e-04\n",
            "Epoch 815/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8449e-04\n",
            "Epoch 816/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8538e-04\n",
            "Epoch 817/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8914e-04\n",
            "Epoch 818/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8129e-04\n",
            "Epoch 819/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8486e-04\n",
            "Epoch 820/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8925e-04\n",
            "Epoch 821/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8585e-04\n",
            "Epoch 822/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8132e-04\n",
            "Epoch 823/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7510e-04\n",
            "Epoch 824/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9629e-04\n",
            "Epoch 825/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9377e-04\n",
            "Epoch 826/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9588e-04\n",
            "Epoch 827/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9821e-04\n",
            "Epoch 828/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9426e-04\n",
            "Epoch 829/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9019e-04\n",
            "Epoch 830/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9287e-04\n",
            "Epoch 831/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8746e-04\n",
            "Epoch 832/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8803e-04\n",
            "Epoch 833/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8777e-04\n",
            "Epoch 834/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8963e-04\n",
            "Epoch 835/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7840e-04\n",
            "Epoch 836/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9576e-04\n",
            "Epoch 837/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8088e-04\n",
            "Epoch 838/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 5.0610e-04\n",
            "Epoch 839/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9377e-04\n",
            "Epoch 840/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0388e-04\n",
            "Epoch 841/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9240e-04\n",
            "Epoch 842/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.0690e-04\n",
            "Epoch 843/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0373e-04\n",
            "Epoch 844/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8733e-04\n",
            "Epoch 845/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9238e-04\n",
            "Epoch 846/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8082e-04\n",
            "Epoch 847/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8978e-04\n",
            "Epoch 848/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8377e-04\n",
            "Epoch 849/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7397e-04\n",
            "Epoch 850/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9154e-04\n",
            "Epoch 851/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8141e-04\n",
            "Epoch 852/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9826e-04\n",
            "Epoch 853/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8075e-04\n",
            "Epoch 854/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7898e-04\n",
            "Epoch 855/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8541e-04\n",
            "Epoch 856/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9116e-04\n",
            "Epoch 857/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9887e-04\n",
            "Epoch 858/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8686e-04\n",
            "Epoch 859/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8365e-04\n",
            "Epoch 860/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8049e-04\n",
            "Epoch 861/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8217e-04\n",
            "Epoch 862/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8139e-04\n",
            "Epoch 863/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8965e-04\n",
            "Epoch 864/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9429e-04\n",
            "Epoch 865/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8974e-04\n",
            "Epoch 866/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8095e-04\n",
            "Epoch 867/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8576e-04\n",
            "Epoch 868/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9285e-04\n",
            "Epoch 869/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8513e-04\n",
            "Epoch 870/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.9577e-04\n",
            "Epoch 871/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8702e-04\n",
            "Epoch 872/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8264e-04\n",
            "Epoch 873/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8682e-04\n",
            "Epoch 874/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9173e-04\n",
            "Epoch 875/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9050e-04\n",
            "Epoch 876/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8865e-04\n",
            "Epoch 877/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8160e-04\n",
            "Epoch 878/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8439e-04\n",
            "Epoch 879/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.8034e-04\n",
            "Epoch 880/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8190e-04\n",
            "Epoch 881/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8748e-04\n",
            "Epoch 882/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9397e-04\n",
            "Epoch 883/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8718e-04\n",
            "Epoch 884/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7743e-04\n",
            "Epoch 885/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7535e-04\n",
            "Epoch 886/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7996e-04\n",
            "Epoch 887/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8587e-04\n",
            "Epoch 888/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7366e-04\n",
            "Epoch 889/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7830e-04\n",
            "Epoch 890/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8799e-04\n",
            "Epoch 891/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7905e-04\n",
            "Epoch 892/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9558e-04\n",
            "Epoch 893/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8676e-04\n",
            "Epoch 894/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8905e-04\n",
            "Epoch 895/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8207e-04\n",
            "Epoch 896/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8657e-04\n",
            "Epoch 897/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8493e-04\n",
            "Epoch 898/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7976e-04\n",
            "Epoch 899/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8104e-04\n",
            "Epoch 900/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8117e-04\n",
            "Epoch 901/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8704e-04\n",
            "Epoch 902/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8391e-04\n",
            "Epoch 903/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7431e-04\n",
            "Epoch 904/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8218e-04\n",
            "Epoch 905/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8269e-04\n",
            "Epoch 906/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7521e-04\n",
            "Epoch 907/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7535e-04\n",
            "Epoch 908/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8590e-04\n",
            "Epoch 909/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8425e-04\n",
            "Epoch 910/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8246e-04\n",
            "Epoch 911/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8646e-04\n",
            "Epoch 912/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7746e-04\n",
            "Epoch 913/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7235e-04\n",
            "Epoch 914/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7858e-04\n",
            "Epoch 915/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8176e-04\n",
            "Epoch 916/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8899e-04\n",
            "Epoch 917/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8779e-04\n",
            "Epoch 918/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9847e-04\n",
            "Epoch 919/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7386e-04\n",
            "Epoch 920/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8326e-04\n",
            "Epoch 921/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7655e-04\n",
            "Epoch 922/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8426e-04\n",
            "Epoch 923/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7605e-04\n",
            "Epoch 924/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9575e-04\n",
            "Epoch 925/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0021e-04\n",
            "Epoch 926/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6461e-04\n",
            "Epoch 927/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8605e-04\n",
            "Epoch 928/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0377e-04\n",
            "Epoch 929/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8965e-04\n",
            "Epoch 930/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7858e-04\n",
            "Epoch 931/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7836e-04\n",
            "Epoch 932/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9274e-04\n",
            "Epoch 933/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7755e-04\n",
            "Epoch 934/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8050e-04\n",
            "Epoch 935/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 5.0407e-04\n",
            "Epoch 936/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9438e-04\n",
            "Epoch 937/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9476e-04\n",
            "Epoch 938/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9529e-04\n",
            "Epoch 939/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8674e-04\n",
            "Epoch 940/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 5.0702e-04\n",
            "Epoch 941/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9858e-04\n",
            "Epoch 942/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9328e-04\n",
            "Epoch 943/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8597e-04\n",
            "Epoch 944/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8644e-04\n",
            "Epoch 945/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8990e-04\n",
            "Epoch 946/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8667e-04\n",
            "Epoch 947/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8842e-04\n",
            "Epoch 948/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9600e-04\n",
            "Epoch 949/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8709e-04\n",
            "Epoch 950/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9564e-04\n",
            "Epoch 951/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9149e-04\n",
            "Epoch 952/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8165e-04\n",
            "Epoch 953/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8540e-04\n",
            "Epoch 954/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8430e-04\n",
            "Epoch 955/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8436e-04\n",
            "Epoch 956/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8583e-04\n",
            "Epoch 957/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8555e-04\n",
            "Epoch 958/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9320e-04\n",
            "Epoch 959/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9572e-04\n",
            "Epoch 960/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8505e-04\n",
            "Epoch 961/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9113e-04\n",
            "Epoch 962/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9303e-04\n",
            "Epoch 963/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9013e-04\n",
            "Epoch 964/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8460e-04\n",
            "Epoch 965/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8394e-04\n",
            "Epoch 966/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8501e-04\n",
            "Epoch 967/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8323e-04\n",
            "Epoch 968/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9060e-04\n",
            "Epoch 969/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8678e-04\n",
            "Epoch 970/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8608e-04\n",
            "Epoch 971/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8571e-04\n",
            "Epoch 972/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8998e-04\n",
            "Epoch 973/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8314e-04\n",
            "Epoch 974/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7753e-04\n",
            "Epoch 975/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8115e-04\n",
            "Epoch 976/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8501e-04\n",
            "Epoch 977/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8173e-04\n",
            "Epoch 978/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8054e-04\n",
            "Epoch 979/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 5.1213e-04\n",
            "Epoch 980/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9418e-04\n",
            "Epoch 981/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0396e-04\n",
            "Epoch 982/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8865e-04\n",
            "Epoch 983/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8201e-04\n",
            "Epoch 984/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8377e-04\n",
            "Epoch 985/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9271e-04\n",
            "Epoch 986/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0286e-04\n",
            "Epoch 987/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8802e-04\n",
            "Epoch 988/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8859e-04\n",
            "Epoch 989/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8794e-04\n",
            "Epoch 990/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8605e-04\n",
            "Epoch 991/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8502e-04\n",
            "Epoch 992/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8706e-04\n",
            "Epoch 993/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8315e-04\n",
            "Epoch 994/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.9008e-04\n",
            "Epoch 995/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8583e-04\n",
            "Epoch 996/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9202e-04\n",
            "Epoch 997/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9280e-04\n",
            "Epoch 998/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9973e-04\n",
            "Epoch 999/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.9569e-04\n",
            "Epoch 1000/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9435e-04\n",
            "Epoch 1001/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9006e-04\n",
            "Epoch 1002/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9222e-04\n",
            "Epoch 1003/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.8881e-04\n",
            "Epoch 1004/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8085e-04\n",
            "Epoch 1005/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8299e-04\n",
            "Epoch 1006/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8102e-04\n",
            "Epoch 1007/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9273e-04\n",
            "Epoch 1008/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8521e-04\n",
            "Epoch 1009/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7952e-04\n",
            "Epoch 1010/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8041e-04\n",
            "Epoch 1011/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8290e-04\n",
            "Epoch 1012/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8152e-04\n",
            "Epoch 1013/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8323e-04\n",
            "Epoch 1014/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0200e-04\n",
            "Epoch 1015/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8920e-04\n",
            "Epoch 1016/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8145e-04\n",
            "Epoch 1017/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8140e-04\n",
            "Epoch 1018/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8225e-04\n",
            "Epoch 1019/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.7579e-04\n",
            "Epoch 1020/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8219e-04\n",
            "Epoch 1021/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8574e-04\n",
            "Epoch 1022/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8407e-04\n",
            "Epoch 1023/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8926e-04\n",
            "Epoch 1024/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8261e-04\n",
            "Epoch 1025/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7959e-04\n",
            "Epoch 1026/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8145e-04\n",
            "Epoch 1027/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8132e-04\n",
            "Epoch 1028/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8062e-04\n",
            "Epoch 1029/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8503e-04\n",
            "Epoch 1030/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8443e-04\n",
            "Epoch 1031/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8241e-04\n",
            "Epoch 1032/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8560e-04\n",
            "Epoch 1033/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8419e-04\n",
            "Epoch 1034/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9076e-04\n",
            "Epoch 1035/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8824e-04\n",
            "Epoch 1036/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8010e-04\n",
            "Epoch 1037/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8137e-04\n",
            "Epoch 1038/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8226e-04\n",
            "Epoch 1039/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8584e-04\n",
            "Epoch 1040/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8629e-04\n",
            "Epoch 1041/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8327e-04\n",
            "Epoch 1042/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9177e-04\n",
            "Epoch 1043/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8330e-04\n",
            "Epoch 1044/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7994e-04\n",
            "Epoch 1045/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8587e-04\n",
            "Epoch 1046/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8398e-04\n",
            "Epoch 1047/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9217e-04\n",
            "Epoch 1048/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9527e-04\n",
            "Epoch 1049/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8766e-04\n",
            "Epoch 1050/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8564e-04\n",
            "Epoch 1051/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8516e-04\n",
            "Epoch 1052/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7758e-04\n",
            "Epoch 1053/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8177e-04\n",
            "Epoch 1054/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9251e-04\n",
            "Epoch 1055/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8951e-04\n",
            "Epoch 1056/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8874e-04\n",
            "Epoch 1057/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8581e-04\n",
            "Epoch 1058/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8598e-04\n",
            "Epoch 1059/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8323e-04\n",
            "Epoch 1060/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8942e-04\n",
            "Epoch 1061/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8499e-04\n",
            "Epoch 1062/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8753e-04\n",
            "Epoch 1063/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9698e-04\n",
            "Epoch 1064/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 5.0110e-04\n",
            "Epoch 1065/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8715e-04\n",
            "Epoch 1066/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8493e-04\n",
            "Epoch 1067/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7840e-04\n",
            "Epoch 1068/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7741e-04\n",
            "Epoch 1069/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8039e-04\n",
            "Epoch 1070/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9737e-04\n",
            "Epoch 1071/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8449e-04\n",
            "Epoch 1072/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8035e-04\n",
            "Epoch 1073/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8074e-04\n",
            "Epoch 1074/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9190e-04\n",
            "Epoch 1075/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9122e-04\n",
            "Epoch 1076/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9709e-04\n",
            "Epoch 1077/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8791e-04\n",
            "Epoch 1078/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8388e-04\n",
            "Epoch 1079/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8235e-04\n",
            "Epoch 1080/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8271e-04\n",
            "Epoch 1081/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7889e-04\n",
            "Epoch 1082/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7955e-04\n",
            "Epoch 1083/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7932e-04\n",
            "Epoch 1084/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8022e-04\n",
            "Epoch 1085/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8337e-04\n",
            "Epoch 1086/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8260e-04\n",
            "Epoch 1087/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8928e-04\n",
            "Epoch 1088/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8223e-04\n",
            "Epoch 1089/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8319e-04\n",
            "Epoch 1090/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0224e-04\n",
            "Epoch 1091/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8484e-04\n",
            "Epoch 1092/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8256e-04\n",
            "Epoch 1093/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7992e-04\n",
            "Epoch 1094/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8811e-04\n",
            "Epoch 1095/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8320e-04\n",
            "Epoch 1096/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8850e-04\n",
            "Epoch 1097/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8509e-04\n",
            "Epoch 1098/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9205e-04\n",
            "Epoch 1099/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8157e-04\n",
            "Epoch 1100/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8102e-04\n",
            "Epoch 1101/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8301e-04\n",
            "Epoch 1102/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7731e-04\n",
            "Epoch 1103/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7846e-04\n",
            "Epoch 1104/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9346e-04\n",
            "Epoch 1105/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8398e-04\n",
            "Epoch 1106/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8403e-04\n",
            "Epoch 1107/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8469e-04\n",
            "Epoch 1108/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7886e-04\n",
            "Epoch 1109/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7772e-04\n",
            "Epoch 1110/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9148e-04\n",
            "Epoch 1111/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9348e-04\n",
            "Epoch 1112/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8661e-04\n",
            "Epoch 1113/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8436e-04\n",
            "Epoch 1114/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8114e-04\n",
            "Epoch 1115/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8639e-04\n",
            "Epoch 1116/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8028e-04\n",
            "Epoch 1117/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7731e-04\n",
            "Epoch 1118/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8416e-04\n",
            "Epoch 1119/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8054e-04\n",
            "Epoch 1120/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7766e-04\n",
            "Epoch 1121/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7354e-04\n",
            "Epoch 1122/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9347e-04\n",
            "Epoch 1123/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8003e-04\n",
            "Epoch 1124/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8548e-04\n",
            "Epoch 1125/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8449e-04\n",
            "Epoch 1126/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8790e-04\n",
            "Epoch 1127/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8711e-04\n",
            "Epoch 1128/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9020e-04\n",
            "Epoch 1129/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7778e-04\n",
            "Epoch 1130/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.7423e-04\n",
            "Epoch 1131/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7796e-04\n",
            "Epoch 1132/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7973e-04\n",
            "Epoch 1133/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7821e-04\n",
            "Epoch 1134/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8516e-04\n",
            "Epoch 1135/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7893e-04\n",
            "Epoch 1136/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8842e-04\n",
            "Epoch 1137/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8367e-04\n",
            "Epoch 1138/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8327e-04\n",
            "Epoch 1139/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8254e-04\n",
            "Epoch 1140/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8230e-04\n",
            "Epoch 1141/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9094e-04\n",
            "Epoch 1142/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8455e-04\n",
            "Epoch 1143/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8073e-04\n",
            "Epoch 1144/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7986e-04\n",
            "Epoch 1145/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7955e-04\n",
            "Epoch 1146/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8283e-04\n",
            "Epoch 1147/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8566e-04\n",
            "Epoch 1148/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7775e-04\n",
            "Epoch 1149/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8944e-04\n",
            "Epoch 1150/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8041e-04\n",
            "Epoch 1151/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7533e-04\n",
            "Epoch 1152/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7640e-04\n",
            "Epoch 1153/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8100e-04\n",
            "Epoch 1154/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7715e-04\n",
            "Epoch 1155/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7387e-04\n",
            "Epoch 1156/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8036e-04\n",
            "Epoch 1157/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9732e-04\n",
            "Epoch 1158/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8969e-04\n",
            "Epoch 1159/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8261e-04\n",
            "Epoch 1160/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8883e-04\n",
            "Epoch 1161/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8573e-04\n",
            "Epoch 1162/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8384e-04\n",
            "Epoch 1163/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7902e-04\n",
            "Epoch 1164/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8096e-04\n",
            "Epoch 1165/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8259e-04\n",
            "Epoch 1166/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7728e-04\n",
            "Epoch 1167/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7743e-04\n",
            "Epoch 1168/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7818e-04\n",
            "Epoch 1169/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7986e-04\n",
            "Epoch 1170/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7867e-04\n",
            "Epoch 1171/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9150e-04\n",
            "Epoch 1172/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9052e-04\n",
            "Epoch 1173/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8026e-04\n",
            "Epoch 1174/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7565e-04\n",
            "Epoch 1175/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8835e-04\n",
            "Epoch 1176/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7993e-04\n",
            "Epoch 1177/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8114e-04\n",
            "Epoch 1178/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7964e-04\n",
            "Epoch 1179/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8083e-04\n",
            "Epoch 1180/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8330e-04\n",
            "Epoch 1181/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7983e-04\n",
            "Epoch 1182/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7551e-04\n",
            "Epoch 1183/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8141e-04\n",
            "Epoch 1184/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7774e-04\n",
            "Epoch 1185/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9299e-04\n",
            "Epoch 1186/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8220e-04\n",
            "Epoch 1187/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8064e-04\n",
            "Epoch 1188/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8370e-04\n",
            "Epoch 1189/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.7922e-04\n",
            "Epoch 1190/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7831e-04\n",
            "Epoch 1191/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8140e-04\n",
            "Epoch 1192/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7907e-04\n",
            "Epoch 1193/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8108e-04\n",
            "Epoch 1194/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8109e-04\n",
            "Epoch 1195/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8571e-04\n",
            "Epoch 1196/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9607e-04\n",
            "Epoch 1197/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9361e-04\n",
            "Epoch 1198/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9135e-04\n",
            "Epoch 1199/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8346e-04\n",
            "Epoch 1200/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8355e-04\n",
            "Epoch 1201/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8377e-04\n",
            "Epoch 1202/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7853e-04\n",
            "Epoch 1203/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8025e-04\n",
            "Epoch 1204/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7785e-04\n",
            "Epoch 1205/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.8507e-04\n",
            "Epoch 1206/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.7768e-04\n",
            "Epoch 1207/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7906e-04\n",
            "Epoch 1208/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7549e-04\n",
            "Epoch 1209/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7520e-04\n",
            "Epoch 1210/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7633e-04\n",
            "Epoch 1211/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8681e-04\n",
            "Epoch 1212/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7508e-04\n",
            "Epoch 1213/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8109e-04\n",
            "Epoch 1214/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7680e-04\n",
            "Epoch 1215/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8468e-04\n",
            "Epoch 1216/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7683e-04\n",
            "Epoch 1217/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8127e-04\n",
            "Epoch 1218/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8079e-04\n",
            "Epoch 1219/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8648e-04\n",
            "Epoch 1220/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7853e-04\n",
            "Epoch 1221/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7821e-04\n",
            "Epoch 1222/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7610e-04\n",
            "Epoch 1223/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7653e-04\n",
            "Epoch 1224/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8194e-04\n",
            "Epoch 1225/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8765e-04\n",
            "Epoch 1226/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8356e-04\n",
            "Epoch 1227/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8246e-04\n",
            "Epoch 1228/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7846e-04\n",
            "Epoch 1229/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7460e-04\n",
            "Epoch 1230/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8447e-04\n",
            "Epoch 1231/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7357e-04\n",
            "Epoch 1232/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8198e-04\n",
            "Epoch 1233/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8039e-04\n",
            "Epoch 1234/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7614e-04\n",
            "Epoch 1235/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8370e-04\n",
            "Epoch 1236/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7928e-04\n",
            "Epoch 1237/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8180e-04\n",
            "Epoch 1238/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7810e-04\n",
            "Epoch 1239/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8097e-04\n",
            "Epoch 1240/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8316e-04\n",
            "Epoch 1241/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8369e-04\n",
            "Epoch 1242/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8151e-04\n",
            "Epoch 1243/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7620e-04\n",
            "Epoch 1244/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7993e-04\n",
            "Epoch 1245/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8248e-04\n",
            "Epoch 1246/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8206e-04\n",
            "Epoch 1247/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8867e-04\n",
            "Epoch 1248/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8130e-04\n",
            "Epoch 1249/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7915e-04\n",
            "Epoch 1250/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8605e-04\n",
            "Epoch 1251/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8203e-04\n",
            "Epoch 1252/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9543e-04\n",
            "Epoch 1253/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.9125e-04\n",
            "Epoch 1254/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7690e-04\n",
            "Epoch 1255/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7941e-04\n",
            "Epoch 1256/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8072e-04\n",
            "Epoch 1257/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8783e-04\n",
            "Epoch 1258/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8192e-04\n",
            "Epoch 1259/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8085e-04\n",
            "Epoch 1260/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8486e-04\n",
            "Epoch 1261/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7325e-04\n",
            "Epoch 1262/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8072e-04\n",
            "Epoch 1263/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7948e-04\n",
            "Epoch 1264/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7976e-04\n",
            "Epoch 1265/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9284e-04\n",
            "Epoch 1266/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9525e-04\n",
            "Epoch 1267/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7947e-04\n",
            "Epoch 1268/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8301e-04\n",
            "Epoch 1269/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7817e-04\n",
            "Epoch 1270/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8161e-04\n",
            "Epoch 1271/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7941e-04\n",
            "Epoch 1272/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8178e-04\n",
            "Epoch 1273/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7509e-04\n",
            "Epoch 1274/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8028e-04\n",
            "Epoch 1275/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7958e-04\n",
            "Epoch 1276/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8454e-04\n",
            "Epoch 1277/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8282e-04\n",
            "Epoch 1278/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7615e-04\n",
            "Epoch 1279/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7742e-04\n",
            "Epoch 1280/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7446e-04\n",
            "Epoch 1281/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8328e-04\n",
            "Epoch 1282/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8505e-04\n",
            "Epoch 1283/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.8405e-04\n",
            "Epoch 1284/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8283e-04\n",
            "Epoch 1285/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7869e-04\n",
            "Epoch 1286/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7917e-04\n",
            "Epoch 1287/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7567e-04\n",
            "Epoch 1288/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8184e-04\n",
            "Epoch 1289/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8523e-04\n",
            "Epoch 1290/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7998e-04\n",
            "Epoch 1291/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7864e-04\n",
            "Epoch 1292/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7463e-04\n",
            "Epoch 1293/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7828e-04\n",
            "Epoch 1294/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8920e-04\n",
            "Epoch 1295/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9111e-04\n",
            "Epoch 1296/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8841e-04\n",
            "Epoch 1297/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7960e-04\n",
            "Epoch 1298/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7779e-04\n",
            "Epoch 1299/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8140e-04\n",
            "Epoch 1300/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7977e-04\n",
            "Epoch 1301/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7802e-04\n",
            "Epoch 1302/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7590e-04\n",
            "Epoch 1303/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8031e-04\n",
            "Epoch 1304/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7726e-04\n",
            "Epoch 1305/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8591e-04\n",
            "Epoch 1306/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7961e-04\n",
            "Epoch 1307/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7811e-04\n",
            "Epoch 1308/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7839e-04\n",
            "Epoch 1309/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7338e-04\n",
            "Epoch 1310/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9772e-04\n",
            "Epoch 1311/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8983e-04\n",
            "Epoch 1312/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8691e-04\n",
            "Epoch 1313/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9198e-04\n",
            "Epoch 1314/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0094e-04\n",
            "Epoch 1315/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0529e-04\n",
            "Epoch 1316/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9178e-04\n",
            "Epoch 1317/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8984e-04\n",
            "Epoch 1318/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9417e-04\n",
            "Epoch 1319/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9862e-04\n",
            "Epoch 1320/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8453e-04\n",
            "Epoch 1321/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8441e-04\n",
            "Epoch 1322/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8712e-04\n",
            "Epoch 1323/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8950e-04\n",
            "Epoch 1324/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8424e-04\n",
            "Epoch 1325/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9481e-04\n",
            "Epoch 1326/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8714e-04\n",
            "Epoch 1327/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9776e-04\n",
            "Epoch 1328/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9372e-04\n",
            "Epoch 1329/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8971e-04\n",
            "Epoch 1330/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8649e-04\n",
            "Epoch 1331/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8215e-04\n",
            "Epoch 1332/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7848e-04\n",
            "Epoch 1333/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8305e-04\n",
            "Epoch 1334/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8261e-04\n",
            "Epoch 1335/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8913e-04\n",
            "Epoch 1336/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8559e-04\n",
            "Epoch 1337/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8399e-04\n",
            "Epoch 1338/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7631e-04\n",
            "Epoch 1339/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9706e-04\n",
            "Epoch 1340/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.8592e-04\n",
            "Epoch 1341/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9130e-04\n",
            "Epoch 1342/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8088e-04\n",
            "Epoch 1343/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7399e-04\n",
            "Epoch 1344/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8038e-04\n",
            "Epoch 1345/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7936e-04\n",
            "Epoch 1346/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7737e-04\n",
            "Epoch 1347/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7648e-04\n",
            "Epoch 1348/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7483e-04\n",
            "Epoch 1349/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7992e-04\n",
            "Epoch 1350/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 5.0248e-04\n",
            "Epoch 1351/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7735e-04\n",
            "Epoch 1352/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7424e-04\n",
            "Epoch 1353/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7388e-04\n",
            "Epoch 1354/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8284e-04\n",
            "Epoch 1355/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8474e-04\n",
            "Epoch 1356/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8237e-04\n",
            "Epoch 1357/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7811e-04\n",
            "Epoch 1358/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7573e-04\n",
            "Epoch 1359/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7277e-04\n",
            "Epoch 1360/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8604e-04\n",
            "Epoch 1361/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9071e-04\n",
            "Epoch 1362/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8225e-04\n",
            "Epoch 1363/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7716e-04\n",
            "Epoch 1364/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8240e-04\n",
            "Epoch 1365/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7599e-04\n",
            "Epoch 1366/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7611e-04\n",
            "Epoch 1367/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8059e-04\n",
            "Epoch 1368/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7334e-04\n",
            "Epoch 1369/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7914e-04\n",
            "Epoch 1370/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8034e-04\n",
            "Epoch 1371/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8212e-04\n",
            "Epoch 1372/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7295e-04\n",
            "Epoch 1373/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8608e-04\n",
            "Epoch 1374/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8422e-04\n",
            "Epoch 1375/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8627e-04\n",
            "Epoch 1376/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8169e-04\n",
            "Epoch 1377/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7849e-04\n",
            "Epoch 1378/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8983e-04\n",
            "Epoch 1379/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7973e-04\n",
            "Epoch 1380/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7888e-04\n",
            "Epoch 1381/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8041e-04\n",
            "Epoch 1382/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7445e-04\n",
            "Epoch 1383/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7345e-04\n",
            "Epoch 1384/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7265e-04\n",
            "Epoch 1385/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8169e-04\n",
            "Epoch 1386/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8895e-04\n",
            "Epoch 1387/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8308e-04\n",
            "Epoch 1388/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7800e-04\n",
            "Epoch 1389/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7437e-04\n",
            "Epoch 1390/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7653e-04\n",
            "Epoch 1391/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8910e-04\n",
            "Epoch 1392/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7815e-04\n",
            "Epoch 1393/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8535e-04\n",
            "Epoch 1394/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8110e-04\n",
            "Epoch 1395/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8519e-04\n",
            "Epoch 1396/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8052e-04\n",
            "Epoch 1397/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7646e-04\n",
            "Epoch 1398/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7586e-04\n",
            "Epoch 1399/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8467e-04\n",
            "Epoch 1400/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9112e-04\n",
            "Epoch 1401/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8164e-04\n",
            "Epoch 1402/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8339e-04\n",
            "Epoch 1403/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8968e-04\n",
            "Epoch 1404/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7745e-04\n",
            "Epoch 1405/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7970e-04\n",
            "Epoch 1406/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7816e-04\n",
            "Epoch 1407/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8124e-04\n",
            "Epoch 1408/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9138e-04\n",
            "Epoch 1409/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7445e-04\n",
            "Epoch 1410/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7654e-04\n",
            "Epoch 1411/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7628e-04\n",
            "Epoch 1412/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8339e-04\n",
            "Epoch 1413/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7837e-04\n",
            "Epoch 1414/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.7208e-04\n",
            "Epoch 1415/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7343e-04\n",
            "Epoch 1416/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8003e-04\n",
            "Epoch 1417/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7677e-04\n",
            "Epoch 1418/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8275e-04\n",
            "Epoch 1419/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7987e-04\n",
            "Epoch 1420/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7890e-04\n",
            "Epoch 1421/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9302e-04\n",
            "Epoch 1422/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8491e-04\n",
            "Epoch 1423/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8113e-04\n",
            "Epoch 1424/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8186e-04\n",
            "Epoch 1425/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7598e-04\n",
            "Epoch 1426/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7438e-04\n",
            "Epoch 1427/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8978e-04\n",
            "Epoch 1428/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9674e-04\n",
            "Epoch 1429/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8171e-04\n",
            "Epoch 1430/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9066e-04\n",
            "Epoch 1431/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8717e-04\n",
            "Epoch 1432/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9118e-04\n",
            "Epoch 1433/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.9413e-04\n",
            "Epoch 1434/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8897e-04\n",
            "Epoch 1435/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8783e-04\n",
            "Epoch 1436/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8222e-04\n",
            "Epoch 1437/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8186e-04\n",
            "Epoch 1438/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8495e-04\n",
            "Epoch 1439/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7896e-04\n",
            "Epoch 1440/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7885e-04\n",
            "Epoch 1441/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7960e-04\n",
            "Epoch 1442/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8622e-04\n",
            "Epoch 1443/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8547e-04\n",
            "Epoch 1444/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7973e-04\n",
            "Epoch 1445/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7934e-04\n",
            "Epoch 1446/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7675e-04\n",
            "Epoch 1447/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8582e-04\n",
            "Epoch 1448/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8501e-04\n",
            "Epoch 1449/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7866e-04\n",
            "Epoch 1450/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8270e-04\n",
            "Epoch 1451/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8560e-04\n",
            "Epoch 1452/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8403e-04\n",
            "Epoch 1453/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8078e-04\n",
            "Epoch 1454/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8088e-04\n",
            "Epoch 1455/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8015e-04\n",
            "Epoch 1456/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8742e-04\n",
            "Epoch 1457/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8715e-04\n",
            "Epoch 1458/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8805e-04\n",
            "Epoch 1459/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7955e-04\n",
            "Epoch 1460/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7701e-04\n",
            "Epoch 1461/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8128e-04\n",
            "Epoch 1462/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7471e-04\n",
            "Epoch 1463/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7743e-04\n",
            "Epoch 1464/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7782e-04\n",
            "Epoch 1465/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7711e-04\n",
            "Epoch 1466/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7604e-04\n",
            "Epoch 1467/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9943e-04\n",
            "Epoch 1468/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8643e-04\n",
            "Epoch 1469/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8949e-04\n",
            "Epoch 1470/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7975e-04\n",
            "Epoch 1471/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7608e-04\n",
            "Epoch 1472/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7616e-04\n",
            "Epoch 1473/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8276e-04\n",
            "Epoch 1474/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8883e-04\n",
            "Epoch 1475/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7857e-04\n",
            "Epoch 1476/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7599e-04\n",
            "Epoch 1477/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7930e-04\n",
            "Epoch 1478/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8641e-04\n",
            "Epoch 1479/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7760e-04\n",
            "Epoch 1480/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7683e-04\n",
            "Epoch 1481/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7737e-04\n",
            "Epoch 1482/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7840e-04\n",
            "Epoch 1483/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7775e-04\n",
            "Epoch 1484/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7955e-04\n",
            "Epoch 1485/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7957e-04\n",
            "Epoch 1486/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9375e-04\n",
            "Epoch 1487/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8172e-04\n",
            "Epoch 1488/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8228e-04\n",
            "Epoch 1489/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7862e-04\n",
            "Epoch 1490/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7935e-04\n",
            "Epoch 1491/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7585e-04\n",
            "Epoch 1492/2000\n",
            "1566/1566 [==============================] - 0s 39us/step - loss: 4.7464e-04\n",
            "Epoch 1493/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7926e-04\n",
            "Epoch 1494/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8071e-04\n",
            "Epoch 1495/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8484e-04\n",
            "Epoch 1496/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8888e-04\n",
            "Epoch 1497/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8309e-04\n",
            "Epoch 1498/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8111e-04\n",
            "Epoch 1499/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8109e-04\n",
            "Epoch 1500/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 5.0571e-04\n",
            "Epoch 1501/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8980e-04\n",
            "Epoch 1502/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7786e-04\n",
            "Epoch 1503/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8540e-04\n",
            "Epoch 1504/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8848e-04\n",
            "Epoch 1505/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8076e-04\n",
            "Epoch 1506/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8700e-04\n",
            "Epoch 1507/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8115e-04\n",
            "Epoch 1508/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7355e-04\n",
            "Epoch 1509/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7789e-04\n",
            "Epoch 1510/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9006e-04\n",
            "Epoch 1511/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9194e-04\n",
            "Epoch 1512/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8665e-04\n",
            "Epoch 1513/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8149e-04\n",
            "Epoch 1514/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7826e-04\n",
            "Epoch 1515/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7416e-04\n",
            "Epoch 1516/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8099e-04\n",
            "Epoch 1517/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8118e-04\n",
            "Epoch 1518/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8149e-04\n",
            "Epoch 1519/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.7943e-04\n",
            "Epoch 1520/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7833e-04\n",
            "Epoch 1521/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7579e-04\n",
            "Epoch 1522/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7872e-04\n",
            "Epoch 1523/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7796e-04\n",
            "Epoch 1524/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7507e-04\n",
            "Epoch 1525/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7691e-04\n",
            "Epoch 1526/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8586e-04\n",
            "Epoch 1527/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8534e-04\n",
            "Epoch 1528/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8322e-04\n",
            "Epoch 1529/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8126e-04\n",
            "Epoch 1530/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8170e-04\n",
            "Epoch 1531/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7465e-04\n",
            "Epoch 1532/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8163e-04\n",
            "Epoch 1533/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8879e-04\n",
            "Epoch 1534/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7655e-04\n",
            "Epoch 1535/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7896e-04\n",
            "Epoch 1536/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7617e-04\n",
            "Epoch 1537/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8636e-04\n",
            "Epoch 1538/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8283e-04\n",
            "Epoch 1539/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7605e-04\n",
            "Epoch 1540/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7326e-04\n",
            "Epoch 1541/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7213e-04\n",
            "Epoch 1542/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7353e-04\n",
            "Epoch 1543/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7866e-04\n",
            "Epoch 1544/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8047e-04\n",
            "Epoch 1545/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7584e-04\n",
            "Epoch 1546/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7489e-04\n",
            "Epoch 1547/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7910e-04\n",
            "Epoch 1548/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8440e-04\n",
            "Epoch 1549/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7761e-04\n",
            "Epoch 1550/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8568e-04\n",
            "Epoch 1551/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9005e-04\n",
            "Epoch 1552/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8453e-04\n",
            "Epoch 1553/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8033e-04\n",
            "Epoch 1554/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8058e-04\n",
            "Epoch 1555/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8590e-04\n",
            "Epoch 1556/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9403e-04\n",
            "Epoch 1557/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8625e-04\n",
            "Epoch 1558/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7849e-04\n",
            "Epoch 1559/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7668e-04\n",
            "Epoch 1560/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8137e-04\n",
            "Epoch 1561/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8928e-04\n",
            "Epoch 1562/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7995e-04\n",
            "Epoch 1563/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8253e-04\n",
            "Epoch 1564/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7936e-04\n",
            "Epoch 1565/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8753e-04\n",
            "Epoch 1566/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9538e-04\n",
            "Epoch 1567/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9623e-04\n",
            "Epoch 1568/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8081e-04\n",
            "Epoch 1569/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8659e-04\n",
            "Epoch 1570/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8663e-04\n",
            "Epoch 1571/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7867e-04\n",
            "Epoch 1572/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7555e-04\n",
            "Epoch 1573/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8001e-04\n",
            "Epoch 1574/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7994e-04\n",
            "Epoch 1575/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8767e-04\n",
            "Epoch 1576/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7717e-04\n",
            "Epoch 1577/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8135e-04\n",
            "Epoch 1578/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8150e-04\n",
            "Epoch 1579/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8364e-04\n",
            "Epoch 1580/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7536e-04\n",
            "Epoch 1581/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8104e-04\n",
            "Epoch 1582/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8202e-04\n",
            "Epoch 1583/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8115e-04\n",
            "Epoch 1584/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8113e-04\n",
            "Epoch 1585/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7832e-04\n",
            "Epoch 1586/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8116e-04\n",
            "Epoch 1587/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7569e-04\n",
            "Epoch 1588/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7533e-04\n",
            "Epoch 1589/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8381e-04\n",
            "Epoch 1590/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7636e-04\n",
            "Epoch 1591/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7963e-04\n",
            "Epoch 1592/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9205e-04\n",
            "Epoch 1593/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8616e-04\n",
            "Epoch 1594/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8944e-04\n",
            "Epoch 1595/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8321e-04\n",
            "Epoch 1596/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7808e-04\n",
            "Epoch 1597/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8778e-04\n",
            "Epoch 1598/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8275e-04\n",
            "Epoch 1599/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7944e-04\n",
            "Epoch 1600/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7849e-04\n",
            "Epoch 1601/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7493e-04\n",
            "Epoch 1602/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8066e-04\n",
            "Epoch 1603/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7615e-04\n",
            "Epoch 1604/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8686e-04\n",
            "Epoch 1605/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8729e-04\n",
            "Epoch 1606/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8033e-04\n",
            "Epoch 1607/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7984e-04\n",
            "Epoch 1608/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9740e-04\n",
            "Epoch 1609/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8101e-04\n",
            "Epoch 1610/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8095e-04\n",
            "Epoch 1611/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8296e-04\n",
            "Epoch 1612/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7406e-04\n",
            "Epoch 1613/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8324e-04\n",
            "Epoch 1614/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7999e-04\n",
            "Epoch 1615/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8289e-04\n",
            "Epoch 1616/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8461e-04\n",
            "Epoch 1617/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7371e-04\n",
            "Epoch 1618/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7875e-04\n",
            "Epoch 1619/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.7822e-04\n",
            "Epoch 1620/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7371e-04\n",
            "Epoch 1621/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7271e-04\n",
            "Epoch 1622/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8323e-04\n",
            "Epoch 1623/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8785e-04\n",
            "Epoch 1624/2000\n",
            "1566/1566 [==============================] - 0s 42us/step - loss: 4.8252e-04\n",
            "Epoch 1625/2000\n",
            "1566/1566 [==============================] - 0s 43us/step - loss: 4.7687e-04\n",
            "Epoch 1626/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8015e-04\n",
            "Epoch 1627/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7842e-04\n",
            "Epoch 1628/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8612e-04\n",
            "Epoch 1629/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7974e-04\n",
            "Epoch 1630/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9332e-04\n",
            "Epoch 1631/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.8621e-04\n",
            "Epoch 1632/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7925e-04\n",
            "Epoch 1633/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8033e-04\n",
            "Epoch 1634/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7769e-04\n",
            "Epoch 1635/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7451e-04\n",
            "Epoch 1636/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8741e-04\n",
            "Epoch 1637/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8011e-04\n",
            "Epoch 1638/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8514e-04\n",
            "Epoch 1639/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8152e-04\n",
            "Epoch 1640/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7986e-04\n",
            "Epoch 1641/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8150e-04\n",
            "Epoch 1642/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7649e-04\n",
            "Epoch 1643/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8312e-04\n",
            "Epoch 1644/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7445e-04\n",
            "Epoch 1645/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8111e-04\n",
            "Epoch 1646/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8497e-04\n",
            "Epoch 1647/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8675e-04\n",
            "Epoch 1648/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8369e-04\n",
            "Epoch 1649/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7591e-04\n",
            "Epoch 1650/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9002e-04\n",
            "Epoch 1651/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9652e-04\n",
            "Epoch 1652/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8055e-04\n",
            "Epoch 1653/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8203e-04\n",
            "Epoch 1654/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8243e-04\n",
            "Epoch 1655/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7471e-04\n",
            "Epoch 1656/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7975e-04\n",
            "Epoch 1657/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7800e-04\n",
            "Epoch 1658/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8107e-04\n",
            "Epoch 1659/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7430e-04\n",
            "Epoch 1660/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7522e-04\n",
            "Epoch 1661/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7305e-04\n",
            "Epoch 1662/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7793e-04\n",
            "Epoch 1663/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8047e-04\n",
            "Epoch 1664/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8613e-04\n",
            "Epoch 1665/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8102e-04\n",
            "Epoch 1666/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9844e-04\n",
            "Epoch 1667/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8356e-04\n",
            "Epoch 1668/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7775e-04\n",
            "Epoch 1669/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8739e-04\n",
            "Epoch 1670/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7897e-04\n",
            "Epoch 1671/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8518e-04\n",
            "Epoch 1672/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7999e-04\n",
            "Epoch 1673/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7476e-04\n",
            "Epoch 1674/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7738e-04\n",
            "Epoch 1675/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7767e-04\n",
            "Epoch 1676/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7998e-04\n",
            "Epoch 1677/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8153e-04\n",
            "Epoch 1678/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7239e-04\n",
            "Epoch 1679/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7257e-04\n",
            "Epoch 1680/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7751e-04\n",
            "Epoch 1681/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7459e-04\n",
            "Epoch 1682/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6999e-04\n",
            "Epoch 1683/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7310e-04\n",
            "Epoch 1684/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7975e-04\n",
            "Epoch 1685/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9272e-04\n",
            "Epoch 1686/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9080e-04\n",
            "Epoch 1687/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8740e-04\n",
            "Epoch 1688/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8948e-04\n",
            "Epoch 1689/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8738e-04\n",
            "Epoch 1690/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8177e-04\n",
            "Epoch 1691/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7889e-04\n",
            "Epoch 1692/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7543e-04\n",
            "Epoch 1693/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7273e-04\n",
            "Epoch 1694/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7436e-04\n",
            "Epoch 1695/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7297e-04\n",
            "Epoch 1696/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7360e-04\n",
            "Epoch 1697/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7269e-04\n",
            "Epoch 1698/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6851e-04\n",
            "Epoch 1699/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8050e-04\n",
            "Epoch 1700/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7295e-04\n",
            "Epoch 1701/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7672e-04\n",
            "Epoch 1702/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6574e-04\n",
            "Epoch 1703/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8808e-04\n",
            "Epoch 1704/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8319e-04\n",
            "Epoch 1705/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8737e-04\n",
            "Epoch 1706/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8296e-04\n",
            "Epoch 1707/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8025e-04\n",
            "Epoch 1708/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9430e-04\n",
            "Epoch 1709/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8279e-04\n",
            "Epoch 1710/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9228e-04\n",
            "Epoch 1711/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8703e-04\n",
            "Epoch 1712/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8013e-04\n",
            "Epoch 1713/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8094e-04\n",
            "Epoch 1714/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8145e-04\n",
            "Epoch 1715/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8060e-04\n",
            "Epoch 1716/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9079e-04\n",
            "Epoch 1717/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9138e-04\n",
            "Epoch 1718/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8372e-04\n",
            "Epoch 1719/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7769e-04\n",
            "Epoch 1720/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7468e-04\n",
            "Epoch 1721/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7324e-04\n",
            "Epoch 1722/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7303e-04\n",
            "Epoch 1723/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7491e-04\n",
            "Epoch 1724/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7912e-04\n",
            "Epoch 1725/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8298e-04\n",
            "Epoch 1726/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9530e-04\n",
            "Epoch 1727/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8725e-04\n",
            "Epoch 1728/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7720e-04\n",
            "Epoch 1729/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8239e-04\n",
            "Epoch 1730/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7721e-04\n",
            "Epoch 1731/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7555e-04\n",
            "Epoch 1732/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8007e-04\n",
            "Epoch 1733/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9098e-04\n",
            "Epoch 1734/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8485e-04\n",
            "Epoch 1735/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7122e-04\n",
            "Epoch 1736/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7002e-04\n",
            "Epoch 1737/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7725e-04\n",
            "Epoch 1738/2000\n",
            "1566/1566 [==============================] - 0s 36us/step - loss: 4.7426e-04\n",
            "Epoch 1739/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7192e-04\n",
            "Epoch 1740/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7198e-04\n",
            "Epoch 1741/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7642e-04\n",
            "Epoch 1742/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7865e-04\n",
            "Epoch 1743/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7449e-04\n",
            "Epoch 1744/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7548e-04\n",
            "Epoch 1745/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0112e-04\n",
            "Epoch 1746/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8777e-04\n",
            "Epoch 1747/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8353e-04\n",
            "Epoch 1748/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8406e-04\n",
            "Epoch 1749/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8540e-04\n",
            "Epoch 1750/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8489e-04\n",
            "Epoch 1751/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7274e-04\n",
            "Epoch 1752/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7695e-04\n",
            "Epoch 1753/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8734e-04\n",
            "Epoch 1754/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7553e-04\n",
            "Epoch 1755/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7706e-04\n",
            "Epoch 1756/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8548e-04\n",
            "Epoch 1757/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8234e-04\n",
            "Epoch 1758/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7588e-04\n",
            "Epoch 1759/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9161e-04\n",
            "Epoch 1760/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7976e-04\n",
            "Epoch 1761/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8155e-04\n",
            "Epoch 1762/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7699e-04\n",
            "Epoch 1763/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7624e-04\n",
            "Epoch 1764/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7178e-04\n",
            "Epoch 1765/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7225e-04\n",
            "Epoch 1766/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7901e-04\n",
            "Epoch 1767/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7441e-04\n",
            "Epoch 1768/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7791e-04\n",
            "Epoch 1769/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7077e-04\n",
            "Epoch 1770/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7538e-04\n",
            "Epoch 1771/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.8603e-04\n",
            "Epoch 1772/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7825e-04\n",
            "Epoch 1773/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6647e-04\n",
            "Epoch 1774/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6796e-04\n",
            "Epoch 1775/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6861e-04\n",
            "Epoch 1776/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7179e-04\n",
            "Epoch 1777/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7351e-04\n",
            "Epoch 1778/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7363e-04\n",
            "Epoch 1779/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7253e-04\n",
            "Epoch 1780/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6952e-04\n",
            "Epoch 1781/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6697e-04\n",
            "Epoch 1782/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9597e-04\n",
            "Epoch 1783/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8963e-04\n",
            "Epoch 1784/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8391e-04\n",
            "Epoch 1785/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8086e-04\n",
            "Epoch 1786/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8707e-04\n",
            "Epoch 1787/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7608e-04\n",
            "Epoch 1788/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8451e-04\n",
            "Epoch 1789/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8927e-04\n",
            "Epoch 1790/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7880e-04\n",
            "Epoch 1791/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8221e-04\n",
            "Epoch 1792/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8416e-04\n",
            "Epoch 1793/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9798e-04\n",
            "Epoch 1794/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8605e-04\n",
            "Epoch 1795/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7686e-04\n",
            "Epoch 1796/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7477e-04\n",
            "Epoch 1797/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7359e-04\n",
            "Epoch 1798/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7131e-04\n",
            "Epoch 1799/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7479e-04\n",
            "Epoch 1800/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.8273e-04\n",
            "Epoch 1801/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7049e-04\n",
            "Epoch 1802/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7633e-04\n",
            "Epoch 1803/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8426e-04\n",
            "Epoch 1804/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7408e-04\n",
            "Epoch 1805/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7034e-04\n",
            "Epoch 1806/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6592e-04\n",
            "Epoch 1807/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8299e-04\n",
            "Epoch 1808/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7057e-04\n",
            "Epoch 1809/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6842e-04\n",
            "Epoch 1810/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6832e-04\n",
            "Epoch 1811/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7099e-04\n",
            "Epoch 1812/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7566e-04\n",
            "Epoch 1813/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7369e-04\n",
            "Epoch 1814/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7552e-04\n",
            "Epoch 1815/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6652e-04\n",
            "Epoch 1816/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7651e-04\n",
            "Epoch 1817/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.9084e-04\n",
            "Epoch 1818/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7679e-04\n",
            "Epoch 1819/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8411e-04\n",
            "Epoch 1820/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7898e-04\n",
            "Epoch 1821/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8030e-04\n",
            "Epoch 1822/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.6888e-04\n",
            "Epoch 1823/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9981e-04\n",
            "Epoch 1824/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9857e-04\n",
            "Epoch 1825/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9474e-04\n",
            "Epoch 1826/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8174e-04\n",
            "Epoch 1827/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8804e-04\n",
            "Epoch 1828/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8981e-04\n",
            "Epoch 1829/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8739e-04\n",
            "Epoch 1830/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7681e-04\n",
            "Epoch 1831/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8487e-04\n",
            "Epoch 1832/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7095e-04\n",
            "Epoch 1833/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7764e-04\n",
            "Epoch 1834/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7208e-04\n",
            "Epoch 1835/2000\n",
            "1566/1566 [==============================] - 0s 40us/step - loss: 4.6806e-04\n",
            "Epoch 1836/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.7835e-04\n",
            "Epoch 1837/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8444e-04\n",
            "Epoch 1838/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7817e-04\n",
            "Epoch 1839/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7712e-04\n",
            "Epoch 1840/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8530e-04\n",
            "Epoch 1841/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7720e-04\n",
            "Epoch 1842/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9179e-04\n",
            "Epoch 1843/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7791e-04\n",
            "Epoch 1844/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7538e-04\n",
            "Epoch 1845/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7103e-04\n",
            "Epoch 1846/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9592e-04\n",
            "Epoch 1847/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8791e-04\n",
            "Epoch 1848/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8370e-04\n",
            "Epoch 1849/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8180e-04\n",
            "Epoch 1850/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8678e-04\n",
            "Epoch 1851/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8555e-04\n",
            "Epoch 1852/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7755e-04\n",
            "Epoch 1853/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7523e-04\n",
            "Epoch 1854/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9472e-04\n",
            "Epoch 1855/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.9546e-04\n",
            "Epoch 1856/2000\n",
            "1566/1566 [==============================] - 0s 34us/step - loss: 4.7631e-04\n",
            "Epoch 1857/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.8258e-04\n",
            "Epoch 1858/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8142e-04\n",
            "Epoch 1859/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8097e-04\n",
            "Epoch 1860/2000\n",
            "1566/1566 [==============================] - 0s 38us/step - loss: 4.7590e-04\n",
            "Epoch 1861/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7057e-04\n",
            "Epoch 1862/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8852e-04\n",
            "Epoch 1863/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9351e-04\n",
            "Epoch 1864/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7597e-04\n",
            "Epoch 1865/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8194e-04\n",
            "Epoch 1866/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7082e-04\n",
            "Epoch 1867/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7315e-04\n",
            "Epoch 1868/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7510e-04\n",
            "Epoch 1869/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8280e-04\n",
            "Epoch 1870/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7916e-04\n",
            "Epoch 1871/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8351e-04\n",
            "Epoch 1872/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7523e-04\n",
            "Epoch 1873/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7429e-04\n",
            "Epoch 1874/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8054e-04\n",
            "Epoch 1875/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7363e-04\n",
            "Epoch 1876/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8220e-04\n",
            "Epoch 1877/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6733e-04\n",
            "Epoch 1878/2000\n",
            "1566/1566 [==============================] - 0s 32us/step - loss: 4.7247e-04\n",
            "Epoch 1879/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6718e-04\n",
            "Epoch 1880/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8508e-04\n",
            "Epoch 1881/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8615e-04\n",
            "Epoch 1882/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7832e-04\n",
            "Epoch 1883/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7890e-04\n",
            "Epoch 1884/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7625e-04\n",
            "Epoch 1885/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7567e-04\n",
            "Epoch 1886/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6991e-04\n",
            "Epoch 1887/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7924e-04\n",
            "Epoch 1888/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7436e-04\n",
            "Epoch 1889/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7086e-04\n",
            "Epoch 1890/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8407e-04\n",
            "Epoch 1891/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7366e-04\n",
            "Epoch 1892/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7726e-04\n",
            "Epoch 1893/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7254e-04\n",
            "Epoch 1894/2000\n",
            "1566/1566 [==============================] - 0s 33us/step - loss: 4.7099e-04\n",
            "Epoch 1895/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9152e-04\n",
            "Epoch 1896/2000\n",
            "1566/1566 [==============================] - 0s 37us/step - loss: 4.7694e-04\n",
            "Epoch 1897/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7146e-04\n",
            "Epoch 1898/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7076e-04\n",
            "Epoch 1899/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6793e-04\n",
            "Epoch 1900/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8006e-04\n",
            "Epoch 1901/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7852e-04\n",
            "Epoch 1902/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6717e-04\n",
            "Epoch 1903/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7177e-04\n",
            "Epoch 1904/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7128e-04\n",
            "Epoch 1905/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6904e-04\n",
            "Epoch 1906/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6522e-04\n",
            "Epoch 1907/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8577e-04\n",
            "Epoch 1908/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7678e-04\n",
            "Epoch 1909/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7937e-04\n",
            "Epoch 1910/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8726e-04\n",
            "Epoch 1911/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8284e-04\n",
            "Epoch 1912/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6956e-04\n",
            "Epoch 1913/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7755e-04\n",
            "Epoch 1914/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6995e-04\n",
            "Epoch 1915/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6774e-04\n",
            "Epoch 1916/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8193e-04\n",
            "Epoch 1917/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7694e-04\n",
            "Epoch 1918/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7300e-04\n",
            "Epoch 1919/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7158e-04\n",
            "Epoch 1920/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9679e-04\n",
            "Epoch 1921/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 5.0095e-04\n",
            "Epoch 1922/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8795e-04\n",
            "Epoch 1923/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8919e-04\n",
            "Epoch 1924/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7444e-04\n",
            "Epoch 1925/2000\n",
            "1566/1566 [==============================] - 0s 35us/step - loss: 4.7509e-04\n",
            "Epoch 1926/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7518e-04\n",
            "Epoch 1927/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8524e-04\n",
            "Epoch 1928/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7634e-04\n",
            "Epoch 1929/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7202e-04\n",
            "Epoch 1930/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7213e-04\n",
            "Epoch 1931/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.6833e-04\n",
            "Epoch 1932/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8113e-04\n",
            "Epoch 1933/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7895e-04\n",
            "Epoch 1934/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7878e-04\n",
            "Epoch 1935/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8212e-04\n",
            "Epoch 1936/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7652e-04\n",
            "Epoch 1937/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8371e-04\n",
            "Epoch 1938/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7646e-04\n",
            "Epoch 1939/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7961e-04\n",
            "Epoch 1940/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7207e-04\n",
            "Epoch 1941/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.6842e-04\n",
            "Epoch 1942/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7607e-04\n",
            "Epoch 1943/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7093e-04\n",
            "Epoch 1944/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7752e-04\n",
            "Epoch 1945/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.6694e-04\n",
            "Epoch 1946/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7142e-04\n",
            "Epoch 1947/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8487e-04\n",
            "Epoch 1948/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9392e-04\n",
            "Epoch 1949/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8522e-04\n",
            "Epoch 1950/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8358e-04\n",
            "Epoch 1951/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7831e-04\n",
            "Epoch 1952/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7889e-04\n",
            "Epoch 1953/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6892e-04\n",
            "Epoch 1954/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7223e-04\n",
            "Epoch 1955/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7292e-04\n",
            "Epoch 1956/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8993e-04\n",
            "Epoch 1957/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8897e-04\n",
            "Epoch 1958/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.9433e-04\n",
            "Epoch 1959/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.9564e-04\n",
            "Epoch 1960/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8185e-04\n",
            "Epoch 1961/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8253e-04\n",
            "Epoch 1962/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7790e-04\n",
            "Epoch 1963/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8248e-04\n",
            "Epoch 1964/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7864e-04\n",
            "Epoch 1965/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8509e-04\n",
            "Epoch 1966/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8341e-04\n",
            "Epoch 1967/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8971e-04\n",
            "Epoch 1968/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8617e-04\n",
            "Epoch 1969/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8437e-04\n",
            "Epoch 1970/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7221e-04\n",
            "Epoch 1971/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.6764e-04\n",
            "Epoch 1972/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6937e-04\n",
            "Epoch 1973/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9232e-04\n",
            "Epoch 1974/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.8319e-04\n",
            "Epoch 1975/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7261e-04\n",
            "Epoch 1976/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8908e-04\n",
            "Epoch 1977/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8284e-04\n",
            "Epoch 1978/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.7477e-04\n",
            "Epoch 1979/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.6949e-04\n",
            "Epoch 1980/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.7503e-04\n",
            "Epoch 1981/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.9273e-04\n",
            "Epoch 1982/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.9704e-04\n",
            "Epoch 1983/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8801e-04\n",
            "Epoch 1984/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8385e-04\n",
            "Epoch 1985/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8518e-04\n",
            "Epoch 1986/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8471e-04\n",
            "Epoch 1987/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8042e-04\n",
            "Epoch 1988/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7703e-04\n",
            "Epoch 1989/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7184e-04\n",
            "Epoch 1990/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7833e-04\n",
            "Epoch 1991/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.8072e-04\n",
            "Epoch 1992/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7503e-04\n",
            "Epoch 1993/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7864e-04\n",
            "Epoch 1994/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.7109e-04\n",
            "Epoch 1995/2000\n",
            "1566/1566 [==============================] - 0s 30us/step - loss: 4.7061e-04\n",
            "Epoch 1996/2000\n",
            "1566/1566 [==============================] - 0s 29us/step - loss: 4.8298e-04\n",
            "Epoch 1997/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.8187e-04\n",
            "Epoch 1998/2000\n",
            "1566/1566 [==============================] - 0s 27us/step - loss: 4.9268e-04\n",
            "Epoch 1999/2000\n",
            "1566/1566 [==============================] - 0s 31us/step - loss: 4.8061e-04\n",
            "Epoch 2000/2000\n",
            "1566/1566 [==============================] - 0s 28us/step - loss: 4.7812e-04\n",
            "Best: -0.000485 using {'l1_reg': 0, 'n_units': 50}\n",
            "-0.000499 (0.000217) with {'l1_reg': 0, 'n_units': 10}\n",
            "-0.000492 (0.000220) with {'l1_reg': 0, 'n_units': 20}\n",
            "-0.000495 (0.000224) with {'l1_reg': 0, 'n_units': 30}\n",
            "-0.000500 (0.000233) with {'l1_reg': 0, 'n_units': 40}\n",
            "-0.000485 (0.000213) with {'l1_reg': 0, 'n_units': 50}\n",
            "-0.000485 (0.000206) with {'l1_reg': 0, 'n_units': 60}\n",
            "-0.000489 (0.000213) with {'l1_reg': 0, 'n_units': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZAYw2NIw4jn",
        "outputId": "f77674f6-d7ad-49d1-bc84-8927d1131053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "param_grid = dict(n_units=n_units,l1_reg=l1_reg) \n",
        "n_splits = 10\n",
        "#kf = KFold(n_splits)\n",
        "#kf.get_n_splits(x_train_reg)\n",
        "MSE_train = 0\n",
        "MSE_test = 0\n",
        "TimeSeriesSplit(max_train_size=None, n_splits=10)\n",
        "tscv = TimeSeriesSplit()\n",
        "for train_fold, test_fold in tscv.split(x_train_reg):\n",
        "    x_train, x_test = x_train_reg[train_fold], x_train_reg[test_fold]\n",
        "    y_train, y_test = y_train_reg[train_fold], y_train_reg[test_fold]\n",
        "    rnn_model = RNN_model2(nodes,l1_reg)\n",
        "   \n",
        "    rnn_fit = rnn_model.fit(x_train,y_train, epochs=2000, batch_size=100, callbacks=[es])\n",
        "    rnn_pred_train = rnn_model.predict(x_train, verbose=0)\n",
        "    rnn_pred_test = rnn_model.predict(x_test, verbose=0) \n",
        "\n",
        "    # hyper parameter turning\n",
        "    #MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:,0])\n",
        "    MSE_train += mean_squared_error(y_train[:,0], rnn_pred_train[:,0])\n",
        "    MSE_test += mean_squared_error(y_test[:,0], rnn_pred_test[:,0])\n",
        "    #MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:,0])\n",
        "    \n",
        "rnn_model.summary()\n",
        "MSE_train_ave = MSE_train/n_splits\n",
        "MSE_train_std = np.math.sqrt(MSE_train/(n_splits-1))\n",
        "MSE_test_ave = MSE_test/n_splits\n",
        "MSE_test_std = np.math.sqrt(MSE_test/(n_splits-1))\n",
        "print(\"MSE_train_rnn_ave = \" + str(MSE_train_ave))\n",
        "print(\"MSE_test_rnn_ave = \" + str(MSE_test_ave))\n",
        "print(\"MSE_train_rnn_std = \" + str(MSE_train_std))\n",
        "print(\"MSE_test_rnn_std = \" + str(MSE_test_std))\n",
        "print(\"p = \" + str(p))\n",
        "\n",
        "rnn_model = RNN_model2(nodes,l1_reg)\n",
        "rnn_fit = rnn_model.fit(x_test_reg,y_test_reg, epochs=500, batch_size=100, callbacks=[es])\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],rnn_pred_test[:,0])\n",
        "print(\"RNN test data mse = \" + str(mse))\n",
        "print(\"RNN test std mse = \" + str(np.sqrt(mse)))\n",
        "rnn_model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.0433\n",
            "Epoch 2/2000\n",
            "261/261 [==============================] - 0s 58us/step - loss: 0.0287\n",
            "Epoch 3/2000\n",
            "261/261 [==============================] - 0s 51us/step - loss: 0.0152\n",
            "Epoch 4/2000\n",
            "261/261 [==============================] - 0s 65us/step - loss: 0.0048\n",
            "Epoch 5/2000\n",
            "261/261 [==============================] - 0s 63us/step - loss: 0.0043\n",
            "Epoch 6/2000\n",
            "261/261 [==============================] - 0s 65us/step - loss: 0.0042\n",
            "Epoch 7/2000\n",
            "261/261 [==============================] - 0s 68us/step - loss: 0.0012\n",
            "Epoch 8/2000\n",
            "261/261 [==============================] - 0s 61us/step - loss: 0.0027\n",
            "Epoch 9/2000\n",
            "261/261 [==============================] - 0s 59us/step - loss: 0.0019\n",
            "Epoch 10/2000\n",
            "261/261 [==============================] - 0s 66us/step - loss: 9.2133e-04\n",
            "Epoch 11/2000\n",
            "261/261 [==============================] - 0s 55us/step - loss: 0.0012\n",
            "Epoch 12/2000\n",
            "261/261 [==============================] - 0s 57us/step - loss: 0.0011\n",
            "Epoch 13/2000\n",
            "261/261 [==============================] - 0s 54us/step - loss: 7.8942e-04\n",
            "Epoch 14/2000\n",
            "261/261 [==============================] - 0s 53us/step - loss: 9.4929e-04\n",
            "Epoch 15/2000\n",
            "261/261 [==============================] - 0s 59us/step - loss: 8.2547e-04\n",
            "Epoch 16/2000\n",
            "261/261 [==============================] - 0s 52us/step - loss: 7.4801e-04\n",
            "Epoch 17/2000\n",
            "261/261 [==============================] - 0s 51us/step - loss: 7.8928e-04\n",
            "Epoch 18/2000\n",
            "261/261 [==============================] - 0s 63us/step - loss: 7.4192e-04\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/2000\n",
            "522/522 [==============================] - 1s 3ms/step - loss: 0.0438\n",
            "Epoch 2/2000\n",
            "522/522 [==============================] - 0s 50us/step - loss: 0.0203\n",
            "Epoch 3/2000\n",
            "522/522 [==============================] - 0s 48us/step - loss: 0.0040\n",
            "Epoch 4/2000\n",
            "522/522 [==============================] - 0s 49us/step - loss: 0.0023\n",
            "Epoch 5/2000\n",
            "522/522 [==============================] - 0s 50us/step - loss: 0.0014\n",
            "Epoch 6/2000\n",
            "522/522 [==============================] - 0s 50us/step - loss: 0.0014\n",
            "Epoch 7/2000\n",
            "522/522 [==============================] - 0s 47us/step - loss: 0.0013\n",
            "Epoch 8/2000\n",
            "522/522 [==============================] - 0s 47us/step - loss: 0.0012\n",
            "Epoch 9/2000\n",
            "522/522 [==============================] - 0s 48us/step - loss: 0.0012\n",
            "Epoch 10/2000\n",
            "522/522 [==============================] - 0s 47us/step - loss: 9.8972e-04\n",
            "Epoch 11/2000\n",
            "522/522 [==============================] - 0s 48us/step - loss: 0.0010\n",
            "Epoch 12/2000\n",
            "522/522 [==============================] - 0s 55us/step - loss: 0.0011\n",
            "Epoch 13/2000\n",
            "522/522 [==============================] - 0s 52us/step - loss: 0.0010\n",
            "Epoch 14/2000\n",
            "522/522 [==============================] - 0s 54us/step - loss: 9.6093e-04\n",
            "Epoch 15/2000\n",
            "522/522 [==============================] - 0s 45us/step - loss: 9.6798e-04\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/2000\n",
            "783/783 [==============================] - 2s 2ms/step - loss: 0.0530\n",
            "Epoch 2/2000\n",
            "783/783 [==============================] - 0s 40us/step - loss: 0.0317\n",
            "Epoch 3/2000\n",
            "783/783 [==============================] - 0s 42us/step - loss: 0.0117\n",
            "Epoch 4/2000\n",
            "783/783 [==============================] - 0s 42us/step - loss: 0.0026\n",
            "Epoch 5/2000\n",
            "783/783 [==============================] - 0s 41us/step - loss: 0.0014\n",
            "Epoch 6/2000\n",
            "783/783 [==============================] - 0s 44us/step - loss: 0.0013\n",
            "Epoch 7/2000\n",
            "783/783 [==============================] - 0s 45us/step - loss: 0.0010\n",
            "Epoch 8/2000\n",
            "783/783 [==============================] - 0s 45us/step - loss: 9.0548e-04\n",
            "Epoch 9/2000\n",
            "783/783 [==============================] - 0s 39us/step - loss: 8.5223e-04\n",
            "Epoch 10/2000\n",
            "783/783 [==============================] - 0s 46us/step - loss: 8.5062e-04\n",
            "Epoch 11/2000\n",
            "783/783 [==============================] - 0s 43us/step - loss: 8.2629e-04\n",
            "Epoch 12/2000\n",
            "783/783 [==============================] - 0s 46us/step - loss: 7.9545e-04\n",
            "Epoch 13/2000\n",
            "783/783 [==============================] - 0s 42us/step - loss: 7.7289e-04\n",
            "Epoch 14/2000\n",
            "783/783 [==============================] - 0s 46us/step - loss: 7.5619e-04\n",
            "Epoch 15/2000\n",
            "783/783 [==============================] - 0s 46us/step - loss: 7.4334e-04\n",
            "Epoch 16/2000\n",
            "783/783 [==============================] - 0s 42us/step - loss: 7.4504e-04\n",
            "Epoch 17/2000\n",
            "783/783 [==============================] - 0s 41us/step - loss: 7.4579e-04\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/2000\n",
            "1044/1044 [==============================] - 2s 2ms/step - loss: 0.0329\n",
            "Epoch 2/2000\n",
            "1044/1044 [==============================] - 0s 39us/step - loss: 0.0059\n",
            "Epoch 3/2000\n",
            "1044/1044 [==============================] - 0s 43us/step - loss: 0.0023\n",
            "Epoch 4/2000\n",
            "1044/1044 [==============================] - 0s 40us/step - loss: 0.0014\n",
            "Epoch 5/2000\n",
            "1044/1044 [==============================] - 0s 41us/step - loss: 0.0011\n",
            "Epoch 6/2000\n",
            "1044/1044 [==============================] - 0s 42us/step - loss: 9.5242e-04\n",
            "Epoch 7/2000\n",
            "1044/1044 [==============================] - 0s 49us/step - loss: 9.5319e-04\n",
            "Epoch 8/2000\n",
            "1044/1044 [==============================] - 0s 62us/step - loss: 8.6711e-04\n",
            "Epoch 9/2000\n",
            "1044/1044 [==============================] - 0s 47us/step - loss: 8.0677e-04\n",
            "Epoch 10/2000\n",
            "1044/1044 [==============================] - 0s 42us/step - loss: 8.2731e-04\n",
            "Epoch 11/2000\n",
            "1044/1044 [==============================] - 0s 42us/step - loss: 7.8058e-04\n",
            "Epoch 12/2000\n",
            "1044/1044 [==============================] - 0s 46us/step - loss: 7.9732e-04\n",
            "Epoch 13/2000\n",
            "1044/1044 [==============================] - 0s 46us/step - loss: 7.3766e-04\n",
            "Epoch 14/2000\n",
            "1044/1044 [==============================] - 0s 43us/step - loss: 7.1430e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/2000\n",
            "1305/1305 [==============================] - 2s 1ms/step - loss: 0.0171\n",
            "Epoch 2/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 0.0028\n",
            "Epoch 3/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 0.0014\n",
            "Epoch 4/2000\n",
            "1305/1305 [==============================] - 0s 39us/step - loss: 0.0011\n",
            "Epoch 5/2000\n",
            "1305/1305 [==============================] - 0s 41us/step - loss: 9.9697e-04\n",
            "Epoch 6/2000\n",
            "1305/1305 [==============================] - 0s 44us/step - loss: 9.2396e-04\n",
            "Epoch 7/2000\n",
            "1305/1305 [==============================] - 0s 42us/step - loss: 8.6297e-04\n",
            "Epoch 8/2000\n",
            "1305/1305 [==============================] - 0s 41us/step - loss: 8.5961e-04\n",
            "Epoch 9/2000\n",
            "1305/1305 [==============================] - 0s 41us/step - loss: 8.3262e-04\n",
            "Epoch 10/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 7.7599e-04\n",
            "Epoch 11/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 7.5094e-04\n",
            "Epoch 12/2000\n",
            "1305/1305 [==============================] - 0s 43us/step - loss: 7.4432e-04\n",
            "Epoch 13/2000\n",
            "1305/1305 [==============================] - 0s 43us/step - loss: 7.1410e-04\n",
            "Epoch 14/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 7.1161e-04\n",
            "Epoch 15/2000\n",
            "1305/1305 [==============================] - 0s 40us/step - loss: 7.0026e-04\n",
            "Epoch 16/2000\n",
            "1305/1305 [==============================] - 0s 41us/step - loss: 6.7867e-04\n",
            "Epoch 00016: early stopping\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_6 (SimpleRNN)     (None, 100)               10200     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 10,301\n",
            "Trainable params: 10,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "MSE_train_rnn_ave = 0.00040835409551388796\n",
            "MSE_test_rnn_ave = 0.00045248446909228\n",
            "MSE_train_rnn_std = 0.021300863193617504\n",
            "MSE_test_rnn_std = 0.022422321940727824\n",
            "p = 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b543090d0c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mrnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_model2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrnn_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mrnn_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_reg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_pred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_7 to have 2 dimensions, but got array with shape (384, 1, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Popk1QDixAXt",
        "outputId": "f6026f35-8861-4d8d-9b03-059bc521f4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Cross Validating LSTM with optmized parameters....\")\n",
        "#kf = KFold(n_splits)\n",
        "#kf.get_n_splits(x_train_reg)\n",
        "MSE_train = 0\n",
        "MSE_test = 0 \n",
        "for train_fold, test_fold in tscv.split(x_train_reg):\n",
        "    x_train, x_test = x_train_reg[train_fold], x_train_reg[test_fold]\n",
        "    y_train, y_test = y_train_reg[train_fold], y_train_reg[test_fold]\n",
        "    lstm_model = LSTM_model2(nodes,l1_reg)\n",
        "   \n",
        "    lstm_fit = lstm_model.fit(x_train,y_train, epochs=2000, batch_size=100, callbacks=[es])\n",
        "    lstm_pred_train = lstm_model.predict(x_train, verbose=0)\n",
        "    lstm_pred_test = lstm_model.predict(x_test, verbose=0) \n",
        "\n",
        "    # hyper parameter turning\n",
        "    #MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:,0])\n",
        "    MSE_train += mean_squared_error(y_train[:,0], lstm_pred_train[:,0])\n",
        "    MSE_test += mean_squared_error(y_test[:,0], lstm_pred_test[:,0])\n",
        "    #MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:,0])\n",
        "    \n",
        "lstm_model.summary()\n",
        "MSE_train_lstm_ave = MSE_train/n_splits\n",
        "MSE_test_lstm_ave = MSE_test/n_splits\n",
        "MSE_train_lstm_std = np.math.sqrt(MSE_train/(n_splits-1))\n",
        "MSE_test_lstm_std = np.math.sqrt(MSE_test/(n_splits-1))\n",
        "print(\"MSE_train_lstm_ave = \" + str(MSE_train_lstm_ave))\n",
        "print(\"MSE_test_lstm_ave = \" + str(MSE_test_lstm_ave))\n",
        "print(\"MSE_train_lmst_std = \" + str(MSE_train_lstm_std))\n",
        "print(\"MSE_test_lmst_std = \" + str(MSE_test_lstm_std))\n",
        "\n",
        "#print(\"neurons = \" + str(n_steps))\n",
        "#print(\"p = \" + str(p))\n",
        "#print(\"neurons = \" + str(n_steps))\n",
        "#print(\"l1 reg = 0\")\n",
        "# measure performane of test data using hyperparameters of best model\n",
        "lstm_model = LSTM_model2(nodes,l1_reg)\n",
        "lstm_fit = lstm_model.fit(x_test_reg,y_test_reg, epochs=500, batch_size=100, callbacks=[es])\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],lstm_pred_test[:,0])\n",
        "print(\"LSTM test data mse = \" + str(mse))\n",
        "print(\"LSTM test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "#score = cross_val_score(y_test_reg[:,0],g\n",
        "lstm_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validating LSTM with optmized parameters....\n",
            "Epoch 1/2000\n",
            "261/261 [==============================] - 3s 12ms/step - loss: 0.0605\n",
            "Epoch 2/2000\n",
            "261/261 [==============================] - 0s 166us/step - loss: 0.0559\n",
            "Epoch 3/2000\n",
            "261/261 [==============================] - 0s 165us/step - loss: 0.0512\n",
            "Epoch 4/2000\n",
            "261/261 [==============================] - 0s 167us/step - loss: 0.0460\n",
            "Epoch 5/2000\n",
            "261/261 [==============================] - 0s 167us/step - loss: 0.0407\n",
            "Epoch 6/2000\n",
            "261/261 [==============================] - 0s 166us/step - loss: 0.0345\n",
            "Epoch 7/2000\n",
            "261/261 [==============================] - 0s 163us/step - loss: 0.0271\n",
            "Epoch 8/2000\n",
            "261/261 [==============================] - 0s 173us/step - loss: 0.0182\n",
            "Epoch 9/2000\n",
            "261/261 [==============================] - 0s 213us/step - loss: 0.0103\n",
            "Epoch 10/2000\n",
            "261/261 [==============================] - 0s 177us/step - loss: 0.0056\n",
            "Epoch 11/2000\n",
            "261/261 [==============================] - 0s 180us/step - loss: 0.0064\n",
            "Epoch 12/2000\n",
            "261/261 [==============================] - 0s 168us/step - loss: 0.0036\n",
            "Epoch 13/2000\n",
            "261/261 [==============================] - 0s 166us/step - loss: 0.0034\n",
            "Epoch 14/2000\n",
            "261/261 [==============================] - 0s 181us/step - loss: 0.0030\n",
            "Epoch 15/2000\n",
            "261/261 [==============================] - 0s 184us/step - loss: 0.0022\n",
            "Epoch 16/2000\n",
            "261/261 [==============================] - 0s 183us/step - loss: 0.0024\n",
            "Epoch 17/2000\n",
            "261/261 [==============================] - 0s 179us/step - loss: 0.0019\n",
            "Epoch 18/2000\n",
            "261/261 [==============================] - 0s 181us/step - loss: 0.0021\n",
            "Epoch 19/2000\n",
            "261/261 [==============================] - 0s 181us/step - loss: 0.0018\n",
            "Epoch 20/2000\n",
            "261/261 [==============================] - 0s 180us/step - loss: 0.0019\n",
            "Epoch 21/2000\n",
            "261/261 [==============================] - 0s 184us/step - loss: 0.0017\n",
            "Epoch 22/2000\n",
            "261/261 [==============================] - 0s 181us/step - loss: 0.0018\n",
            "Epoch 23/2000\n",
            "261/261 [==============================] - 0s 176us/step - loss: 0.0016\n",
            "Epoch 24/2000\n",
            "261/261 [==============================] - 0s 178us/step - loss: 0.0017\n",
            "Epoch 25/2000\n",
            "261/261 [==============================] - 0s 219us/step - loss: 0.0016\n",
            "Epoch 26/2000\n",
            "261/261 [==============================] - 0s 176us/step - loss: 0.0016\n",
            "Epoch 00026: early stopping\n",
            "Epoch 1/2000\n",
            "522/522 [==============================] - 3s 7ms/step - loss: 0.0472\n",
            "Epoch 2/2000\n",
            "522/522 [==============================] - 0s 156us/step - loss: 0.0382\n",
            "Epoch 3/2000\n",
            "522/522 [==============================] - 0s 154us/step - loss: 0.0288\n",
            "Epoch 4/2000\n",
            "522/522 [==============================] - 0s 167us/step - loss: 0.0180\n",
            "Epoch 5/2000\n",
            "522/522 [==============================] - 0s 158us/step - loss: 0.0095\n",
            "Epoch 6/2000\n",
            "522/522 [==============================] - 0s 157us/step - loss: 0.0077\n",
            "Epoch 7/2000\n",
            "522/522 [==============================] - 0s 157us/step - loss: 0.0046\n",
            "Epoch 8/2000\n",
            "522/522 [==============================] - 0s 161us/step - loss: 0.0031\n",
            "Epoch 9/2000\n",
            "522/522 [==============================] - 0s 158us/step - loss: 0.0025\n",
            "Epoch 10/2000\n",
            "522/522 [==============================] - 0s 157us/step - loss: 0.0031\n",
            "Epoch 11/2000\n",
            "522/522 [==============================] - 0s 162us/step - loss: 0.0024\n",
            "Epoch 12/2000\n",
            "522/522 [==============================] - 0s 161us/step - loss: 0.0022\n",
            "Epoch 13/2000\n",
            "522/522 [==============================] - 0s 171us/step - loss: 0.0023\n",
            "Epoch 14/2000\n",
            "522/522 [==============================] - 0s 161us/step - loss: 0.0022\n",
            "Epoch 15/2000\n",
            "522/522 [==============================] - 0s 163us/step - loss: 0.0021\n",
            "Epoch 16/2000\n",
            "522/522 [==============================] - 0s 157us/step - loss: 0.0020\n",
            "Epoch 17/2000\n",
            "522/522 [==============================] - 0s 159us/step - loss: 0.0020\n",
            "Epoch 18/2000\n",
            "522/522 [==============================] - 0s 159us/step - loss: 0.0020\n",
            "Epoch 19/2000\n",
            "522/522 [==============================] - 0s 184us/step - loss: 0.0020\n",
            "Epoch 20/2000\n",
            "522/522 [==============================] - 0s 162us/step - loss: 0.0020\n",
            "Epoch 21/2000\n",
            "522/522 [==============================] - 0s 161us/step - loss: 0.0019\n",
            "Epoch 22/2000\n",
            "522/522 [==============================] - 0s 154us/step - loss: 0.0018\n",
            "Epoch 23/2000\n",
            "522/522 [==============================] - 0s 154us/step - loss: 0.0018\n",
            "Epoch 24/2000\n",
            "522/522 [==============================] - 0s 161us/step - loss: 0.0018\n",
            "Epoch 25/2000\n",
            "522/522 [==============================] - 0s 162us/step - loss: 0.0017\n",
            "Epoch 26/2000\n",
            "522/522 [==============================] - 0s 173us/step - loss: 0.0017\n",
            "Epoch 27/2000\n",
            "522/522 [==============================] - 0s 191us/step - loss: 0.0016\n",
            "Epoch 28/2000\n",
            "522/522 [==============================] - 0s 182us/step - loss: 0.0016\n",
            "Epoch 29/2000\n",
            "522/522 [==============================] - 0s 165us/step - loss: 0.0016\n",
            "Epoch 30/2000\n",
            "522/522 [==============================] - 0s 157us/step - loss: 0.0016\n",
            "Epoch 31/2000\n",
            "522/522 [==============================] - 0s 155us/step - loss: 0.0015\n",
            "Epoch 32/2000\n",
            "522/522 [==============================] - 0s 162us/step - loss: 0.0015\n",
            "Epoch 33/2000\n",
            "522/522 [==============================] - 0s 166us/step - loss: 0.0015\n",
            "Epoch 34/2000\n",
            "522/522 [==============================] - 0s 163us/step - loss: 0.0014\n",
            "Epoch 35/2000\n",
            "522/522 [==============================] - 0s 170us/step - loss: 0.0014\n",
            "Epoch 36/2000\n",
            "522/522 [==============================] - 0s 162us/step - loss: 0.0014\n",
            "Epoch 37/2000\n",
            "522/522 [==============================] - 0s 179us/step - loss: 0.0013\n",
            "Epoch 38/2000\n",
            "522/522 [==============================] - 0s 169us/step - loss: 0.0013\n",
            "Epoch 39/2000\n",
            "522/522 [==============================] - 0s 172us/step - loss: 0.0013\n",
            "Epoch 40/2000\n",
            "522/522 [==============================] - 0s 167us/step - loss: 0.0012\n",
            "Epoch 41/2000\n",
            "522/522 [==============================] - 0s 171us/step - loss: 0.0012\n",
            "Epoch 42/2000\n",
            "522/522 [==============================] - 0s 177us/step - loss: 0.0012\n",
            "Epoch 43/2000\n",
            "522/522 [==============================] - 0s 168us/step - loss: 0.0012\n",
            "Epoch 44/2000\n",
            "522/522 [==============================] - 0s 179us/step - loss: 0.0012\n",
            "Epoch 45/2000\n",
            "522/522 [==============================] - 0s 184us/step - loss: 0.0011\n",
            "Epoch 46/2000\n",
            "522/522 [==============================] - 0s 175us/step - loss: 0.0011\n",
            "Epoch 47/2000\n",
            "522/522 [==============================] - 0s 177us/step - loss: 0.0011\n",
            "Epoch 48/2000\n",
            "522/522 [==============================] - 0s 167us/step - loss: 0.0012\n",
            "Epoch 00048: early stopping\n",
            "Epoch 1/2000\n",
            "783/783 [==============================] - 4s 5ms/step - loss: 0.0394\n",
            "Epoch 2/2000\n",
            "783/783 [==============================] - 0s 146us/step - loss: 0.0279\n",
            "Epoch 3/2000\n",
            "783/783 [==============================] - 0s 155us/step - loss: 0.0160\n",
            "Epoch 4/2000\n",
            "783/783 [==============================] - 0s 144us/step - loss: 0.0068\n",
            "Epoch 5/2000\n",
            "783/783 [==============================] - 0s 146us/step - loss: 0.0035\n",
            "Epoch 6/2000\n",
            "783/783 [==============================] - 0s 154us/step - loss: 0.0024\n",
            "Epoch 7/2000\n",
            "783/783 [==============================] - 0s 145us/step - loss: 0.0022\n",
            "Epoch 8/2000\n",
            "783/783 [==============================] - 0s 159us/step - loss: 0.0021\n",
            "Epoch 9/2000\n",
            "783/783 [==============================] - 0s 152us/step - loss: 0.0020\n",
            "Epoch 10/2000\n",
            "783/783 [==============================] - 0s 155us/step - loss: 0.0019\n",
            "Epoch 11/2000\n",
            "783/783 [==============================] - 0s 148us/step - loss: 0.0019\n",
            "Epoch 12/2000\n",
            "783/783 [==============================] - 0s 154us/step - loss: 0.0019\n",
            "Epoch 13/2000\n",
            "783/783 [==============================] - 0s 149us/step - loss: 0.0018\n",
            "Epoch 14/2000\n",
            "783/783 [==============================] - 0s 156us/step - loss: 0.0017\n",
            "Epoch 15/2000\n",
            "783/783 [==============================] - 0s 150us/step - loss: 0.0017\n",
            "Epoch 16/2000\n",
            "783/783 [==============================] - 0s 156us/step - loss: 0.0016\n",
            "Epoch 17/2000\n",
            "783/783 [==============================] - 0s 146us/step - loss: 0.0016\n",
            "Epoch 18/2000\n",
            "783/783 [==============================] - 0s 162us/step - loss: 0.0015\n",
            "Epoch 19/2000\n",
            "783/783 [==============================] - 0s 154us/step - loss: 0.0015\n",
            "Epoch 20/2000\n",
            "783/783 [==============================] - 0s 170us/step - loss: 0.0014\n",
            "Epoch 21/2000\n",
            "783/783 [==============================] - 0s 148us/step - loss: 0.0014\n",
            "Epoch 22/2000\n",
            "783/783 [==============================] - 0s 148us/step - loss: 0.0014\n",
            "Epoch 23/2000\n",
            "783/783 [==============================] - 0s 149us/step - loss: 0.0013\n",
            "Epoch 24/2000\n",
            "783/783 [==============================] - 0s 145us/step - loss: 0.0013\n",
            "Epoch 25/2000\n",
            "783/783 [==============================] - 0s 147us/step - loss: 0.0013\n",
            "Epoch 26/2000\n",
            "783/783 [==============================] - 0s 147us/step - loss: 0.0012\n",
            "Epoch 27/2000\n",
            "783/783 [==============================] - 0s 143us/step - loss: 0.0012\n",
            "Epoch 28/2000\n",
            "783/783 [==============================] - 0s 152us/step - loss: 0.0012\n",
            "Epoch 29/2000\n",
            "783/783 [==============================] - 0s 146us/step - loss: 0.0011\n",
            "Epoch 30/2000\n",
            "783/783 [==============================] - 0s 149us/step - loss: 0.0011\n",
            "Epoch 31/2000\n",
            "783/783 [==============================] - 0s 150us/step - loss: 0.0011\n",
            "Epoch 00031: early stopping\n",
            "Epoch 1/2000\n",
            "1044/1044 [==============================] - 4s 4ms/step - loss: 0.0428\n",
            "Epoch 2/2000\n",
            "1044/1044 [==============================] - 0s 151us/step - loss: 0.0252\n",
            "Epoch 3/2000\n",
            "1044/1044 [==============================] - 0s 161us/step - loss: 0.0068\n",
            "Epoch 4/2000\n",
            "1044/1044 [==============================] - 0s 159us/step - loss: 0.0027\n",
            "Epoch 5/2000\n",
            "1044/1044 [==============================] - 0s 160us/step - loss: 0.0023\n",
            "Epoch 6/2000\n",
            "1044/1044 [==============================] - 0s 153us/step - loss: 0.0021\n",
            "Epoch 7/2000\n",
            "1044/1044 [==============================] - 0s 155us/step - loss: 0.0020\n",
            "Epoch 8/2000\n",
            "1044/1044 [==============================] - 0s 161us/step - loss: 0.0018\n",
            "Epoch 9/2000\n",
            "1044/1044 [==============================] - 0s 148us/step - loss: 0.0018\n",
            "Epoch 10/2000\n",
            "1044/1044 [==============================] - 0s 157us/step - loss: 0.0017\n",
            "Epoch 11/2000\n",
            "1044/1044 [==============================] - 0s 157us/step - loss: 0.0017\n",
            "Epoch 12/2000\n",
            "1044/1044 [==============================] - 0s 151us/step - loss: 0.0016\n",
            "Epoch 13/2000\n",
            "1044/1044 [==============================] - 0s 154us/step - loss: 0.0015\n",
            "Epoch 14/2000\n",
            "1044/1044 [==============================] - 0s 167us/step - loss: 0.0015\n",
            "Epoch 15/2000\n",
            "1044/1044 [==============================] - 0s 156us/step - loss: 0.0014\n",
            "Epoch 16/2000\n",
            "1044/1044 [==============================] - 0s 155us/step - loss: 0.0014\n",
            "Epoch 17/2000\n",
            "1044/1044 [==============================] - 0s 164us/step - loss: 0.0013\n",
            "Epoch 18/2000\n",
            "1044/1044 [==============================] - 0s 156us/step - loss: 0.0012\n",
            "Epoch 19/2000\n",
            "1044/1044 [==============================] - 0s 159us/step - loss: 0.0012\n",
            "Epoch 20/2000\n",
            "1044/1044 [==============================] - 0s 155us/step - loss: 0.0011\n",
            "Epoch 21/2000\n",
            "1044/1044 [==============================] - 0s 162us/step - loss: 0.0010\n",
            "Epoch 22/2000\n",
            "1044/1044 [==============================] - 0s 153us/step - loss: 0.0010\n",
            "Epoch 23/2000\n",
            "1044/1044 [==============================] - 0s 163us/step - loss: 0.0010\n",
            "Epoch 24/2000\n",
            "1044/1044 [==============================] - 0s 154us/step - loss: 9.7244e-04\n",
            "Epoch 25/2000\n",
            "1044/1044 [==============================] - 0s 157us/step - loss: 9.5508e-04\n",
            "Epoch 26/2000\n",
            "1044/1044 [==============================] - 0s 152us/step - loss: 9.5067e-04\n",
            "Epoch 00026: early stopping\n",
            "Epoch 1/2000\n",
            "1305/1305 [==============================] - 5s 4ms/step - loss: 0.0399\n",
            "Epoch 2/2000\n",
            "1305/1305 [==============================] - 0s 150us/step - loss: 0.0151\n",
            "Epoch 3/2000\n",
            "1305/1305 [==============================] - 0s 158us/step - loss: 0.0046\n",
            "Epoch 4/2000\n",
            "1305/1305 [==============================] - 0s 158us/step - loss: 0.0025\n",
            "Epoch 5/2000\n",
            "1305/1305 [==============================] - 0s 153us/step - loss: 0.0022\n",
            "Epoch 6/2000\n",
            "1305/1305 [==============================] - 0s 165us/step - loss: 0.0023\n",
            "Epoch 7/2000\n",
            "1305/1305 [==============================] - 0s 157us/step - loss: 0.0020\n",
            "Epoch 8/2000\n",
            "1305/1305 [==============================] - 0s 160us/step - loss: 0.0019\n",
            "Epoch 9/2000\n",
            "1305/1305 [==============================] - 0s 158us/step - loss: 0.0018\n",
            "Epoch 10/2000\n",
            "1305/1305 [==============================] - 0s 157us/step - loss: 0.0018\n",
            "Epoch 11/2000\n",
            "1305/1305 [==============================] - 0s 160us/step - loss: 0.0017\n",
            "Epoch 12/2000\n",
            "1305/1305 [==============================] - 0s 166us/step - loss: 0.0016\n",
            "Epoch 13/2000\n",
            "1305/1305 [==============================] - 0s 155us/step - loss: 0.0016\n",
            "Epoch 14/2000\n",
            "1305/1305 [==============================] - 0s 159us/step - loss: 0.0015\n",
            "Epoch 15/2000\n",
            "1305/1305 [==============================] - 0s 157us/step - loss: 0.0015\n",
            "Epoch 16/2000\n",
            "1305/1305 [==============================] - 0s 154us/step - loss: 0.0014\n",
            "Epoch 17/2000\n",
            "1305/1305 [==============================] - 0s 156us/step - loss: 0.0013\n",
            "Epoch 18/2000\n",
            "1305/1305 [==============================] - 0s 152us/step - loss: 0.0012\n",
            "Epoch 19/2000\n",
            "1305/1305 [==============================] - 0s 156us/step - loss: 0.0012\n",
            "Epoch 20/2000\n",
            "1305/1305 [==============================] - 0s 153us/step - loss: 0.0012\n",
            "Epoch 21/2000\n",
            "1305/1305 [==============================] - 0s 167us/step - loss: 0.0011\n",
            "Epoch 22/2000\n",
            "1305/1305 [==============================] - 0s 160us/step - loss: 0.0011\n",
            "Epoch 23/2000\n",
            "1305/1305 [==============================] - 0s 159us/step - loss: 0.0011\n",
            "Epoch 24/2000\n",
            "1305/1305 [==============================] - 0s 156us/step - loss: 0.0011\n",
            "Epoch 00024: early stopping\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 100)               40800     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 40,901\n",
            "Trainable params: 40,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "MSE_train_lstm_ave = 0.0005745918280220325\n",
            "MSE_test_lstm_ave = 0.0006809159745900725\n",
            "MSE_train_lmst_std = 0.02526727853309345\n",
            "MSE_test_lmst_std = 0.027505877646424603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-0ee831381123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# measure performane of test data using hyperparameters of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlstm_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mlstm_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_reg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_pred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_13 to have 2 dimensions, but got array with shape (384, 1, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RKpErbvBxF3h",
        "outputId": "36329ee4-cbb4-4072-8824-e72506f37a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Cross Validating GRU with optmized parameters....\")\n",
        "#kf = KFold(n_splits)\n",
        "#kf.get_n_splits(x_train_reg)\n",
        "MSE_train = 0\n",
        "MSE_test = 0 \n",
        "for train_fold, test_fold in tscv.split(x_train_reg):\n",
        "    x_train, x_test = x_train_reg[train_fold], x_train_reg[test_fold]\n",
        "    y_train, y_test = y_train_reg[train_fold], y_train_reg[test_fold]\n",
        "    gru_model = GRU_model2(nodes,l1_reg)\n",
        "   \n",
        "    gru_fit = gru_model.fit(x_train,y_train, epochs=2000, batch_size=100, callbacks=[es])\n",
        "    gru_pred_train = gru_model.predict(x_train, verbose=0)\n",
        "    gru_pred_test = gru_model.predict(x_test, verbose=0) \n",
        "\n",
        "    # hyper parameter turning\n",
        "    #MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:,0])\n",
        "    MSE_train += mean_squared_error(y_train[:,0], gru_pred_train[:,0])\n",
        "    MSE_test += mean_squared_error(y_test[:,0], gru_pred_test[:,0])\n",
        "    #MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:,0])\n",
        "    \n",
        "gru_model.summary()\n",
        "MSE_train_gru_ave = MSE_train/n_splits\n",
        "MSE_test_gru_ave = MSE_test/n_splits\n",
        "MSE_train_gru_std = np.math.sqrt(MSE_train/(n_splits-1))\n",
        "MSE_test_gru_std = np.math.sqrt(MSE_test/(n_splits-1))\n",
        "print(\"MSE_train_gru_ave = \" + str(MSE_train_gru_ave))\n",
        "print(\"MSE_test_gru_ave = \" + str(MSE_test_gru_ave))\n",
        "print(\"MSE_train_gru_std = \" + str(MSE_train_gru_std))\n",
        "print(\"MSE_test_gru_std = \" + str(MSE_test_gru_std))\n",
        "\n",
        "print(\"p = \" + str(p))  \n",
        "     \n",
        "gru_model = GRU_model2(nodes,l1_reg)\n",
        "gru_fit = gru_model.fit(x_test_reg,y_test_reg, epochs=2000, batch_size=100, callbacks=[es])\n",
        "gru_pred_test = gru_model.predict(x_test_reg,verbose=1)\n",
        "mse = mean_squared_error(y_test_reg[:,0],gru_pred_test[:,0])\n",
        "print(\"GRU test data mse = \" + str(mse))\n",
        "print(\"GRU test std mse =  \" + str(np.math.sqrt(mse)))\n",
        "gru_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validating GRU with optmized parameters....\n",
            "Epoch 1/2000\n",
            "261/261 [==============================] - 3s 12ms/step - loss: 0.0607\n",
            "Epoch 2/2000\n",
            "261/261 [==============================] - 0s 114us/step - loss: 0.0545\n",
            "Epoch 3/2000\n",
            "261/261 [==============================] - 0s 121us/step - loss: 0.0488\n",
            "Epoch 4/2000\n",
            "261/261 [==============================] - 0s 112us/step - loss: 0.0439\n",
            "Epoch 5/2000\n",
            "261/261 [==============================] - 0s 124us/step - loss: 0.0392\n",
            "Epoch 6/2000\n",
            "261/261 [==============================] - 0s 139us/step - loss: 0.0345\n",
            "Epoch 7/2000\n",
            "261/261 [==============================] - 0s 117us/step - loss: 0.0301\n",
            "Epoch 8/2000\n",
            "261/261 [==============================] - 0s 121us/step - loss: 0.0252\n",
            "Epoch 9/2000\n",
            "261/261 [==============================] - 0s 122us/step - loss: 0.0202\n",
            "Epoch 10/2000\n",
            "261/261 [==============================] - 0s 119us/step - loss: 0.0151\n",
            "Epoch 11/2000\n",
            "261/261 [==============================] - 0s 122us/step - loss: 0.0101\n",
            "Epoch 12/2000\n",
            "261/261 [==============================] - 0s 132us/step - loss: 0.0056\n",
            "Epoch 13/2000\n",
            "261/261 [==============================] - 0s 122us/step - loss: 0.0022\n",
            "Epoch 14/2000\n",
            "261/261 [==============================] - 0s 117us/step - loss: 0.0010\n",
            "Epoch 15/2000\n",
            "261/261 [==============================] - 0s 114us/step - loss: 0.0013\n",
            "Epoch 16/2000\n",
            "261/261 [==============================] - 0s 134us/step - loss: 9.7913e-04\n",
            "Epoch 17/2000\n",
            "261/261 [==============================] - 0s 138us/step - loss: 8.7270e-04\n",
            "Epoch 18/2000\n",
            "261/261 [==============================] - 0s 141us/step - loss: 9.5090e-04\n",
            "Epoch 19/2000\n",
            "261/261 [==============================] - 0s 130us/step - loss: 9.7378e-04\n",
            "Epoch 20/2000\n",
            "261/261 [==============================] - 0s 130us/step - loss: 9.1349e-04\n",
            "Epoch 21/2000\n",
            "261/261 [==============================] - 0s 143us/step - loss: 8.4733e-04\n",
            "Epoch 22/2000\n",
            "261/261 [==============================] - 0s 138us/step - loss: 8.4687e-04\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/2000\n",
            "522/522 [==============================] - 4s 7ms/step - loss: 0.0438\n",
            "Epoch 2/2000\n",
            "522/522 [==============================] - 0s 113us/step - loss: 0.0332\n",
            "Epoch 3/2000\n",
            "522/522 [==============================] - 0s 105us/step - loss: 0.0231\n",
            "Epoch 4/2000\n",
            "522/522 [==============================] - 0s 111us/step - loss: 0.0130\n",
            "Epoch 5/2000\n",
            "522/522 [==============================] - 0s 100us/step - loss: 0.0054\n",
            "Epoch 6/2000\n",
            "522/522 [==============================] - 0s 101us/step - loss: 0.0029\n",
            "Epoch 7/2000\n",
            "522/522 [==============================] - 0s 103us/step - loss: 0.0016\n",
            "Epoch 8/2000\n",
            "522/522 [==============================] - 0s 102us/step - loss: 0.0016\n",
            "Epoch 9/2000\n",
            "522/522 [==============================] - 0s 110us/step - loss: 0.0016\n",
            "Epoch 10/2000\n",
            "522/522 [==============================] - 0s 105us/step - loss: 0.0015\n",
            "Epoch 11/2000\n",
            "522/522 [==============================] - 0s 104us/step - loss: 0.0014\n",
            "Epoch 12/2000\n",
            "522/522 [==============================] - 0s 94us/step - loss: 0.0013\n",
            "Epoch 13/2000\n",
            "522/522 [==============================] - 0s 101us/step - loss: 0.0012\n",
            "Epoch 14/2000\n",
            "522/522 [==============================] - 0s 100us/step - loss: 0.0012\n",
            "Epoch 15/2000\n",
            "522/522 [==============================] - 0s 104us/step - loss: 0.0011\n",
            "Epoch 16/2000\n",
            "522/522 [==============================] - 0s 112us/step - loss: 0.0011\n",
            "Epoch 17/2000\n",
            "522/522 [==============================] - 0s 122us/step - loss: 0.0011\n",
            "Epoch 18/2000\n",
            "522/522 [==============================] - 0s 126us/step - loss: 0.0011\n",
            "Epoch 19/2000\n",
            "522/522 [==============================] - 0s 119us/step - loss: 0.0010\n",
            "Epoch 20/2000\n",
            "522/522 [==============================] - 0s 115us/step - loss: 0.0010\n",
            "Epoch 21/2000\n",
            "522/522 [==============================] - 0s 123us/step - loss: 0.0010\n",
            "Epoch 22/2000\n",
            "522/522 [==============================] - 0s 126us/step - loss: 9.8136e-04\n",
            "Epoch 23/2000\n",
            "522/522 [==============================] - 0s 124us/step - loss: 9.6207e-04\n",
            "Epoch 24/2000\n",
            "522/522 [==============================] - 0s 122us/step - loss: 9.6131e-04\n",
            "Epoch 25/2000\n",
            "522/522 [==============================] - 0s 154us/step - loss: 9.2615e-04\n",
            "Epoch 26/2000\n",
            "522/522 [==============================] - 0s 121us/step - loss: 9.1158e-04\n",
            "Epoch 27/2000\n",
            "522/522 [==============================] - 0s 175us/step - loss: 9.3159e-04\n",
            "Epoch 00027: early stopping\n",
            "Epoch 1/2000\n",
            "783/783 [==============================] - 4s 5ms/step - loss: 0.0395\n",
            "Epoch 2/2000\n",
            "783/783 [==============================] - 0s 114us/step - loss: 0.0271\n",
            "Epoch 3/2000\n",
            "783/783 [==============================] - 0s 108us/step - loss: 0.0154\n",
            "Epoch 4/2000\n",
            "783/783 [==============================] - 0s 116us/step - loss: 0.0050\n",
            "Epoch 5/2000\n",
            "783/783 [==============================] - 0s 119us/step - loss: 0.0017\n",
            "Epoch 6/2000\n",
            "783/783 [==============================] - 0s 106us/step - loss: 0.0016\n",
            "Epoch 7/2000\n",
            "783/783 [==============================] - 0s 98us/step - loss: 0.0013\n",
            "Epoch 8/2000\n",
            "783/783 [==============================] - 0s 99us/step - loss: 0.0013\n",
            "Epoch 9/2000\n",
            "783/783 [==============================] - 0s 98us/step - loss: 0.0012\n",
            "Epoch 10/2000\n",
            "783/783 [==============================] - 0s 110us/step - loss: 0.0011\n",
            "Epoch 11/2000\n",
            "783/783 [==============================] - 0s 115us/step - loss: 0.0011\n",
            "Epoch 12/2000\n",
            "783/783 [==============================] - 0s 111us/step - loss: 0.0011\n",
            "Epoch 13/2000\n",
            "783/783 [==============================] - 0s 113us/step - loss: 0.0010\n",
            "Epoch 14/2000\n",
            "783/783 [==============================] - 0s 107us/step - loss: 0.0010\n",
            "Epoch 15/2000\n",
            "783/783 [==============================] - 0s 110us/step - loss: 9.4050e-04\n",
            "Epoch 16/2000\n",
            "783/783 [==============================] - 0s 110us/step - loss: 9.7445e-04\n",
            "Epoch 17/2000\n",
            "783/783 [==============================] - 0s 95us/step - loss: 9.2821e-04\n",
            "Epoch 18/2000\n",
            "783/783 [==============================] - 0s 98us/step - loss: 8.7636e-04\n",
            "Epoch 19/2000\n",
            "783/783 [==============================] - 0s 105us/step - loss: 8.8742e-04\n",
            "Epoch 20/2000\n",
            "783/783 [==============================] - 0s 96us/step - loss: 8.6112e-04\n",
            "Epoch 00020: early stopping\n",
            "Epoch 1/2000\n",
            "1044/1044 [==============================] - 4s 4ms/step - loss: 0.0435\n",
            "Epoch 2/2000\n",
            "1044/1044 [==============================] - 0s 104us/step - loss: 0.0270\n",
            "Epoch 3/2000\n",
            "1044/1044 [==============================] - 0s 109us/step - loss: 0.0101\n",
            "Epoch 4/2000\n",
            "1044/1044 [==============================] - 0s 107us/step - loss: 0.0016\n",
            "Epoch 5/2000\n",
            "1044/1044 [==============================] - 0s 115us/step - loss: 0.0013\n",
            "Epoch 6/2000\n",
            "1044/1044 [==============================] - 0s 103us/step - loss: 0.0012\n",
            "Epoch 7/2000\n",
            "1044/1044 [==============================] - 0s 107us/step - loss: 0.0011\n",
            "Epoch 8/2000\n",
            "1044/1044 [==============================] - 0s 109us/step - loss: 0.0011\n",
            "Epoch 9/2000\n",
            "1044/1044 [==============================] - 0s 107us/step - loss: 0.0010\n",
            "Epoch 10/2000\n",
            "1044/1044 [==============================] - 0s 111us/step - loss: 9.5857e-04\n",
            "Epoch 11/2000\n",
            "1044/1044 [==============================] - 0s 108us/step - loss: 9.1436e-04\n",
            "Epoch 12/2000\n",
            "1044/1044 [==============================] - 0s 113us/step - loss: 8.7913e-04\n",
            "Epoch 13/2000\n",
            "1044/1044 [==============================] - 0s 108us/step - loss: 8.6592e-04\n",
            "Epoch 14/2000\n",
            "1044/1044 [==============================] - 0s 118us/step - loss: 8.2606e-04\n",
            "Epoch 15/2000\n",
            "1044/1044 [==============================] - 0s 114us/step - loss: 8.1111e-04\n",
            "Epoch 16/2000\n",
            "1044/1044 [==============================] - 0s 109us/step - loss: 7.9340e-04\n",
            "Epoch 17/2000\n",
            "1044/1044 [==============================] - 0s 98us/step - loss: 7.9494e-04\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/2000\n",
            "1305/1305 [==============================] - 5s 3ms/step - loss: 0.0433\n",
            "Epoch 2/2000\n",
            "1305/1305 [==============================] - 0s 106us/step - loss: 0.0243\n",
            "Epoch 3/2000\n",
            "1305/1305 [==============================] - 0s 99us/step - loss: 0.0051\n",
            "Epoch 4/2000\n",
            "1305/1305 [==============================] - 0s 118us/step - loss: 0.0015\n",
            "Epoch 5/2000\n",
            "1305/1305 [==============================] - 0s 111us/step - loss: 0.0013\n",
            "Epoch 6/2000\n",
            "1305/1305 [==============================] - 0s 103us/step - loss: 0.0011\n",
            "Epoch 7/2000\n",
            "1305/1305 [==============================] - 0s 113us/step - loss: 0.0011\n",
            "Epoch 8/2000\n",
            "1305/1305 [==============================] - 0s 113us/step - loss: 0.0010\n",
            "Epoch 9/2000\n",
            "1305/1305 [==============================] - 0s 110us/step - loss: 9.7389e-04\n",
            "Epoch 10/2000\n",
            "1305/1305 [==============================] - 0s 107us/step - loss: 9.1039e-04\n",
            "Epoch 11/2000\n",
            "1305/1305 [==============================] - 0s 101us/step - loss: 9.0291e-04\n",
            "Epoch 12/2000\n",
            "1305/1305 [==============================] - 0s 112us/step - loss: 8.4070e-04\n",
            "Epoch 13/2000\n",
            "1305/1305 [==============================] - 0s 111us/step - loss: 8.1758e-04\n",
            "Epoch 14/2000\n",
            "1305/1305 [==============================] - 0s 120us/step - loss: 7.7813e-04\n",
            "Epoch 15/2000\n",
            "1305/1305 [==============================] - 0s 116us/step - loss: 7.5416e-04\n",
            "Epoch 16/2000\n",
            "1305/1305 [==============================] - 0s 109us/step - loss: 7.3572e-04\n",
            "Epoch 17/2000\n",
            "1305/1305 [==============================] - 0s 109us/step - loss: 7.2091e-04\n",
            "Epoch 18/2000\n",
            "1305/1305 [==============================] - 0s 106us/step - loss: 7.3164e-04\n",
            "Epoch 19/2000\n",
            "1305/1305 [==============================] - 0s 101us/step - loss: 6.8100e-04\n",
            "Epoch 20/2000\n",
            "1305/1305 [==============================] - 0s 117us/step - loss: 6.9688e-04\n",
            "Epoch 21/2000\n",
            "1305/1305 [==============================] - 0s 120us/step - loss: 6.5980e-04\n",
            "Epoch 00021: early stopping\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_5 (GRU)                  (None, 100)               30600     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 30,701\n",
            "Trainable params: 30,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "MSE_train_gru_ave = 0.0004820272857379294\n",
            "MSE_test_gru_ave = 0.0005256709873301317\n",
            "MSE_train_gru_std = 0.02314272829728906\n",
            "MSE_test_gru_std = 0.024167723823547337\n",
            "p = 30\n",
            "Epoch 1/2000\n",
            "384/384 [==============================] - 5s 12ms/step - loss: 0.0426\n",
            "Epoch 2/2000\n",
            "384/384 [==============================] - 0s 110us/step - loss: 0.0372\n",
            "Epoch 3/2000\n",
            "384/384 [==============================] - 0s 114us/step - loss: 0.0318\n",
            "Epoch 4/2000\n",
            "384/384 [==============================] - 0s 112us/step - loss: 0.0259\n",
            "Epoch 5/2000\n",
            "384/384 [==============================] - 0s 115us/step - loss: 0.0200\n",
            "Epoch 6/2000\n",
            "384/384 [==============================] - 0s 119us/step - loss: 0.0137\n",
            "Epoch 7/2000\n",
            "384/384 [==============================] - 0s 104us/step - loss: 0.0075\n",
            "Epoch 8/2000\n",
            "384/384 [==============================] - 0s 105us/step - loss: 0.0030\n",
            "Epoch 9/2000\n",
            "384/384 [==============================] - 0s 125us/step - loss: 0.0012\n",
            "Epoch 10/2000\n",
            "384/384 [==============================] - 0s 116us/step - loss: 0.0015\n",
            "Epoch 11/2000\n",
            "384/384 [==============================] - 0s 141us/step - loss: 0.0014\n",
            "Epoch 12/2000\n",
            "384/384 [==============================] - 0s 118us/step - loss: 0.0011\n",
            "Epoch 13/2000\n",
            "384/384 [==============================] - 0s 111us/step - loss: 0.0011\n",
            "Epoch 14/2000\n",
            "384/384 [==============================] - 0s 108us/step - loss: 0.0011\n",
            "Epoch 15/2000\n",
            "384/384 [==============================] - 0s 95us/step - loss: 0.0010\n",
            "Epoch 16/2000\n",
            "384/384 [==============================] - 0s 102us/step - loss: 0.0010\n",
            "Epoch 17/2000\n",
            "384/384 [==============================] - 0s 102us/step - loss: 9.8333e-04\n",
            "Epoch 18/2000\n",
            "384/384 [==============================] - 0s 121us/step - loss: 9.4642e-04\n",
            "Epoch 19/2000\n",
            "384/384 [==============================] - 0s 109us/step - loss: 9.2795e-04\n",
            "Epoch 20/2000\n",
            "384/384 [==============================] - 0s 105us/step - loss: 9.3426e-04\n",
            "Epoch 00020: early stopping\n",
            "384/384 [==============================] - 2s 5ms/step\n",
            "GRU test data mse = 0.0009010238362576086\n",
            "GRU test std mse =  0.030017059087419085\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_6 (GRU)                  (None, 100)               30600     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 30,701\n",
            "Trainable params: 30,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16ZWjzOxxKIJ",
        "outputId": "7e252926-36ed-4c31-b417-2f010e7c86bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained LSTM\n",
        "lstm_pred_train = lstm_model.predict(x_train_reg, verbose=1)\n",
        "lstm_pred_test = lstm_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 4s 3ms/step\n",
            "384/384 [==============================] - 0s 74us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Imz5JXTnxS1y",
        "outputId": "63d0d9b9-dc6e-4e0c-8605-b5207560884b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# make predictions with the trained RNN\n",
        "rnn_pred_train = rnn_model.predict(x_train_reg, verbose=1)\n",
        "rnn_pred_test = rnn_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 0s 40us/step\n",
            "384/384 [==============================] - 0s 47us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJgsZCf2xVGl",
        "colab": {}
      },
      "source": [
        "gru_pred_train = gru_model.predict(x_train_reg, verbose=1)\n",
        "gru_pred_test = gru_model.predict(x_test_reg, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRQEeO42DpVv",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the plain RNN\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], rnn_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], rnn_pred_test[:, 0])\n",
        "print(MSE_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7fHAc8zHDyFZ",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], gru_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], gru_pred_test[:, 0])\n",
        "print(MSE_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKcHJU79D_K7",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the GRU\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], lstm_pred_train[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:],lstm_pred_test[:, 0])\n",
        "print(MSE_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Igbsz8AD4vi",
        "colab": {}
      },
      "source": [
        "# calculate mean squared error of the alpha RNN\n",
        "\n",
        "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], train_losses[:, 0])\n",
        "print(MSE_train)\n",
        "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], validation_losses[:, 0])\n",
        "print(MSE_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t6SkTDvIxXcV",
        "outputId": "e8a9eb38-6a81-4aa0-87fa-7ea5e10c9dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "train_line_real = plt.plot(df_train.index[n_steps:], df_train[use_feature][n_steps:], color=\"orange\", label=\"Observed (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], lstm_pred_train[:, 0], color=\"red\", label=\"LSTM Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], rnn_pred_train[:, 0], color=\"blue\", label=\"RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:], gru_pred_train[:, 0], color=\"black\", label=\"GRU Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:],y_predicted_ar_train, color='green', label=\"alpha RNN Predict (Training)\")\n",
        "train_line_pred = plt.plot(df_train.index[n_steps:],y_predicted_ar_trian_t, color=\"yellow\", label=\"alpha_t RNN Predict (Training)\" )\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Training)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "fig = plt.figure(figsize=(12,7))\n",
        "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"orange\", label=\"Observed (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], rnn_pred_test[:, 0], color=\"red\", label=\"RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], rnn_pred_test[:, 0], color=\"blue\", label=\"RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], gru_pred_test[:, 0], color=\"black\", label=\"GRU Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test.index[n_steps:], y_predicted_ar,color=\"green\", label=\"alpha RNN Predict (Testing)\")\n",
        "test_line_pred = plt.plot(df_test,index[n_steps:], y_predicted_ar_t, color=\"yellow\", label= \"alpha_t RNN Predict (Testing)\")\n",
        "# train_line_pred = plt.plot(session.run(output, feed_dict={alpharnn.input_layer: x_test_reg})\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=12)\n",
        "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
        "plt.xlabel('Time', fontsize=20)\n",
        "plt.ylabel('Y', fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b7ac5ae89c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_line_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"orange\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Observed (Training)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_line_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_pred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LSTM Predict (Training)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_line_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_pred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RNN Predict (Training)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_line_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_pred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GRU Predict (Training)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m     return gca().plot(\n\u001b[1;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2789\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1566,) and (1305,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAGeCAYAAABvvTxFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRiGUYT/D5LuqEySBCtcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}